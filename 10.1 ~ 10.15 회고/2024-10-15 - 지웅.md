# 📝 2024.10.15 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 사전 학습 모델 (Pre-trained Model)
- 사전 학습이란
- 자비어 방법 등을 통해 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법
  - 텍스트 유사도 예측 모델을 만들기 전 감정 분석 문제를 학습한 모델의 가중치를 활용해 텍스트 유사도 모델의 가 중치로 활용하는 방법 → 즉, 감정 분석 문제를 학습하면서 얻은 언어에 대한 이해를 학습한 후 그 정보를 유사도 문제를 학습하는 데 활 용하는 방식

- 사전 학습 모델
  - 사전 학습을 통해 학습이 완료된 가중치가 저장되어 있는 모델

- 하위 문제 : 사전 학습한 가중치를 활용해 학습하고자 하는 본 문제
  - 상위문제 : 사전 학습한 모델인 감정 분석 문제 (하위 문제를 하기 위해 사전에 해야하는 문제)
  - 하위 문제: 사전 학습된 가중치를 활용해 본격적으로 학습하고자 하는 문제인 텍스트 유사도 문제


#### 사전 학습 모델을 이용하지 않고 학습하는 과정
- 데이터 셋 분석 및 전처리
  - 데이터 셋을 EDA(Exploratory Data Analysis, 탐색적 자료 분석)과정을 거쳐 분석, 토크나이징, 임베딩 등 전처리 수행
- 모델 설계, 구현 및 생성
  - 이떄 생성된 모델은 임의의 값으로 초기화 된 가중치를 가짐
- 모델 학습
  - 유사도 데이터를 활용해 학습 수행
 
#### 사전 학습 모델을 이용한 학습 과정

- 적절한 데이터 셋에 대하여 잘 학습된 사전 학습 모델 확보  
  - 이때 모델은 데이터 셋에 대하여 학습이 완료된 가중치를 가지고 있음
- 모델 학습
  - 가중치 초기화 등의 과정을 수행하지 않고
  - 기존에 학습이 완료된 가중치를 초기값으로 사용함
  - 현재 해결하고자 하는 문제에 대한 유사도 데이터를 활용하여 학습 수행
  - 단, 사전 학습 모델의 최종 출력 값을 뽑는 가중치 층은 제외
    - → 우리가 해결하고자 하는 문제와 관계없는 형태의 데이터가 포함되어 있기 때문

- 사전 학습된 가중치의 활용

![image](https://github.com/user-attachments/assets/afe047b5-9b96-4775-9458-1f5e665f5e9b)


- 언어 모델을 사전학습한 모델

![image](https://github.com/user-attachments/assets/0203c8f9-7d7d-4faa-be3c-6ecb41a92520)


- 사전 학습은 어떤 문제에도 적용이 가능함 (음악, 영상, 코딩 등)
- 대부분의 언어 처리 문제는 Label 이 주어지지 않는 비지도 학습 문제에 포함됨 → 데이터에 제약이 없고 언어에 대한 전반적인 이해를 사전 학습하는 언어 모델이 효율적
- 대부분의 경우, 하위 문제 모델의 성능을 향상시키는 결과를 가지고옴
- 언어 모델은 대규모 데이터를 활용한 사전 학습을 통해 언어에 대한 전반적인 이해를 하게 됨 → 이렇게 학습된 지식을 기반으로 하위 문제에 대한 성능을 향상시킴

#### 특징 기반(Feature-Based) 방법
- 사전 학습된 특징을 하위 문제의 모델에 부가적인 특징으로 활용하는 방법
- 대표사례: word2vec 
  - → 학습한 임베딩 특징을 우리가 학습하고자 하는 모델의 임베딩 특징으로 활용


#### 미세 조정(Fine-Tuning)
- 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해서 모델을 추가로 학습(미세 조정)하는 방법
- 대표사례: 감정 분석 문제에 사전 학습 시킨 가중치와 더불어 텍스트 유사도를 위한 부가적인 가중치를 추 가하여 텍스트 유사도 문제를 학습하는 것

#### 트랜스포머 모델의 활용 및 확산

- 트랜스포머 모델 이후
  - 대부분 자연어 처리 연구에서 트랜스포머 모델을 기반으로 하는 비지도 사전 학습을 통해 학습한 많은 가중치를 활용해 자연어 처리 모델을 미세 조정하는 방법이 각광 받고 있음

- 대표적인 트랜스포머 기반 사전 학습 모델
  - GPT (Generative Pre-trained Transformer)
    - OpenAI에서 발표한 최초의 트랜스포머 기반 사전 학습 언어 모델(2018)
    - 현재 GPT-1, GPT-2를 거쳐 GPT-4까지 발표됨 (트랜스포머의 디코더만 사용)
    - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf (최초 논문)
  - BERT (Bidirectional Encoder Representations from Transformer) <양방향 트랜스포머>
    - Google에서 발표한 트랜스포머 기반 사전 학습 언어 모델(2018) (트랜스포머의 인코더만 사용)
    - https://arxiv.org/pdf/1810.04805.pdf


### GPT1
- 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 해결하고자 하는 문제에 미세조정을 적용하는 방식의 언어 모델
- 트랜스포머 모델을 기반으로 하고, 트랜스포머의 디코더 구조만 사용, 순방향 마스크 어텐션 적용

![image](https://github.com/user-attachments/assets/c4ce01cf-2115-4b39-b792-9ce6995832d8)

- 하나의 사전 학습 방식(전통적 언어 모델 방식) 사용 → 앞 단어들을 활용해 해당 단어 예측하는 방식
- 비교 모델인 BERT의 경우, 사전 학습에서만 언어 모델의 손실 값(Loss)을 사용해서 학습하지만, GPT-1: 본 학습 시에도 실제로 학습해야 하는 문제의 손실 값과 더불어 언어 모델의 손실 값 또한 학습함

- GPT-1:
- Label이 따로 존재하지 않아도 학습 진행 가능 → 비지도 학습
- 접근법: 비지도 사전 학습과 지도학습 미세조정(파인튜닝) 을 결합한 반지도학습적 접근
- 목표: 넓은 범위에서의 작업에 약간의 조정만으로도 전이할 수 있는 범용 표현의 학습
  - 2단계 구조 적용 : Unlabeled Data  상에서 동작하는 언어 모델 개체 사용해 사전학습 후 지도 학습에 해당하는 Target Task를 대상으로 미세조정
  - 언어 모델 개체에 트랜스포머 구조를 적용
  - 지도학습 모델의 일반화 성능을 개선하고, 학습 수렴을 빠르게 해 주는 장점을 보임
 
- 기존 트랜스포머 모델의 단점
  - 테스크가 가진 특정 아키텍처의 목표에 따라 학습 수행
  - 상당한 양의 테스크 특정 커스터마이징이 필요, 전이 학습을 어렵게 함 (사전 학습 모델이 잘 동작할 수 있도록 입력 구조를 변환시키는 방법을 적용)

![image](https://github.com/user-attachments/assets/c925d550-9a7c-4473-a30e-d888ba68d540)

![image](https://github.com/user-attachments/assets/063a4d0b-a0c3-42f5-ac0d-b7e0d092551b)

#### GPT 성능 향상 → GPT2

- 기존 디코더에서 각 레이어 직후 잔차연결과 함께 적용되던 층 정규화가 각 부분 블록의 입력 쪽으로 이동
- 마지막 셀프 어텐션 레이어 이후 층 정규화가 적용

- 사전 학습 데이터 크기 : 한 영역의 텍스트를 사용(GPT1)
- 사전 학습 데이터 크기 : 다양한 영역의 텍스트를 활용해 사전 학습 진행 → 모델이 좀 더 다양한 문맥과 영역의 글을 이해할 수 있게 함 (GPT2)

- 모델의 크기
  - GPT-1: 총 12개의 Layers, 총 117만개의 가중치
  - GPT-2: 총 48개의 Layers, 총 1,542만개

- 모델의 입력
- GPT-1
  - 텍스트를 특정 단위(예: 문자 단위, 단어 단위 등)로 나눠서 모델의 입력으로 사용
- GPT-2
  - BPE(Byte Pair Encoding) 방식을 사용해 텍스트를 나눠서 모델의 입력으로 사용
  - 글자와 문자 사이의 적절한 단위를 나누어 줌으로써 높은 성능을 발휘함

- GPT-2 논문
- https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

- 기존에는 pre-training과 supervised fine-tuning의 결합으로 만들어졌던 두 개의 작업을 연결, 기존에 사용해 오던 전이(Transfer) 방법은 계속 유지
- 지도 학습이 없는 상태로 만들어진다면 일반 상식 추론 등의 다양하게 범용저긍로 사용할 수 있을것

- 언어모델에 대하여 매개변수나 모델 아키텍처 수정없이 사용할 수 있는 zero-shot 방식 적용
  - 모델이 바로 하위 작업에 적용됨
  - Zero-shot 설정으로 인하여 범용성 있는 언어 모델 능력 향상이 가능해 짐

- 핵심은 언어모델임
![image](https://github.com/user-attachments/assets/399ea6e3-8849-4d47-817c-a2a073b900ae)

- 학습 데이터 셋은 기존에는 기사, wikipedia를 사용(Single Domain Data)
- GPT2 에서는 다양한 도메인과 컨텍스트, 주제를 가진 대규모의 데이터 셋 수집
- 데이터 품질에 문제가 있을 수 있으므로, Web Text 사용
  - 인간에 의해 수동으로 필터링 된 웹페이지 수집, Reddit(소셜 뉴스 웹사이트)에서 3Karma 이상의 데이터 수집 → Heuristic indicator로 생각할 수 있음(Interesting, educational of just funny)
- 중복 제거, Wikipedia document 제거 등 전처리 수행, 40GB, 800만 개 이상 문서, 4500만 링크 포함

- Byte Pair Encoding 활용을 활용해
  - 글자(byte)와 단어의 중간 단위를 사용할 수 있음
  - Subword 분리 알고리즘 중 하나로 OOV(Out Of Vocabulary) 문제를 해결
  - Subword들을 활용하기 때문에 OOV와 신조어 같은 단어에 강점이 있음

- 모델의 경우 Transformer decoder를 활용
- 기본적으로 GPT-1과 동일함

![image](https://github.com/user-attachments/assets/e8c05dcd-faa1-4b5d-a22e-b6e938456801)


- 언어모델로서의 GPT2의 강점
  - Byte 수준에서 동작
  - 손실이 있는 전처리 또는 토큰화 불필요
  - 모든 언어 모델에 대한 벤치마크 평가 가능
- WebText 언어 모델에 따른 dataset의 log 확률을 계산하는 방식으로 통일
- <UNK>은 400억 byte 중 26번 밖에 나타나지 않음
- Zero shot으로 8개중 7개에서 SOTA(최상의 결과) 달성

![image](https://github.com/user-attachments/assets/4ad8d737-be20-4ac4-b4fc-5c594d21928f)

- 일부 분야에서는 좋은 성적을 보였지만, 더욱 개선할 필요성이 보였다.

### GPT-3
- 2020년 5월 공개
- 총 1,750억 개의 가중치 보유
- 기존 사전 학습-미세 조정 방법론의 구조적 한계 지정, 새로운 방법론 제시
  - 사전 학습-미세 조정 방법의 가장 큰 한계는 학습된 모델이 특정 문제에 국한된다는 점
  - 각 하위 문제를 해결하기 위하여 미세 조정 과정이 필요할 뿐만 아니라
  - 미세 조정을 위한 하위 문제의 데이터셋이 추가적으로 필요함
  - 일반적으로 이러한 데이터셋의 경우 각 문제에 대하여 수십만 개의 데이터를 요구함

- 기존 방법론에 대한 새로운 접근법의 필요성이 필요해 고안된 것이 메타 학습 방법론

##### 메타 학습 방법론
- 사전 학습 과정에서 학습된 다양한 언어적인 능력 및 패턴을 인식하는 능력만을 활용해서 새로운 문제에 적용하는 방법
- 방대한 데이터로 가중치를 사전 학습하고, 학습된 모델의 능력을 활용해서 특정 문제에 적용 및 예측하는 방법

- 기존 방법론: 사전 학습+미세 조정 후 새로운 문제에 특정된 가중치로 업데이트
- 메타 학습 방법론: 사전 학습 과정에서 학습된 정보만을 활용하여 문제 해결

##### 메타 학습 방법론 종류
- 제로샷 러닝 (Zero-shot Learning): 0개의 예시 활용
- 원샷 러닝 (One-shot Learning): 1개의 예시 활용
- 퓨샷 러닝 (Few-shot Learning): n개의 예시 활용

- 문제를 해결하기 위해 n개의 예시를 언어 모델에 제공하고 그 예시를 활용해서 문제를 해결함
- 미세조정에서는 추가 데이터를 활용해 모델 자체를 추가 학습하지만, 퓨샷 러닝에서는 모델을 추가 학습하지 않고 문제를 해결하기 위해 단순히 예시만 활용한다는 점이 다름

##### GPT-3 한계
- 1,750억개의 매개변수 → 인간이 평생 보는 정보보다 많은 데이터를 학습해야 함 (낮은 효율성)
- 현실 물리적 상식 부족 → 세상을 글로만 학습했기 때문. 현실에서 직접 겪어봐야 알 수 있는 매우 당연한 상식을 학습할 기회가 적었음

- 모든 분야에서 뛰어난 것은 아님
  - 아직까지 대부분 태스크에서 사람보다 떨어진 성능을 보임
  - 주어진 태스크 마다 성능차이가 심함
    - 두 가지 이상의 복합연산 능력이 떨어짐, 태스크를 수행하기 위해 주어진 데이터가 적을수록 성능이 크게 떨어지는 경향을 보임
    - 학습에 사용된 예제를 외운 것인지 실제 추론한 것인지 구분하기 어렵다.
    - 새로운 정보를 수용하기 어렵다. 한마디로 "기억력"이 없다
    - 방대한 양의 텍스트를 통해 다음 단어를 예측하는 방식으로 학습됨

## BERT 모델
- BERT (Bidirectional Encoder Representation Transformers)
- 2018년 Google이 발표한 자연어 처리를 위한 딥 러닝 모델
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(https://arxiv.org/abs/1810.04805)
- 비지도 사전 학습을 한 모델에 추가로 하나의 완전 연결 계층만 추가한 후 미세 조정을 통해 총 11개의 자 연어 처리 문제에서 최고의 성능을 보여줌
- 영상인식 계열에 비하여 발전이 늦은 언어처리 딥 러닝의 한계를 돌파하는 계기가 될 것으로 기대 받음


- 양방향성의 사전 학습 모델
  - GPT: 단방향, ELMo: 단방향 모델 2개를 반대 방향으로 결합시켜 만든 (불완전) 양방향

![image](https://github.com/user-attachments/assets/b091be5d-dc57-436e-9282-06d54d9e7c52)

- BERT는 사전 학습 언어 모델이면서 문맥을 고려한 임베딩 모델이라고도 부름
  - Word2Vec: 두 단어를 동일한 표현으로 임베딩 → 문맥 독립 모델
  - BERT: 두 단어에 대하여 서로 다른 임베딩을 제공 → 문맥 기반 모델

![image](https://github.com/user-attachments/assets/0075d1aa-d825-4733-a044-5c03bd93e171)

- 트랜스포머 인코더는 양방향으로 문장을 읽을 수 있으므로 양방향 → BERT는 양방향 인코더 표현을 사용
- 문장의 각 단어를 문장의 다른 모든 단어와 연결해 관계 및 문맥을 고려해 의미를 학습함
- BERT에 입력된 문장의 각 간어 표현 출력
![image](https://github.com/user-attachments/assets/552cb220-e707-47be-9e86-07a4eb09f607)


#### BERT의 구조
- BERT-base와 BERT-large의 두 가지 구성의 모델 제시

![image](https://github.com/user-attachments/assets/21ff4235-09c0-4c8a-91a4-a2d9390b0c42)

- 컴퓨팅 리소스가 제한된 환경 → 작은 BERT가 적합
- BERT-base, BERT-large와 같은 표준 구조가 더 정확한 결과를 제공하기때문에 가장 널리 사 용됨

- 사전 학습
  - 특정 태스크에 대한 방대한 데이터셋으로 모델을 학습시키고 저장
  - 새로운 태스크가 주어지면 임의의 가중치로 모델을 초기화하는 대신 이미 학습된 모델(사전 학습된 모델)의 가중치로 모델을 초기화
  - 새로운 태스크에 따라 가중치를 조정(미세 조정, Fine-Tuning)

- 사전학습 수행 방식을 입력 표현과 MLM, NSP의 두 가지 태스크를 이용하여 사전 학습 으로 변경

- BERT의 입력 표현
  - BERT의 입력은 다음 세 가지의 임베딩의 합으로 구성
    - 토큰 임베딩
    - 세그먼트 임베딩
    - 위치 임베딩

- BERT에서 사용하는 토크나이저: WordPiece Tokenizer 만들어서 사용

![image](https://github.com/user-attachments/assets/cb13cc50-903d-40ff-900c-99a098fdb13f)

- 개별 문자에 도달할 때까지 반복 → OOV(Out of Vocabulary)의 단어 처리에 효과적

- 토큰 임베딩

![image](https://github.com/user-attachments/assets/ddb72a4b-41b1-45a6-8443-7b9ee91f96e2)

- 세그먼트 임베딩

![image](https://github.com/user-attachments/assets/d72a441e-407c-498c-bec3-81b1f7489f26)

- 위치 임베딩
  - 트랜스포머 모델은 어떤 반복 메커니즘도 사용하지 않고 모든 단어를 병렬 처리 → 단어 순서와 관련된 정보 제공이 필수 → 위치 인코딩 사용

- BERT는 본질적으로 트랜스포머의 인코더이므로...
  - BERT에 데이터를 직접 입력하기 전에 문장에서 단어(토큰)의 위치에 대한 정보 제공 요구 → 위치 임베딩 레이어를 사용해 각 토큰에 대한 위치 임베딩 출력을 확보

![image](https://github.com/user-attachments/assets/1a8be2d5-4a87-41d3-bf43-40fe0d01dad8)

- 최종 입력 데이터 표현
  - 입력 문장 → 토큰으로 변환 → 토큰 임베딩 → 세그먼트 임베딩 → 위치임베딩 → 최종 임베딩 결과(세가 지 임베딩의 합산) 확보 → BERT에 입력

![image](https://github.com/user-attachments/assets/81f09576-c005-4b7f-9833-1bc79150f541)

- BERT는 두 가지의 태스크에 대해 사전 학습을 수행함

- 마스크 언어 모델링 (Masked Language Modeling, MLM)
- 다음 문장 예측 (Next Sentence Prediction, NSP)

![image](https://github.com/user-attachments/assets/403c3513-38d6-41d4-bfbc-5ef4e012f396)

- 자동 회귀 언어 모델링
![image](https://github.com/user-attachments/assets/5466436e-d1ce-4f17-b459-34bbde96cda9)

- 자동 인코딩 언어 모델링
![image](https://github.com/user-attachments/assets/5bb9e8fe-1c74-419c-841c-9895c2397a73)

##### 마스크 언어 모델링

- BERT: 자동 인코딩 언어 모델로 예측을 위해 문장을 양방향으로 읽음
- 마스크 언어 모델링은
  - 빈칸 채우기 태스크(Cloze Task)라고도 부름
  - 주어진 입력 문장에서 전체 단어의 15%를 무작위로 마스킹하고
  - 마스크 된 단어를 예측하도록 모델을 학습시키는 것
  - 모델은 마스크 된 단어를 예측하기 위해 양방향으로 문장을 읽고 예측을 시도

![image](https://github.com/user-attachments/assets/a42ddd0b-e0e4-441c-945d-cf8948609b56)


- MLM에서 사전학습과 파인 튜닝 간 불일치 문제의 원인
  - Process를 살펴보면
    1. 먼저 [MASK] 토큰을 예측해서 BERT를 사전학습 시킨다.
    2. 학습 후에는 감정 분석과 같은 하위 작업(Downstream Task)을 위해 사전 학습된 BERT를 파인 튜닝한다.
    3. 그런데 파인 튜닝에는 입력에 [MASK] 토큰이 없다. → 이 때문에 BERT가 사전 학습되는 방식과 파인 튜닝에 사용되는 방식 간에 불일치 발생

- 마스킹 언어 모델링의 문제점개선 방안
  - 토큰을 마스킹하면 사전학습과 파인 튜닝 사이에 불일치가 발생함 → 문제 극복을 위해 80-10-10% 규칙 적용

![image](https://github.com/user-attachments/assets/21829931-342a-4f1e-930f-95431b6b88a1)

- 토큰화 및 마스킹 후, 입력 토큰을 토큰, 세그먼트, 위치 임베딩 레이어에 입력해서 입력 임베딩 확보
![image](https://github.com/user-attachments/assets/42d66f13-93cb-4e6a-ae7b-502de354f43d)

- 각 토큰의 표현 R을 얻은 후
![image](https://github.com/user-attachments/assets/2f783ab9-4c2f-4547-8c10-9b6ac1614101)

- 학습 초기의 경우
  - BERT의 FFN 및 인코더 계층의 가중치가 최적이 아님 → 모델의 올바른 확률 반환 불가능
  - 역전파를 통한 일련의 반복 학습을 거치면서 BERT의 FFN 및 인코더 계층의 가중치 업데이트 반복 → 최적의 가중치 학습

- 전체 단어 마스킹

![image](https://github.com/user-attachments/assets/f1e299e5-db01-4c0e-bf47-e5ac52e2f56b)

##### 다음 문장 예측(NSP : Next Sentence Prediction)
- BERT 학습에 사용되는 Task, 이진 분류 Task이며, NSP Task에서는 BERT에 두 문장을 입력하고, 두 번째 문장이 첫 번째 문장의 다음 문장인지 예측

![image](https://github.com/user-attachments/assets/83fb4e67-99a0-485e-a7cf-d7078c4408aa)

- NSP Task에서 모델의 목표
  - 문장 쌍이 isNext 범주에 속하는지 여부를 예측하는 것 → 문장 쌍을 BERT에 입력하고 B 문장이 A 문장 다음에 오는지 여부를 예측하도록 학습 → 모델은 B 문장이 A 문장에 이어지면 isNext 반환, 그렇지 않으면 notNext 반환 → NSP는 본질적으로 이진 분류 태스크

- NSP Task의 목적
  - NSP 태스크를 수행함으로써 모델은 두 문장 사이의 관계를 파악할 수 있음
  - 질문-응답(QA), 감정 분류, 유사문장 탐지와 같은 하위작업(Downstream Task)에 유용
- 데이터 편중이 데이터셋 확보에 가장 중요한 것인데 isNext 클래스와 notNext 클래스의 비율을 50:50으로 유지하여 클래스가 균형을 이루게 할 수 있다.

![image](https://github.com/user-attachments/assets/81c26e67-9851-4d88-95f9-27f96c27a45f)

- 학습 초기의 경우 BERT의 FFN 및 인코더 계층의 가중치가 최적이 아님 → 모델의 올바른 확률 반환 불가능
- 역전파를 통한 일련의 반복 학습을 거치면서 BERT의 FFN 및 인코더 계층의 가중치 업데이트 반복 → 최적의 가중치 학습

##### BERT 사전 학습 절차
- BERT 사전 학습의 데이터셋 : 토론토 책 말뭉치(Toronto BookCorpus) + 위키피디아 데이터셋
- 말뭉치에서 두 문장(A, B) 샘플링 : A, B 문장의 총 토큰 수의 합은 512 이하(작거나 같음)여야 함

![image](https://github.com/user-attachments/assets/d30b3461-bcf3-40d8-b9e8-c5fe950fb517)

![image](https://github.com/user-attachments/assets/63a8082e-8c9d-4011-8914-e0f54d9367ce)

- 하위 단어 토큰화
  - OOV (Out of Vocabulary) 단어의 처리에 매우 효과적이기 때문에
  - BERT, GPT-3 등 다양한 최신 자연어 모델에서 널리 사용 중

- 단어 수준 토큰화 과정
  - 학습 데이터셋 준비 → 학습 데이터셋에서 어휘 사전 구축 → 데이터셋의 텍스트를 공백으로 분할 → 모든 고유 단어를 어휘 사전에 추가 → 어휘 사전을 이용하여 입력 텍스트를 토큰화

- 어휘 사전 생성에 사용되는 하위 단어 토큰화 알고리즘
- 바이트 쌍 인코딩 (Byte Pair Encoding)
- 바이트 수준 바이트 쌍 인코딩 (Byte-level Byte Pair Encoding)
- 워드피스 (WordPiece) : BERT에서는 이거씀

- 워드피스 (WordPiece)
- BPE와 유사하게 동작

- BPE
  1. 주어진 데이터셋에서 먼저 단어의 빈도수를 추출하고
  2. 단어를 문자 시퀀스로 분할
  3. 빈도수가 높은 기호 쌍을 병합
  4. 어휘 사전 크기에 도달할 때까지 반복적으로 고빈도 기호 쌍 병합
- WordPiece
  1. 빈도에 따라 기호 쌍을 병합하지 않고 가능도(likelihood)를 기준으로 기호 쌍을 병합
  2. 따라서 주어진 학습 데이터에 대해 학습된 언어 모델 가능도가 높은 기호 쌍을 병합


## Chatbot 시스템

- Chatbot = Chatter + Robot : 대화하는 로봇이라는 의미

- 일반적인 chatbot 시스템 동작

![image](https://github.com/user-attachments/assets/b07bec24-b622-49fa-bd02-8b847d490176)

## ChatGPT
- 언어 모델 등장 (데이터가 너무 많아서 처리하기 어려워 점점 통합된 모델이 개발된 것)
![image](https://github.com/user-attachments/assets/ed0c98d6-9724-4360-9097-36422b1c83f1)

#### 자연어 처리 → 언어 모델 → 초거대 AI
- 자연어 처리는 수많은 표현을 가진 언어 데이터를 통계를 기반으로 처리하려고 하다 보니 대규모의 데이터를 요구하게 됨
- 언어 모델을 기반으로 거대 데이터와 거대 파라미터를 적용하고 그 과정에서 발생한 문제를 해결하기 위한 알고리즘으로 각 부분들을 연결해 규모를 키우니 성능이 더 좋아짐
- 이것이 바로 초 거대 AI의 등장이다.
  - 글로벌: GPT 계열(OpenAI), BERT 계열(Google)
  - 국내: 네이버(HyperClova), 카카오(KoGPT, minDALL-E), LG(EXAONE) 등

- ChatGPT의 등장
-  초거대 AI의 성능이 검증되자 더욱 인간의 능력을 잘 흉내 내고자 함
  → 챗봇 시스템과 생성 모델의 도입

#### 생성형 AI란?
- 생성 모델을 기초로 텍스트, 오디오, 이미지 등의 기존 콘텐츠를 활용하여 유사한 콘텐츠를 새로 만들어내는 인공지능(AI) 기술

- 결과물의 형태에 따른 AI의 분류
![image](https://github.com/user-attachments/assets/51ec77d3-70f0-42c7-bd00-88ce52ef394c)

- 생성 모델 분류
- 지도적 생성 모델
  - 정답(Label)이 있는 데이터에 대하여 각 클래스별 특징 데이터의 확률 분포 P(X|Y)를 추정한 다음 베이즈 정리를 사용하 여 P(Y|X)를 계산하는 모델
    - P(X|Y): 사건Y가 발생했다는 조건 하에서 사건X가 일어날 확률 -> Y에 대한 X의 조건부 확률
- 비지도적 생성 모델
  - 정답(Label)이 없는 데이터에 대하여 데이터 자체의 분포를 학습하여 X의 모분포를 추정하는 학습데이터의 분포를 학습 하는 모델(대부분의 생성 모델은 비지도적 생성 모델에 포함됨)
    - 통계적 생성 모델: 관측된 데이터들의 분포로부터 원래 변수의 확률분포를 추정함으로써 새로운 데이터 생성
    - 딥러닝 기반 생성 모델: GAN(생성적 적대 신경망)과 같은 경쟁 학습 등의 방법을 이용하여 새로운 데이터 생성

- 생성 모델은 궁극적으로는 입력(관측)된 데이터를 이용하여 원래의 데이터의 확률 분포를 추정(생성)해 내려는 모델이다.

#### ChatGPT 계열의 AI에서 주의해야 할 점
- 초거대 ai는 언어 모델을 기반으로 함
- 입력된 질의에 대해 확률적으로 가능성이 가장 높은 단어를 선택
- 가장 자연스러워 보이는 표현 패턴을 선택하여 문장을 생성하므로 자연스러워 보이지만 사실(데이터)를 근거로 한 것은 아니다(거짓일 확률이 매우 높음)
- 할루시네이션(Hallucination, 환각) 오류
  - 실제로 존재하지 않는 것을 마치 있었던 말인 것처럼, 즉 틀린 답을 사실인 양 그럴듯하게 대답하는 것
  - 정 분야에 대한 정확한 정보만을 추가로 학습하여 신뢰할 수 있는 전문가 용 시스템 개발(Naver의 HyperCLOVA X, 또는 잘못된 답에 대한 피드백을 주어 반영할 수 있도록 시스템 보완
  - 그러나 초거대 AI의 구조적인 문제로 인해 개선은 가능해도 해결은 거의 불가능에 가까움
  - 사용자 입장에서 극복은 Prompt Engineering
  
### AI 기술 발전의 흐름

![image](https://github.com/user-attachments/assets/e8241468-95f9-40e2-9c6e-93e6b3fd6a61)



## window에서 ollama 사용
- ollama 홈페이지에서 window 버전 다운
- 다운 후 실행하면 자동 실행됨(다음날 사용시 다시 실행시켜주어야함)

![image](https://github.com/user-attachments/assets/92ba71a8-c3a2-4faf-bed8-baf8fa453e17)

- 해당 이미지처럼 모델 부분 클릭

![image](https://github.com/user-attachments/assets/aedbc646-5766-446c-ad34-fa40e74e42b8)

- 위 이미지에 명령어를 cmd창에 입력
- 자동으로 run llama3.2버전이 실행됨
![image](https://github.com/user-attachments/assets/f8d03dce-5243-4d4f-9e64-5cf38893d82a)

- 위와같이 자동으로 잡아줘서 동작하고 내가 원하는 커멘드를 집어 넣으면 해당 내용에 대한 답변을 진행해줌

- 이후 아래 코드를 사용해서 작동하면 아래와 같음
- 코드를 이용하기 전 ollama pull openchat을 먼저 해주어야함

```
# ollama01.py
import ollama
response = ollama.chat(model='openchat', messages=[
  {
    'role': 'user',
    'content': '하늘은 왜 푸른지 설명해줘.',
  },
])
print(response['message']['content'])

```
![image](https://github.com/user-attachments/assets/c593133e-71f0-4206-9e2d-60762181bcf3)

- 아래 코드 사용전에 cmd창에서 !pip install requests 먼저 진행
- 이후 동일하게 실행해주면 됨

```
# ollama02.py
import requests

def query_ollama(model_name, prompt):
    url = "http://localhost:11434/api/generate"
    headers = {"Content-Type": "application/json"}
    data = {
        "model": model_name,
        "prompt": prompt,
        "stream": False
    }

    response = requests.post(url, headers=headers, json=data)
    response.raise_for_status()
    response_data = response.json()
    print(response_data)
    # return response_data['response']

if __name__ == "__main__":
    model = "openchat"
    prompt = "하늘은 왜 푸르게 보일까?"
    result = query_ollama(model, prompt)
    # print(result)

```
![image](https://github.com/user-attachments/assets/a85798bb-0527-4fbc-a2ed-2390bc9ebb08)
