# 📝 2024.10.15 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 사전 학습 모델 (Pre-trained Model)
- 사전 학습이란
- 자비어 방법 등을 통해 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법
  - 텍스트 유사도 예측 모델을 만들기 전 감정 분석 문제를 학습한 모델의 가중치를 활용해 텍스트 유사도 모델의 가 중치로 활용하는 방법 → 즉, 감정 분석 문제를 학습하면서 얻은 언어에 대한 이해를 학습한 후 그 정보를 유사도 문제를 학습하는 데 활 용하는 방식

- 사전 학습 모델
  - 사전 학습을 통해 학습이 완료된 가중치가 저장되어 있는 모델

- 하위 문제 : 사전 학습한 가중치를 활용해 학습하고자 하는 본 문제
  - 상위문제 : 사전 학습한 모델인 감정 분석 문제 (하위 문제를 하기 위해 사전에 해야하는 문제)
  - 하위 문제: 사전 학습된 가중치를 활용해 본격적으로 학습하고자 하는 문제인 텍스트 유사도 문제


#### 사전 학습 모델을 이용하지 않고 학습하는 과정
- 데이터 셋 분석 및 전처리
  - 데이터 셋을 EDA(Exploratory Data Analysis, 탐색적 자료 분석)과정을 거쳐 분석, 토크나이징, 임베딩 등 전처리 수행
- 모델 설계, 구현 및 생성
  - 이떄 생성된 모델은 임의의 값으로 초기화 된 가중치를 가짐
- 모델 학습
  - 유사도 데이터를 활용해 학습 수행
 
#### 사전 학습 모델을 이용한 학습 과정

- 적절한 데이터 셋에 대하여 잘 학습된 사전 학습 모델 확보  
  - 이때 모델은 데이터 셋에 대하여 학습이 완료된 가중치를 가지고 있음
- 모델 학습
  - 가중치 초기화 등의 과정을 수행하지 않고
  - 기존에 학습이 완료된 가중치를 초기값으로 사용함
  - 현재 해결하고자 하는 문제에 대한 유사도 데이터를 활용하여 학습 수행
  - 단, 사전 학습 모델의 최종 출력 값을 뽑는 가중치 층은 제외
    - → 우리가 해결하고자 하는 문제와 관계없는 형태의 데이터가 포함되어 있기 때문

- 사전 학습된 가중치의 활용

![image](https://github.com/user-attachments/assets/afe047b5-9b96-4775-9458-1f5e665f5e9b)


- 언어 모델을 사전학습한 모델

![image](https://github.com/user-attachments/assets/0203c8f9-7d7d-4faa-be3c-6ecb41a92520)


- 사전 학습은 어떤 문제에도 적용이 가능함 (음악, 영상, 코딩 등)
- 대부분의 언어 처리 문제는 Label 이 주어지지 않는 비지도 학습 문제에 포함됨 → 데이터에 제약이 없고 언어에 대한 전반적인 이해를 사전 학습하는 언어 모델이 효율적
- 대부분의 경우, 하위 문제 모델의 성능을 향상시키는 결과를 가지고옴
- 언어 모델은 대규모 데이터를 활용한 사전 학습을 통해 언어에 대한 전반적인 이해를 하게 됨 → 이렇게 학습된 지식을 기반으로 하위 문제에 대한 성능을 향상시킴

#### 특징 기반(Feature-Based) 방법
- 사전 학습된 특징을 하위 문제의 모델에 부가적인 특징으로 활용하는 방법
- 대표사례: word2vec 
  - → 학습한 임베딩 특징을 우리가 학습하고자 하는 모델의 임베딩 특징으로 활용


#### 미세 조정(Fine-Tuning)
- 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가해서 모델을 추가로 학습(미세 조정)하는 방법
- 대표사례: 감정 분석 문제에 사전 학습 시킨 가중치와 더불어 텍스트 유사도를 위한 부가적인 가중치를 추 가하여 텍스트 유사도 문제를 학습하는 것

#### 트랜스포머 모델의 활용 및 확산

- 트랜스포머 모델 이후
  - 대부분 자연어 처리 연구에서 트랜스포머 모델을 기반으로 하는 비지도 사전 학습을 통해 학습한 많은 가중치를 활용해 자연어 처리 모델을 미세 조정하는 방법이 각광 받고 있음

- 대표적인 트랜스포머 기반 사전 학습 모델
  - GPT (Generative Pre-trained Transformer)
    - OpenAI에서 발표한 최초의 트랜스포머 기반 사전 학습 언어 모델(2018)
    - 현재 GPT-1, GPT-2를 거쳐 GPT-4까지 발표됨
    - https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf (최초 논문)
  - BERT (Bidirectional Encoder Representations from Transformer) <양방향 트랜스포머>
    - Google에서 발표한 트랜스포머 기반 사전 학습 언어 모델(2018)
    - https://arxiv.org/pdf/1810.04805.pdf




















































































































```
