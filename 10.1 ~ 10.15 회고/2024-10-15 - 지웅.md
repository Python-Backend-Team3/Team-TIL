# 📝 2024.10.15 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------
# Transformer 아키텍처

### 트랜스포머란?
- 2017 구글이 제안한 Sequence-to-Sequence 모델의 하나
- Attention is all you need라는 논문에서 제안된 모델
- 기존의 seq2seq 구조인 Encoder-Decoder 방식을 따르면서도 논문 제목처럼 Attention 만으로 구현한 모델임
- RNN 모델을 사용하지 않고 Encoder-Decoder를 설계 했음에도 번역 등 성능에서 더 우수한 성능 보임

#### 기존 seq2seq 모델 한계
- 기존의 seq2seq 모델은 인코더-디코더 구조로 구성
- 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 통해서 출력 시퀀스를 만들어 냄 → 이러한 구조는
  - 인코더가 입력 시퀀스를 하나의 벡터(고정길이)로 압축하는 과정에서
  - 입력 시퀀스의 정보가 일부 손실된다는 단점이 발생하였고
  - 이를 보정하기 위해 어텐션(Attention) 모델이 사용됨

### 트랜스포머의 등장
- RNN에서 사용한 순환방식을 사용하지 않고 순수하게 어텐션만 사용한 모델(중요하다 싶은 것만 사용)
- 기존에 사용되었던 RNN, LSTM, GRU 등은 점차 트랜스포머로 대체되기 시작
- GPT, BERT, T5 등과 같은 다양한 자연어 처리 모델에 트랜스포머 아키텍처가 적용됨


#### 트랜스포머의 인코더-디코더 구조
- 인코더
  - 소스 시퀀스의 정보를 압축해 디코더로 보냄
- 디코더
  - 인코더가 보내 준 소스 시퀀스의 정보를 받아서 타겟 시퀀스 생성

![image](https://github.com/user-attachments/assets/fc3e1233-0529-4bec-823a-777b49a19b91)

- 트랜스포머는 Sequence to Sequence 형태의 과제 수행에 특화된 모델.
- 임의의 시퀀스를 해당 시퀀스와 속성이 다른 시퀀스로 변환하는 작업이라면 꼭 기계 번역 이 아니라도 수행할 수 있음
- 예: 필리핀 앞바다의 한 달 치 기온 데이터 → 앞으로 1주일간 하루 단위로 태풍이 발생할지를 맞히는 과제(기온의 시퀀스 → 태풍발생 여부의 시퀀스)도 트랜스포머가 할 수 있는일임

#### 트랜스포머의 학습 방법
- 1. I를 예측하는 학습
- 인코더 입력: 어제, 카페, 갔었어, 거기, 사람, 많더라 → (소스 시퀀스 전체)
  - 소스 시퀀스를 압축하여 디코더로 보내고
- 디코더 입력: <S>
  - 인코더에서 보내온 정보와 현재 디코더 입력을 모두 고려하여 토큰(I)를 예측
- 디코더의 최종 출력:
  - 타깃 언어의 어휘 수만큼의 차원으로 구성된 벡터 → 이 벡터는 요소의 값이 모두 확률 값 (예: 타깃 언어의 어휘가 총 3만개라면 디코더 출력은 3만 차원의 벡터, 3만개 각각은 확률값)

![image](https://github.com/user-attachments/assets/c1623be0-7d72-4cbc-89ba-10513164a65b)

- 2. 트랜스포머의 학습 진행
- 인코더와 디코더의 입력이 주어졌을 때, 정답에 해당하는 단어의 확률값을 높이는 방식으로 학습함
- → 그림에서...
  - 모델은 이번 시점의 정답인 I에 해당하는 확률은 높이고, 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신

![image](https://github.com/user-attachments/assets/b1830c27-b66c-4205-b58a-b2bbcf1173a7)

- 3. went 를 예측할 차례 (y target으로 went가 들어감)
- 인코더 입력: 어제, 카페, 갔었어, 거기, 사람, 많더라 → (소스 시퀀스 전체)
- 디코더 입력: <S> I
   - 특이사항
     - 학습 중의 디코더 입력과 학습을 마친 후 모델을 실제 기계번역에 사용할 때(인퍼런스)의 디코더 입력이 다름
     - 학습 중: 디코더 입력에 예측해야 할 단어(went) 이전의 정답 타깃 시퀀스(<s> I)를 넣어줌
     - 학습 후(인퍼런스 때): 현재 디코더 입력에 직전 디코딩 결과를 사용 (예: 인퍼런스 때 직전 디코더 출력이 I 대신 you가 나오면 다음 디코더 입력은 <s> you)

![image](https://github.com/user-attachments/assets/41ad70b7-288f-4bda-9906-66bec070c025)

- 4. ‘went’확률 높이기
- 학습 과정 중 인코더, 디코더의 입력이 아래 그림과 같으면...
- 모델은 이번 시점의 정답인 went에 해당하는 확률은 높이고
- 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신함

![image](https://github.com/user-attachments/assets/ecaab281-b8f8-45d3-8d75-79f17cff53c8)

- 5. ‘to’ 예측하기
- 인코더 입력은 소스 시퀀스 전체
- 디코더 입력은 정답인 <s> I went
- 인퍼런스 할 때, 디코더 입력은 직전 디코딩 결과
- 이번 시점의 정답인 to에 해당하는 확률은 높이고
- 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신함

![image](https://github.com/user-attachments/assets/a68347fe-10b8-472d-b734-5accb410c1ed)
- 이상의 방식으로 말뭉치 전체를 반복해서 학습

![image](https://github.com/user-attachments/assets/049b50e7-5b00-44d1-bc26-354bcad51796)


### 트랜스포머 블록

- 트랜스포머는 블록 형태로 구성된 인코더, 디코더 수십개가 반복적으로 쌓여 있는 형태
- 이런 구조를 블록 또는 레이어라고 부름
- 인코더 블록: 3가지 요소로 구성
  - 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 & 레이어 정규화
- 디코더 블록: 인코더 블록이 변형된 형태
  - 마스크 멀티 헤드 어텐션 추가됨

![image](https://github.com/user-attachments/assets/4a05560a-f40b-4a94-8b5a-246dedf65ec7)
















































































































































```
