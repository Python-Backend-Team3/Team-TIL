# 📝 2024.10.15 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------
# Transformer 아키텍처

### 트랜스포머란?
- 2017 구글이 제안한 Sequence-to-Sequence 모델의 하나
- Attention is all you need라는 논문에서 제안된 모델
- 기존의 seq2seq 구조인 Encoder-Decoder 방식을 따르면서도 논문 제목처럼 Attention 만으로 구현한 모델임
- RNN 모델을 사용하지 않고 Encoder-Decoder를 설계 했음에도 번역 등 성능에서 더 우수한 성능 보임

#### 기존 seq2seq 모델 한계
- 기존의 seq2seq 모델은 인코더-디코더 구조로 구성
- 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 통해서 출력 시퀀스를 만들어 냄 → 이러한 구조는
  - 인코더가 입력 시퀀스를 하나의 벡터(고정길이)로 압축하는 과정에서
  - 입력 시퀀스의 정보가 일부 손실된다는 단점이 발생하였고
  - 이를 보정하기 위해 어텐션(Attention) 모델이 사용됨

### 트랜스포머의 등장
- RNN에서 사용한 순환방식을 사용하지 않고 순수하게 어텐션만 사용한 모델
- 기존에 사용되었던 RNN, LSTM, GRU 등은 점차 트랜스포머로 대체되기 시작
- GPT, BERT, T5 등과 같은 다양한 자연어 처리 모델에 트랜스포머 아키텍처가 적용됨













































































































































































```
