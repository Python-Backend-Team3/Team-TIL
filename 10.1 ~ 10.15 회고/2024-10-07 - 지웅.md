# 📝 2024.10.07 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 자연어 처리 기술

- 현재 일반적인 AI기술은 하나의 개체(인간)이 가지는 생물학적/기계적인 기능을 흉내 낸 것

#### 언어의 등장
- 인간은 사회적인 동물 이다.
  - 인간은 크고 작은 사회, 조직 안에서 지식과 지혜를 세대를 거쳐가며 누적 시킴으로 문명을 이루며 본능과 다른 지성을 성립시켜 옴
  - 이 과정에서 사람 사이의 소통이 요구됨
  - 이 커뮤니케이션을 위해 발생한 언어, 즉 자연어(Natural Language)

#### 자연어의 발생
- 인간이 스스로의 지식을 구조화 할 수 있게 진화 시킴
- 인간의 사고 행위는 언어(자연어)로 구성됨

- 언어의 발생 의의
  - 인간의 진화는 매우 더디게 진화 -> 언어의 발생과 함께 폭발적 진화
  - 언어 발생 이전 사고 -> 단순한 동물이 보이는 기계적 반응에 그침
  - 언어 사용 -> 지식 구조화 성립, 현대적인 의사소통 능력 발생
- 인간의 사고 능력은 생물학적 기능의 모방으로 구현할 수 없으며 사고 능력은 언어를 기반으로 성립함


- 인간의 지능, 지성은 복합적으로 구성됨
- 하나의 개체에만 적용되는 지능, 지성, 사회를 구성하는 집단에서 발생하는 집단 지성
- 이러한 모든것을 연결하는 핵심이 언어(자연어)다.
- 언어를 컴퓨터/AI가 처리할 수 있게 되면서 단순 산업, 계산, 예측을 위한 시스템에 인간이 쌓아 올린 문명과 문화가 반영되기 시작했다.

## 자연어 처리 기술의 개요
- 프로그래밍 언어와 같은 인공적으로 만든 언어가 아닌, 사람이 일상생활과 의사소통에 사용해 온, 한국어, 영어와 같이 오랜 세월 걸쳐 자연적으로 만들어진 언어라는 의미

#### 자연어 처리란?
- 컴퓨터가 인간의 언어를 알아들을 수 있도록 인간의 언어를 분석 해석해 처리하는 인공지능의 분야
- 자연어를 컴퓨터로 해석하고, 의미를 분석해 이해하고, 자동으로 생성하는 것 등에 관련된 분야
- 자연어 처리의 목표는 "어떻게 자연어를 컴퓨터에게 인식시킬 수 있을까?"라는 것

- 자연어에 대한 연구는 오래전부터 이어지고 있음에도 현재까지도 컴퓨터가 자연어를 사람처럼 이해하지 못함
- 언어에 대한 깊은 이해없이는 피상적인 확률 및 통계를 이용하여 대량의 정보를 처리하는 기술은 많이 발전한 상태이다. (검색 엔진: 단어 간 통계적 유사성 바탕으로 문서 검색)

###### 컴퓨터에 일 시키는 과정
- 기존 컴퓨터 기술에서는 컴퓨터에게 일을 시키기 위해 사람이 컴퓨터 언어를 학습했는데, AI기술을 사용하면 컴퓨터에게 일 시키기 위해 컴퓨터가 사람의 언어를 학습(1대1 매칭)
![image](https://github.com/user-attachments/assets/48da3d84-ac04-4bdc-9b60-907baa0d1d67)


#### 자연어 처리에 요하는 기술
- 인공지능 기술
- 컴퓨터 공학 기술
- 언어학적 기술
  - 광범위한 기술 분포로 인해 쉽게 접근하기 어려움
  - 제대로 기술을 익히고자 한다면 연구/개발에 투자하고자 하면 필수이다.

#### 자연어처리 대표적 분야
- 텍스트 분류
- 텍스트 유사도
- 자연어 생성 (-> 텍스트 생성)
- 기계 이해 (-> 텍스트 이해)

- 4대 문제를 이해하려면 먼저 "단어의 표현"에 대해 이해하여야 함
  - 단어의 표현 (모든 자연어 처리 문제의 기본, 자연어를 어떻게 표현할 것인지 결정하는 것이 문제에 대한 해결의 출발점임)

#### 컴퓨터의 텍스트 인식
- 컴퓨터가 인식할 수 있는 것은 전자 소재의 on/off 상태값 뿐이다.
- 사람 기준에 좀 더 보기 쉽게, 산술적(수학적)으로 표현하기 쉽게 하기 위해 0, 1 의 값으로 대체(변환)해서 표현했다.
- 컴퓨터는 자연어를 이해하지 못하며, on/off 2진수 표현을 기반으로 한 수치 데이터만 사용 가능 -> 단어의 수치화가 필요 -> 통계의 사용이 가능해짐 
- 단어의 수치화 결과는 벡터로 표시 -> 단어 임베딩, 단어 벡터 등으로 표현된다.

###### 단어의 수치화를 위한 대표 방법
- 원-핫 인코딩
  - 각 단어에 인덱스 부여해 각 단어의 벡터에서 해당 인덱스를 1, 나머지는 0

![image](https://github.com/user-attachments/assets/bb4d3804-8f6a-4566-98f9-23c0770c0d5f)

- 분포가설 기반의 인코딩
  - 분포가설 : 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가짐
  - 어떤 글에서 비슷한 위치에 존재하는 단어는 단어 간의 유사도가 높다고 판단
  - 카운트 기반 방법, 예측 방법
 
##### 벡터화 방법의 장단점
- 원-핫 인코딩
  - 간편하고 이해 쉬움
  - 처리할 단어의 수가 많을 수록 벡터의 크기 급증 -> 매우 비효율적(희소벡터)
  - 단어가 무엇인지만 확인 가능해 단어의 의미, 특성, 관계 등의 정보는 표현되지 않음
 
- 분포가설 기반의 인코딩
  - 각 단어 사이 유사도 및 관계성, 특징 등 표현
  - 예측 방법의 경우 성능이 뛰어나므로 가장 많이 활용됨 

##### 분포가설 기반의 인코딩
- 카운트 기반 방법
  - 특정 문맥 안 단어들이 동시에 등장하는 횟수를 직접 세어, 횟수를 행렬로 나타내고, 그 행렬을 수치화해 단어 벡터를 만드는 방법
  - 특이앖 분해, 잠재 의미 분석, Hyperspace Analogue to Language(HAL), Hellinger PCA(Principal Component Analysis, 주성분 분석) 

- 예측 방법 
  - 신경망 또는 통계모델 등을 통해 문맥 안 단어들을 예측하는 방법
  - Word2Vec, NNLM(Neural Network Language Model), RNNLM(Recurrent NNLM)
  - 대표는 Word2Vec(임베딩 모델임)를 이용해 단어를 벡터로 바꿈, Word2Vec은 CBOW, SKip-Gram모델로 구분됨

### 텍스트 분류
##### 텍스트 분류 문제
  - 자연어 처리 문제 중 가장 대표적 문제
  - 자연어 처리 기술을 활용해 특정 텍스트를 사람들이 정한 몇 가지 범주 중 어느 범주에 속하는지 분류하는 문제
  - 스팸 분류, 감정 분류, 뉴스 기사 분류

- 지도학습 을 통한 분류
  - 선형 분류
  - 신경망
  - 나이브 베이즈 분류
  - 서포트 벡터 머신
  - 로지스틱 분류
  - 랜덤 포레스트 등 

![image](https://github.com/user-attachments/assets/f85b5233-034c-47ad-b8b3-bbda1aa318bc)

- 비지도학습을 통한 분류 (알고리즘 만들고하는 과정은 없고, 데이터 특성이 비슷한거끼리 모음)
  - k-평균 군집화
  - 계층적 군집화

![image](https://github.com/user-attachments/assets/6a71a04f-390b-4a71-9555-9113c9229f51)


##### 텍스트 유사도
- 텍스트가 얼마나 유사한지 표현하는 방법
- 이 노래 누가 만들었어?, 지금 나오는 노래 작곡가가 누구야? 이런거
- 예시 같은 두 문장이 얼마나 유사한지 정량화 해 수치로 표현하는 모델을 만드는 것이 중요함

- 유사도 측정을 위한 정량화 방법
  - 같은 단어의 개수를 사용하여 판단하는 방법(카운팅)
  - 형태소로 나눠 형태소 비교를 하는 방법
  - 문장, 단어를 자소 단위로 나눠 각 단어를 비교하는 방법
  - 딥러닝 기반으로 측정하는 방법

- 많이 사용되는 유사도 측정 방법
  -  자카드 유사도, 유클리디언 유사도, 맨해튼 유사도, 코사인 유사도  

### 자연어 생성
- 자연어 생성, 텍스트 생성, 문장 생성 등으로 표현
- 뉴스 기사 생성, 대화 생성, 챗봇 등 다양한 영역에서 활용
- 인간과 컴퓨터 사이의 의사소통을 위한 가장 자연스로운 방식
- 언어모델 기반(최근), 패턴 기반(예전)의 자연어 생성 으로 형태를 나눠서 생각할 수 있다.

- 언어 모델을 이용한 방식
  - OpenAI의 GPT, Naver의 HyperClova 등 다양한 언어 모델 존재
  - 수많은 데이터의 학습을 통해 어떤 표현이 가장 적절한지 예측하여 문장 생성
    - GPT3는 약 1750억개의 파라미터 데이터를 이용해 학습을 진행   

- 언어 모델을 이용한 방식의 문제점
  - 가장 자연스로운 문장을 만드는 것에 강점을 가짐
  - 어떤 데이터를 기반으로 정확한 사실을 전달하는 문장을 만들지 못함
  - 뉴스 기사를 언어모델로 작성하면, 사람이 쓴 것같은 자연스로운 기사를 쓸 수 있지만 데이터 기반으로 하지 않기 때문에 가짜 뉴스가 만들어짐

- 패턴 기반의 방식의 문제점

### 기계 이해 (-> 텍스트 이해)

- QNA문제
- 기계까 텍스트를 이해하고 논리적으로 추론을 할 수 있는지를 데이터 학습을 통해 구현, 확인하는 것
- 앞서 설명한 텍스트 분류, 유사도, 자연어 생성 등 자연어 처리에 대한 전반적인 내용이 모두 포함되어 있다 볼 수 있음

- 자연어 처리의 활용 분야
  - 대량의 텍스트를 이해하고 수치화 하는 분야
    - 감성분석, 문서분류, 문서요약
  - 사용자 의도를 파악하고 대화하거나 도움을 주는 분야
  -   AI 스피커, 
   

### 딥러닝 적용 이전의 자연어 처리 방식

![image](https://github.com/user-attachments/assets/07307353-5cb2-4076-84d0-caec90ec46df)

### 자연어 처리(분석중심)의 일반적인 단계

![image](https://github.com/user-attachments/assets/b00eca9d-b580-4f24-83a6-3e640b9cedbd)

### 어휘 분석
- 기술 구성
  - Sentence Splitting : 마침표, 느낌표, 물음표 등을 기준으로 분리
  - Tokenizing : 문서나 문장을 분석하기 좋도록 나눔(띄어쓰기 또는 형태소 단위)
  - Morphological : 토큰들을 좀 더 일반적인 형태로 분석해 단어 수를 줄임으로 분석의 효율성을 높임(가장 작은 의미 단위로 토큰화 진행)
  - Stemming(어간추출, 형태소 분석) : cars, car -> car
  - = Lemmatization : 단어를 원형으로 표현
- 필요성 : 입력된 문장을 잘 분할해서 효율성을 높이기 위함


#### 구문분석(Syntax Analysis)
- 각각의 어절 단위로 구분, 해당 태그 부여(Parsing Tree 이용)

#### Semantic Analysis (의미분석)
- Syntactic + Meaning
- 문장의 의미에 근거해서 그 문장을 해석하는 방법
- 여러의미 분석 방법과 다양한 유형의 문법 이용
- 문장이 어떻게 구성되었는지 나타내는 규칙들로 구성된 일종의 형식 시스템
- 필요성
  - 규칙에 따라 문장은 만들었는데, 문장이 의미적으로 올바른 것인지 확인해야 함

#### 실용분석 Pragmatic Analysis
- 실용주의
  - 인간의 행동과 실제적 측면에 대한 연구
  - 실제 상황에서의 언어, 표시, 단어 및 문장의 사용에 대한 연구
- 실용적인 상호 작용 컨텍스트(문맥, 맥락)에서 의미 연구 가리킴
 - 발언의 문자적 의미 넘어 그 의미가 어떻게 구성되는지 대한 것만 아닌 묵시적 의미에 초점(상호작용의 도구로 언어 선택)

- 대화 흐름 상 어떤 의미 가지는 지 탐색
- 문맥 구조 분석 : 문장 사이 연관 관계 분석
- 의도 분석 : 전후 관계를 통한 

![image](https://github.com/user-attachments/assets/a84c82f0-f924-4919-8d3a-a7a124096103)

- 실용주의 관점에서 고려할 사항
  - 연사와 청취자 사이 의미 전달, 협상
  - 발언에 대한 맥락
  - 발언이 가지는 의미 및 의미에 대한 잠재력
 
- 필요성 : 대화 흐름을 파악해 발화자 의도에 맞도록 응답


### 딥러닝 기술의 등장에 따른 변화
  - 딥러닝 기술 적용 이전 자연어 처리 방식 문제
  - 여러 단계 모듈로 구성(디자인 복잡, 상황데 따라 또 다른 서브 모듈 추가 빈번)
  - 각각의 모듈이 완벽하게 동작하지 않음 ( 각기 발생한 오차 중첩, 가중으로 뒤로 전파되는 오차의 전파 현상 발생)
  - 시스템이 매우 무겁고 복잡해 구현 및 시스템의 구성이 어려움

- 딥러닝 기술의 적용과 함께 변화 -> 각 하위 모듈의 딥러닝 모델화로 시작 -> 점차 End-to-End 모델로 대체

![image](https://github.com/user-attachments/assets/ead9a416-3791-45c7-a1eb-d8ddc9697ca4)

![image](https://github.com/user-attachments/assets/3d36b396-eb74-432f-a3d3-ae16114d4d6d)

### 딥러닝 기술의 등장에 따른 변화
- Word2Vec 등의 임베딩 기술 적용
  - 단어(토큰)을 연속적 벡터로 표현 -> 모호성 유의성 문제 개선

- 딥러닝 적용
  - End-to-End 방식의 모델 적용 -> 성능 향상
  - 개선된 RNN 계열 모델(LSTM, GRU)활용 고도화
  - 주의 모델(Attention)적용 -> 긴 길이의 시퀀셜 데이터를 이용한 학습 용이

- 딥러닝 기반 자연어 처리 과정
![image](https://github.com/user-attachments/assets/db79cfe9-4050-4bd5-9ebe-d529054732b5)

#### 자연어 처리를 위한 언어학적 요소

- 음성학 & 음운론
  - 언어의 소리가 물리적으로 어떻게 형성되는지에 대한 이산적인 소리체계에 대한 연구
  - 소리를 기준으로 분석하기 때문에 발생하기 쉬운 문제(유사한 발음을 가진 전혀 다른 문장의 경우, 인식 오류가 발생하기 쉽다)

- 음성인식
  - 음성의 파형을 기호화해 인식하는 음성학, 음운론 기반의 연구분야 (음소:더이상 작게 나눌 수 없는 음운론 상의 최소 단위)
 
####  형태론
- 어절 : 양쪽 공백을 가진 띄어쓰기 단위의 문자열
- 단어 : 단일 품사를 가지는 단위
- 형태소 : 의미를 가지는 언어 단위 중 가장 작은 단어 (한국어만 있음)
  - 의미 또는 문법적 기능의 최소 단위(단어일수도 단어 아닐수도 있음)
  - 사전에 등록되어 있는 색인어의 집합  

- 형태소 분석
  - 형태소를 비롯하여 어근, 접두(미)사, 품사 등 다양한 언어적 속성의 구조를 파악
  - 입력된 문자열을 분석하여 형태소라는 최소 의미 단위로 ㅈ분리
 
#### 구문론
- 문법 : 문장의 구조적 성질을 규칙으로 표현한것
  - 각 규칙을 이용해 문장을 생성, 분석 

- 구문 분석:
  - 문법을 이용해 문장의 구조를 찾는 과정
  - 문장 구조는 트리 형태로 표현 가능함
 
#### 의미분석
- 통사 분석 결과에 해석을 가미해 문장이 가진 의미 분석
- 형태소가 가진 의미를 표현하는 지식 표현 기법이 요구
- 통사적으로 옳으나 의미적으로 틀린 문장이 있을 수 있음(시적 표현을 통해 통용되는 경우가 많음, 돌이 걸어간다, 바람이 달린다.)
- 모호성 등의 어려움이 있음(말이 많다. 같은것)

#### 실용분석
- 문장과 실세계가 가지는 연관관계 분석
- 실세계의 지식과 상식의 표현이 요구
- 지시(대명사 등의 지시 대상, 상대방에서 행동을 요하는것), 간접화법 등 분석

### 자연어 처리가 어려운 이유
#### 모호성(중의성)
- 단어의 중의성에 따라 발생하는 특징, 하나의 표현이 여러 의미를 가질 수 있는 성질

- 언어는 계속 진화하고, 특히 효율성을 극대화하는 방향으로 진화하기에
- 최대한 짧은 문장 내 많은 정보를 담고자 하기 위해
- 생략된 문맥을 인간은 여러 지식을 이용해 효율적으로 채울 수 있지만, 기계는 이러한 작업에 매우 취약함

- 모호성의 예시
  - ![image](https://github.com/user-attachments/assets/5c4e053e-7531-466d-a78c-bf924362bf31)
  - 차를 표현한 단어들 :tea, car, kick, dumped
  - 일부 표현을 빠뜨리거나 일부는 단어를 잘못 선택함 -> 정확한 번역은 찾기 어려움

- 문장 내 정보 부족에 따른 예시
![image](https://github.com/user-attachments/assets/c1420049-6695-4aac-bb86-8672ea02d061)

- 다양한 표현 : 여러 상황을 표현, 묘사하기 위해 다양한 표현이 사용되지만 그 의미는 동일한 경우가 매우 많음

- 불연속적 데이터
  - 기존 자연어 처리 : 이산 데이터 대상임으로 처리가 쉬움
  - 딥러닝 기반 : 데이터를 연속적인 값으로 바꿔줘야함

- 차원의 저주(대표적으로 : 원 핫 인코딩에 의해 만들어짐)
  - 불연속 데이터이므로 많은 종류의 데이터를 표현하려면 데이터 종류만큼의 대규모 차원이 필요, 어휘 크기만큼의 차원 요구됨

- 노이즈와 정규화
  - 노이즈가 제대로 분리되지 않거나 영향력이 커지면 데이터의 원래 의미까지 손상 가능
  - 자연어 데이터는 불연속 데이터(심볼)임으로 완전 다른 의미가 되어버릴 수 있음
  - 띄어쓰기, 어순 차이 등에 의한 정제(정규화)이슈도 큰 어려움 요소이다.

 - 한국어 자연어 처리가 어려운 이유
  - 과거에는 우랄알타이어족 이라 하다, 교착어에 속했다 했는데, 최근 분류가 백지화됨
![image](https://github.com/user-attachments/assets/298267c8-6610-47d8-9199-e5a95ad06582)

- 접사에 따라 단어의 역할 결정 -> 어순이 크게 중요하지 않음
- 문장을 끊어서 사용한다면 문제 없는 경우도 발생한다.
- 띄어쓰기 : 한국어는 근대 띄어쓰기가 도입외어 문장에서 띄어쓰기의 중요도가 낮음(영어는 띄어쓰기만으로 단어의 분리가 가능한데 한국어는 불가능함)

- 현서문과 의문문이 같은 형태의 문장구조를 가지는 경우가 많음, 물음표가 없으면 알 수 없는 경우가 많아서
- 주어생략 : 영어는 명사가 중요해 주어 생략 x, 한국어는 동사를 중시해 주어 자주 생략(문맥 정보를 이용해 생략된 정보를 메꿈(컴퓨터가 취약한 부분))

- 제일 문제되는 부분
  - 한자 기반 단어 : 한자를 조합해 만들어진 단어가 많음
  - 한자 기반 단어는 각 글자가 의미를 가지고, 그 의미들이 합쳐져 하나의 단어의 뜻을 나타냄
  - 한글이 한자를 대체하면서 문제가 발생한다
  - 한자는 표어 문자, 한글은 표음문자
    - 표어문자 : 하나하나의 문자가 하나의 말, 단어, 형태소를 이루는 문자 시스템
    - 표음문자 : 문자에서 각 글자가 특정 의미를 가지지 않고, 단지 각 음성에 대응하여 발음을 나타내는 문자 시스템

  - 한자(표어문자) -> 한글(표음문자) -> 정보의 손실 발생 -> 모호성 등의 문제 유발
    - 인간은 정보 손실로 발생한 모호성을 문맥의 이해로 해석 가능(기계는 어려움)
  - 다른 언어보다 중의성에 따른 문제가 더 가중되고 있다.


- 서브 워드 단위로 단어를 분절할 경우 가중되는 중의성 문제 사례

### 자연어 처리의 최근 변화 추세
- 2010. RNN을 활용한 언어모델 시도 (기존 n-gram 기반 언어모델 한계 극복 시도)
- 2013. 구글, word2vec 발표(단순한 구조의 신경망 사용, 단어를 효과적으로 잠재공간에 투사(비슷한 의미의 단어일수록 저차원의 잠재공간에서 가깝게 위치), 딥러닝 기반 자연어 처리 시, 네트워크 내부는 어떤 식으로 동작하는지에 대한 인사이트 획득)
- 2014. yoon kim, cnn만을 활용한 텍스트 분류 모델 제시(cnn 만을 활용해 기존 텍스트 분류보다 성능을 끌어올린 텍스트 분류 모델 제시, 단어 임베딩 벡터와 결합해 성능의 극대화를 이끌어냄)
- 2014, seq2weq(순차열-> 순차열)ahepf, attention(주의) 기법 개발(성공적 기계번역 적용, 주어진 정보 기반 자유롭게 문장을 생성하는 자연어 생성(NLG)가 가능해짐, 기계번역 외 문장요약, 챗봇 시도 증가, 최초로 상용화 시작      

- Attention 기법 성공 이후 연속적 방식 정보를 읽고 쓰는 기법에 대한 관심 증가(메모리 증가 신경망(MANN)확산 : 뉴런 튜링 머신, 차별화 가능한 뉴럴 컴퓨터 등장

#### 강화학습의적용
- 딥러닝 기계번역 분야 적용에 대한 문제점
  - 딥러닝 손실 함수와 기계번역에서의 목적 함수 사이의 괴리
  - 영상 처리분야의 경우, 기존 MSE(평균제곱오차)손실 함수의 한계를 벗어나기 위해 GAN모델을 도입하여 생성 모델 구현
  - 자연어 생성에도 GAN모델에 강화학습 기법을 반영한 SeqGAN 등의 모델 제시(강화학습의 폴리시 그래디언트 기법을 자연어 생성에 적용 성공
- 강화 학습을 이용해 실제 자연어 생성에서의 목적 함수로부터 보상을 받을 수 있게 됨(사람이 생성한 것과 유사한 문장 생성 능력의 극대화 유도)

## 자연어 처리를 위한 전 처리 과정
- 자연어 처리에서 가장 중요한 것은 "전처리 과정"이다.

- 일반적인 전처리 과정
![image](https://github.com/user-attachments/assets/9a86f372-0041-4816-92a9-2260bdcf3592)

#### 코퍼스(Corpus, 말뭉치)
- 여러 단어로 이루어진 문장을 가리킴
- 자연어 처리를 위해 머신러닝/딥러인 수행하려면 훈련데이터가 필요, 다수의 문장으로 구성된 코퍼스가 훈련데이터로 사용됨
- 코퍼스 많고, 오류 없을수록 자연어 처리 모델은 더욱 정교해지고 정확도가 높아짐

- 코퍼스 분류
  - 구성 목적에 따른 분류
    - 단일 언어 코퍼스(base)
    - 이중 언어 코퍼스(번역)
    - 다중 언어 코퍼스(번역)  
  - 구성 방법에 따른 분류
    - 병렬 코퍼스 : 각 언어가 서로 쌍으로 구성되는 코퍼스(번역에 쓰임)

- 코퍼스 수집
  - 공개 데이터 사용
    - 감성 분석 등 텍스트 분류 데이터, 기계번역을 위한 언어 쌍 데이터 등 각종 대회 또는 논문을 위한 데이터가 주류
    - 데이터의 분량과 내용의 분야가 한정적
- 유료 데이터 구매
  - 비용문제 발생













```











