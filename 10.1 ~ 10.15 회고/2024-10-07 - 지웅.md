# 📝 2024.10.07 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 자연어 처리 기술

- 현재 일반적인 AI기술은 하나의 개체(인간)이 가지는 생물학적/기계적인 기능을 흉내 낸 것

#### 언어의 등장
- 인간은 사회적인 동물 이다.
  - 인간은 크고 작은 사회, 조직 안에서 지식과 지혜를 세대를 거쳐가며 누적 시킴으로 문명을 이루며 본능과 다른 지성을 성립시켜 옴
  - 이 과정에서 사람 사이의 소통이 요구됨
  - 이 커뮤니케이션을 위해 발생한 언어, 즉 자연어(Natural Language)

#### 자연어의 발생
- 인간이 스스로의 지식을 구조화 할 수 있게 진화 시킴
- 인간의 사고 행위는 언어(자연어)로 구성됨

- 언어의 발생 의의
  - 인간의 진화는 매우 더디게 진화 -> 언어의 발생과 함께 폭발적 진화
  - 언어 발생 이전 사고 -> 단순한 동물이 보이는 기계적 반응에 그침
  - 언어 사용 -> 지식 구조화 성립, 현대적인 의사소통 능력 발생
- 인간의 사고 능력은 생물학적 기능의 모방으로 구현할 수 없으며 사고 능력은 언어를 기반으로 성립함


- 인간의 지능, 지성은 복합적으로 구성됨
- 하나의 개체에만 적용되는 지능, 지성, 사회를 구성하는 집단에서 발생하는 집단 지성
- 이러한 모든것을 연결하는 핵심이 언어(자연어)다.
- 언어를 컴퓨터/AI가 처리할 수 있게 되면서 단순 산업, 계산, 예측을 위한 시스템에 인간이 쌓아 올린 문명과 문화가 반영되기 시작했다.

## 자연어 처리 기술의 개요
- 프로그래밍 언어와 같은 인공적으로 만든 언어가 아닌, 사람이 일상생활과 의사소통에 사용해 온, 한국어, 영어와 같이 오랜 세월 걸쳐 자연적으로 만들어진 언어라는 의미

#### 자연어 처리란?
- 컴퓨터가 인간의 언어를 알아들을 수 있도록 인간의 언어를 분석 해석해 처리하는 인공지능의 분야
- 자연어를 컴퓨터로 해석하고, 의미를 분석해 이해하고, 자동으로 생성하는 것 등에 관련된 분야
- 자연어 처리의 목표는 "어떻게 자연어를 컴퓨터에게 인식시킬 수 있을까?"라는 것

- 자연어에 대한 연구는 오래전부터 이어지고 있음에도 현재까지도 컴퓨터가 자연어를 사람처럼 이해하지 못함
- 언어에 대한 깊은 이해없이는 피상적인 확률 및 통계를 이용하여 대량의 정보를 처리하는 기술은 많이 발전한 상태이다. (검색 엔진: 단어 간 통계적 유사성 바탕으로 문서 검색)

###### 컴퓨터에 일 시키는 과정
- 기존 컴퓨터 기술에서는 컴퓨터에게 일을 시키기 위해 사람이 컴퓨터 언어를 학습했는데, AI기술을 사용하면 컴퓨터에게 일 시키기 위해 컴퓨터가 사람의 언어를 학습(1대1 매칭)
![image](https://github.com/user-attachments/assets/48da3d84-ac04-4bdc-9b60-907baa0d1d67)


#### 자연어 처리에 요하는 기술
- 인공지능 기술
- 컴퓨터 공학 기술
- 언어학적 기술
  - 광범위한 기술 분포로 인해 쉽게 접근하기 어려움
  - 제대로 기술을 익히고자 한다면 연구/개발에 투자하고자 하면 필수이다.

#### 자연어처리 대표적 분야
- 텍스트 분류
- 텍스트 유사도
- 자연어 생성 (-> 텍스트 생성)
- 기계 이해 (-> 텍스트 이해)

- 4대 문제를 이해하려면 먼저 "단어의 표현"에 대해 이해하여야 함
  - 단어의 표현 (모든 자연어 처리 문제의 기본, 자연어를 어떻게 표현할 것인지 결정하는 것이 문제에 대한 해결의 출발점임)

#### 컴퓨터의 텍스트 인식
- 컴퓨터가 인식할 수 있는 것은 전자 소재의 on/off 상태값 뿐이다.
- 사람 기준에 좀 더 보기 쉽게, 산술적(수학적)으로 표현하기 쉽게 하기 위해 0, 1 의 값으로 대체(변환)해서 표현했다.
- 컴퓨터는 자연어를 이해하지 못하며, on/off 2진수 표현을 기반으로 한 수치 데이터만 사용 가능 -> 단어의 수치화가 필요 -> 통계의 사용이 가능해짐 
- 단어의 수치화 결과는 벡터로 표시 -> 단어 임베딩, 단어 벡터 등으로 표현된다.

###### 단어의 수치화를 위한 대표 방법
- 원-핫 인코딩
  - 각 단어에 인덱스 부여해 각 단어의 벡터에서 해당 인덱스를 1, 나머지는 0

![image](https://github.com/user-attachments/assets/bb4d3804-8f6a-4566-98f9-23c0770c0d5f)

- 분포가설 기반의 인코딩
  - 분포가설 : 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가짐
  - 어떤 글에서 비슷한 위치에 존재하는 단어는 단어 간의 유사도가 높다고 판단
  - 카운트 기반 방법, 예측 방법
 
##### 벡터화 방법의 장단점
- 원-핫 인코딩
  - 간편하고 이해 쉬움
  - 처리할 단어의 수가 많을 수록 벡터의 크기 급증 -> 매우 비효율적(희소벡터)
  - 단어가 무엇인지만 확인 가능해 단어의 의미, 특성, 관계 등의 정보는 표현되지 않음
 
- 분포가설 기반의 인코딩
  - 각 단어 사이 유사도 및 관계성, 특징 등 표현
  - 예측 방법의 경우 성능이 뛰어나므로 가장 많이 활용됨 

##### 분포가설 기반의 인코딩
- 카운트 기반 방법
  - 특정 문맥 안 단어들이 동시에 등장하는 횟수를 직접 세어, 횟수를 행렬로 나타내고, 그 행렬을 수치화해 단어 벡터를 만드는 방법
  - 특이앖 분해, 잠재 의미 분석, Hyperspace Analogue to Language(HAL), Hellinger PCA(Principal Component Analysis, 주성분 분석) 

- 예측 방법 
  - 신경망 또는 통계모델 등을 통해 문맥 안 단어들을 예측하는 방법
  - Word2Vec, NNLM(Neural Network Language Model), RNNLM(Recurrent NNLM)
  - 대표는 Word2Vec(임베딩 모델임)를 이용해 단어를 벡터로 바꿈, Word2Vec은 CBOW, SKip-Gram모델로 구분됨

### 텍스트 분류
##### 텍스트 분류 문제
  - 자연어 처리 문제 중 가장 대표적 문제
  - 자연어 처리 기술을 활용해 특정 텍스트를 사람들이 정한 몇 가지 범주 중 어느 범주에 속하는지 분류하는 문제
  - 스팸 분류, 감정 분류, 뉴스 기사 분류

- 지도학습 을 통한 분류
  - 선형 분류
  - 신경망
  - 나이브 베이즈 분류
  - 서포트 벡터 머신
  - 로지스틱 분류
  - 랜덤 포레스트 등 

![image](https://github.com/user-attachments/assets/f85b5233-034c-47ad-b8b3-bbda1aa318bc)

- 비지도학습을 통한 분류 (알고리즘 만들고하는 과정은 없고, 데이터 특성이 비슷한거끼리 모음)
  - k-평균 군집화
  - 계층적 군집화

![image](https://github.com/user-attachments/assets/6a71a04f-390b-4a71-9555-9113c9229f51)


##### 텍스트 유사도
- 텍스트가 얼마나 유사한지 표현하는 방법
- 이 노래 누가 만들었어?, 지금 나오는 노래 작곡가가 누구야? 이런거
- 예시 같은 두 문장이 얼마나 유사한지 정량화 해 수치로 표현하는 모델을 만드는 것이 중요함

- 유사도 측정을 위한 정량화 방법
  - 같은 단어의 개수를 사용하여 판단하는 방법(카운팅)
  - 형태소로 나눠 형태소 비교를 하는 방법
  - 문장, 단어를 자소 단위로 나눠 각 단어를 비교하는 방법
  - 딥러닝 기반으로 측정하는 방법

- 많이 사용되는 유사도 측정 방법
  -  자카드 유사도, 유클리디언 유사도, 맨해튼 유사도, 코사인 유사도  

### 자연어 생성
- 자연어 생성, 텍스트 생성, 문장 생성 등으로 표현
- 뉴스 기사 생성, 대화 생성, 챗봇 등 다양한 영역에서 활용
- 인간과 컴퓨터 사이의 의사소통을 위한 가장 자연스로운 방식
- 언어모델 기반(최근), 패턴 기반(예전)의 자연어 생성 으로 형태를 나눠서 생각할 수 있다.

- 언어 모델을 이용한 방식
  - OpenAI의 GPT, Naver의 HyperClova 등 다양한 언어 모델 존재
  - 수많은 데이터의 학습을 통해 어떤 표현이 가장 적절한지 예측하여 문장 생성
    - GPT3는 약 1750억개의 파라미터 데이터를 이용해 학습을 진행   

- 언어 모델을 이용한 방식의 문제점
  - 가장 자연스로운 문장을 만드는 것에 강점을 가짐
  - 어떤 데이터를 기반으로 정확한 사실을 전달하는 문장을 만들지 못함
  - 뉴스 기사를 언어모델로 작성하면, 사람이 쓴 것같은 자연스로운 기사를 쓸 수 있지만 데이터 기반으로 하지 않기 때문에 가짜 뉴스가 만들어짐

- 패턴 기반의 방식의 문제점

### 기계 이해 (-> 텍스트 이해)

- QNA문제
- 기계까 텍스트를 이해하고 논리적으로 추론을 할 수 있는지를 데이터 학습을 통해 구현, 확인하는 것
- 앞서 설명한 텍스트 분류, 유사도, 자연어 생성 등 자연어 처리에 대한 전반적인 내용이 모두 포함되어 있다 볼 수 있음

- 자연어 처리의 활용 분야
  - 대량의 텍스트를 이해하고 수치화 하는 분야
    - 감성분석, 문서분류, 문서요약
  - 사용자 의도를 파악하고 대화하거나 도움을 주는 분야
  -   AI 스피커, 
   

### 딥러닝 적용 이전의 자연어 처리 방식

![image](https://github.com/user-attachments/assets/07307353-5cb2-4076-84d0-caec90ec46df)

### 자연어 처리(분석중심)의 일반적인 단계

![image](https://github.com/user-attachments/assets/b00eca9d-b580-4f24-83a6-3e640b9cedbd)

### 어휘 분석
- 기술 구성
  - Sentence Splitting : 마침표, 느낌표, 물음표 등을 기준으로 분리
  - Tokenizing : 문서나 문장을 분석하기 좋도록 나눔(띄어쓰기 또는 형태소 단위)
  - Morphological : 토큰들을 좀 더 일반적인 형태로 분석해 단어 수를 줄임으로 분석의 효율성을 높임(가장 작은 의미 단위로 토큰화 진행)
  - Stemming(어간추출, 형태소 분석) : cars, car -> car
  - = Lemmatization : 단어를 원형으로 표현
- 필요성 : 입력된 문장을 잘 분할해서 효율성을 높이기 위함


#### 구문분석(Syntax Analysis)
- 각각의 어절 단위로 구분, 해당 태그 부여(Parsing Tree 이용)

#### Semantic Analysis (의미분석)
- Syntactic + Meaning
- 문장의 의미에 근거해서 그 문장을 해석하는 방법
- 여러의미 분석 방법과 다양한 유형의 문법 이용
- 문장이 어떻게 구성되었는지 나타내는 규칙들로 구성된 일종의 형식 시스템
- 필요성
  - 규칙에 따라 문장은 만들었는데, 문장이 의미적으로 올바른 것인지 확인해야 함

#### 실용분석 Pragmatic Analysis
- 실용주의
  - 인간의 행동과 실제적 측면에 대한 연구
  - 실제 상황에서의 언어, 표시, 단어 및 문장의 사용에 대한 연구
- 실용적인 상호 작용 컨텍스트(문맥, 맥락)에서 의미 연구 가리킴
 - 발언의 문자적 의미 넘어 그 의미가 어떻게 구성되는지 대한 것만 아닌 묵시적 의미에 초점(상호작용의 도구로 언어 선택)

- 대화 흐름 상 어떤 의미 가지는 지 탐색
- 문맥 구조 분석 : 문장 사이 연관 관계 분석
- 의도 분석 : 전후 관계를 통한 

![image](https://github.com/user-attachments/assets/a84c82f0-f924-4919-8d3a-a7a124096103)

- 실용주의 관점에서 고려할 사항
  - 연사와 청취자 사이 의미 전달, 협상
  - 발언에 대한 맥락
  - 발언이 가지는 의미 및 의미에 대한 잠재력
 
- 필요성 : 대화 흐름을 파악해 발화자 의도에 맞도록 응답


### 딥러닝 기술의 등장에 따른 변화
  - 딥러닝 기술 적용 이전 자연어 처리 방식 문제
  - 여러 단계 모듈로 구성(디자인 복잡, 상황데 따라 또 다른 서브 모듈 추가 빈번)
  - 각각의 모듈이 완벽하게 동작하지 않음 ( 각기 발생한 오차 중첩, 가중으로 뒤로 전파되는 오차의 전파 현상 발생)
  - 시스템이 매우 무겁고 복잡해 구현 및 시스템의 구성이 어려움

- 딥러닝 기술의 적용과 함께 변화 -> 각 하위 모듈의 딥러닝 모델화로 시작 -> 점차 End-to-End 모델로 대체

![image](https://github.com/user-attachments/assets/ead9a416-3791-45c7-a1eb-d8ddc9697ca4)

![image](https://github.com/user-attachments/assets/3d36b396-eb74-432f-a3d3-ae16114d4d6d)

### 딥러닝 기술의 등장에 따른 변화
- Word2Vec 등의 임베딩 기술 적용
  - 단어(토큰)을 연속적 벡터로 표현 -> 모호성 유의성 문제 개선

- 딥러닝 적용
  - End-to-End 방식의 모델 적용 -> 성능 향상
  - 개선된 RNN 계열 모델(LSTM, GRU)활용 고도화
  - 주의 모델(Attention)적용 -> 긴 길이의 시퀀셜 데이터를 이용한 학습 용이

- 딥러닝 기반 자연어 처리 과정
![image](https://github.com/user-attachments/assets/db79cfe9-4050-4bd5-9ebe-d529054732b5)

#### 자연어 처리를 위한 언어학적 요소

- 음성학 & 음운론
  - 언어의 소리가 물리적으로 어떻게 형성되는지에 대한 이산적인 소리체계에 대한 연구
  - 소리를 기준으로 분석하기 때문에 발생하기 쉬운 문제(유사한 발음을 가진 전혀 다른 문장의 경우, 인식 오류가 발생하기 쉽다)

- 음성인식
  - 음성의 파형을 기호화해 인식하는 음성학, 음운론 기반의 연구분야 (음소:더이상 작게 나눌 수 없는 음운론 상의 최소 단위)
 
####  형태론
- 어절 : 양쪽 공백을 가진 띄어쓰기 단위의 문자열
- 단어 : 단일 품사를 가지는 단위
- 형태소 : 의미를 가지는 언어 단위 중 가장 작은 단어 (한국어만 있음)
  - 의미 또는 문법적 기능의 최소 단위(단어일수도 단어 아닐수도 있음)
  - 사전에 등록되어 있는 색인어의 집합  

- 형태소 분석
  - 형태소를 비롯하여 어근, 접두(미)사, 품사 등 다양한 언어적 속성의 구조를 파악
  - 입력된 문자열을 분석하여 형태소라는 최소 의미 단위로 ㅈ분리
 
#### 구문론
- 문법 : 문장의 구조적 성질을 규칙으로 표현한것
  - 각 규칙을 이용해 문장을 생성, 분석 

- 구문 분석:
  - 문법을 이용해 문장의 구조를 찾는 과정
  - 문장 구조는 트리 형태로 표현 가능함
 
#### 의미분석
- 통사 분석 결과에 해석을 가미해 문장이 가진 의미 분석
- 형태소가 가진 의미를 표현하는 지식 표현 기법이 요구
- 통사적으로 옳으나 의미적으로 틀린 문장이 있을 수 있음(시적 표현을 통해 통용되는 경우가 많음, 돌이 걸어간다, 바람이 달린다.)
- 모호성 등의 어려움이 있음(말이 많다. 같은것)

#### 실용분석
- 문장과 실세계가 가지는 연관관계 분석
- 실세계의 지식과 상식의 표현이 요구
- 지시(대명사 등의 지시 대상, 상대방에서 행동을 요하는것), 간접화법 등 분석

### 자연어 처리가 어려운 이유
#### 모호성(중의성)
- 단어의 중의성에 따라 발생하는 특징, 하나의 표현이 여러 의미를 가질 수 있는 성질

- 언어는 계속 진화하고, 특히 효율성을 극대화하는 방향으로 진화하기에
- 최대한 짧은 문장 내 많은 정보를 담고자 하기 위해
- 생략된 문맥을 인간은 여러 지식을 이용해 효율적으로 채울 수 있지만, 기계는 이러한 작업에 매우 취약함

- 모호성의 예시
  - ![image](https://github.com/user-attachments/assets/5c4e053e-7531-466d-a78c-bf924362bf31)
  - 차를 표현한 단어들 :tea, car, kick, dumped
  - 일부 표현을 빠뜨리거나 일부는 단어를 잘못 선택함 -> 정확한 번역은 찾기 어려움

- 문장 내 정보 부족에 따른 예시
![image](https://github.com/user-attachments/assets/c1420049-6695-4aac-bb86-8672ea02d061)

- 다양한 표현 : 여러 상황을 표현, 묘사하기 위해 다양한 표현이 사용되지만 그 의미는 동일한 경우가 매우 많음

- 불연속적 데이터
  - 기존 자연어 처리 : 이산 데이터 대상임으로 처리가 쉬움
  - 딥러닝 기반 : 데이터를 연속적인 값으로 바꿔줘야함

- 차원의 저주(대표적으로 : 원 핫 인코딩에 의해 만들어짐)
  - 불연속 데이터이므로 많은 종류의 데이터를 표현하려면 데이터 종류만큼의 대규모 차원이 필요, 어휘 크기만큼의 차원 요구됨

- 노이즈와 정규화
  - 노이즈가 제대로 분리되지 않거나 영향력이 커지면 데이터의 원래 의미까지 손상 가능
  - 자연어 데이터는 불연속 데이터(심볼)임으로 완전 다른 의미가 되어버릴 수 있음
  - 띄어쓰기, 어순 차이 등에 의한 정제(정규화)이슈도 큰 어려움 요소이다.

 - 한국어 자연어 처리가 어려운 이유
  - 과거에는 우랄알타이어족 이라 하다, 교착어에 속했다 했는데, 최근 분류가 백지화됨
![image](https://github.com/user-attachments/assets/298267c8-6610-47d8-9199-e5a95ad06582)

- 접사에 따라 단어의 역할 결정 -> 어순이 크게 중요하지 않음
- 문장을 끊어서 사용한다면 문제 없는 경우도 발생한다.
- 띄어쓰기 : 한국어는 근대 띄어쓰기가 도입외어 문장에서 띄어쓰기의 중요도가 낮음(영어는 띄어쓰기만으로 단어의 분리가 가능한데 한국어는 불가능함)

- 현서문과 의문문이 같은 형태의 문장구조를 가지는 경우가 많음, 물음표가 없으면 알 수 없는 경우가 많아서
- 주어생략 : 영어는 명사가 중요해 주어 생략 x, 한국어는 동사를 중시해 주어 자주 생략(문맥 정보를 이용해 생략된 정보를 메꿈(컴퓨터가 취약한 부분))

- 제일 문제되는 부분
  - 한자 기반 단어 : 한자를 조합해 만들어진 단어가 많음
  - 한자 기반 단어는 각 글자가 의미를 가지고, 그 의미들이 합쳐져 하나의 단어의 뜻을 나타냄
  - 한글이 한자를 대체하면서 문제가 발생한다
  - 한자는 표어 문자, 한글은 표음문자
    - 표어문자 : 하나하나의 문자가 하나의 말, 단어, 형태소를 이루는 문자 시스템
    - 표음문자 : 문자에서 각 글자가 특정 의미를 가지지 않고, 단지 각 음성에 대응하여 발음을 나타내는 문자 시스템

  - 한자(표어문자) -> 한글(표음문자) -> 정보의 손실 발생 -> 모호성 등의 문제 유발
    - 인간은 정보 손실로 발생한 모호성을 문맥의 이해로 해석 가능(기계는 어려움)
  - 다른 언어보다 중의성에 따른 문제가 더 가중되고 있다.


- 서브 워드 단위로 단어를 분절할 경우 가중되는 중의성 문제 사례

### 자연어 처리의 최근 변화 추세
- 2010. RNN을 활용한 언어모델 시도 (기존 n-gram 기반 언어모델 한계 극복 시도)
- 2013. 구글, word2vec 발표(단순한 구조의 신경망 사용, 단어를 효과적으로 잠재공간에 투사(비슷한 의미의 단어일수록 저차원의 잠재공간에서 가깝게 위치), 딥러닝 기반 자연어 처리 시, 네트워크 내부는 어떤 식으로 동작하는지에 대한 인사이트 획득)
- 2014. yoon kim, cnn만을 활용한 텍스트 분류 모델 제시(cnn 만을 활용해 기존 텍스트 분류보다 성능을 끌어올린 텍스트 분류 모델 제시, 단어 임베딩 벡터와 결합해 성능의 극대화를 이끌어냄)
- 2014, seq2weq(순차열-> 순차열)ahepf, attention(주의) 기법 개발(성공적 기계번역 적용, 주어진 정보 기반 자유롭게 문장을 생성하는 자연어 생성(NLG)가 가능해짐, 기계번역 외 문장요약, 챗봇 시도 증가, 최초로 상용화 시작      

- Attention 기법 성공 이후 연속적 방식 정보를 읽고 쓰는 기법에 대한 관심 증가(메모리 증가 신경망(MANN)확산 : 뉴런 튜링 머신, 차별화 가능한 뉴럴 컴퓨터 등장

#### 강화학습의적용
- 딥러닝 기계번역 분야 적용에 대한 문제점
  - 딥러닝 손실 함수와 기계번역에서의 목적 함수 사이의 괴리
  - 영상 처리분야의 경우, 기존 MSE(평균제곱오차)손실 함수의 한계를 벗어나기 위해 GAN모델을 도입하여 생성 모델 구현
  - 자연어 생성에도 GAN모델에 강화학습 기법을 반영한 SeqGAN 등의 모델 제시(강화학습의 폴리시 그래디언트 기법을 자연어 생성에 적용 성공
- 강화 학습을 이용해 실제 자연어 생성에서의 목적 함수로부터 보상을 받을 수 있게 됨(사람이 생성한 것과 유사한 문장 생성 능력의 극대화 유도)

## 자연어 처리를 위한 전 처리 과정
- 자연어 처리에서 가장 중요한 것은 "전처리 과정"이다.

- 일반적인 전처리 과정
![image](https://github.com/user-attachments/assets/9a86f372-0041-4816-92a9-2260bdcf3592)

#### 코퍼스(Corpus, 말뭉치)
- 여러 단어로 이루어진 문장을 가리킴
- 자연어 처리를 위해 머신러닝/딥러인 수행하려면 훈련데이터가 필요, 다수의 문장으로 구성된 코퍼스가 훈련데이터로 사용됨
- 코퍼스 많고, 오류 없을수록 자연어 처리 모델은 더욱 정교해지고 정확도가 높아짐

- 코퍼스 분류
  - 구성 목적에 따른 분류
    - 단일 언어 코퍼스(base)
    - 이중 언어 코퍼스(번역)
    - 다중 언어 코퍼스(번역)  
  - 구성 방법에 따른 분류
    - 병렬 코퍼스 : 각 언어가 서로 쌍으로 구성되는 코퍼스(번역에 쓰임)

- 코퍼스 수집
  - 공개 데이터 사용
    - 감성 분석 등 텍스트 분류 데이터, 기계번역을 위한 언어 쌍 데이터 등 각종 대회 또는 논문을 위한 데이터가 주류
    - 데이터의 분량과 내용의 분야가 한정적
    - AI허브, 정부 공공 데이터
  - 유료 데이터 구매
    - 비용문제 발생

  - 웹 크롤링
    - 다양한 분야 데이터 획득 가능
    - 특정 분야에 대한 자연어 처리가 아니라면 최대한 많은, 다양한 분야의 데이터가 필수
    - 무분별한 웹 크롤링은 법적 문제 유발 가능, 웹 서버의 불필요한 트래픽 가중

  - 단일 언어 코퍼스 수집
    - 가장 손쉽게 구하는 코퍼스, 올바른 도메인의 코퍼스 수집 필요
    - 사용가능한 형태로 가공하는 과정 필요, 위키피디아 에서 덤프 데이터를 제공하기도함
    - 머신러닝 경진대회 플랫폼 사이트 "캐글"등에서 쉽게 다운로드 가능

- 다중 언어 코퍼스 수집
  - 다중언어 코퍼스는 기계번역을 목적으로 하는 경우 많음
  - 수집이 어려움(단일보다), 자막 뎅이터 등은 저작권이 있는경우 많아 저작권 정보 확인 필수
  - 자막 등은 번역 품질 문제가 킁 영향을 미치므로 주의
  - 자막 등은 대화형 언어이고, 언어별 특징에 따른 문제 고려 요함

#### 정제(Normalization, 정규화)
- 텍스트를 사용하기 위한 필수 과정
- 원하는 업무, 문제, 응용 분야에 따라 필요한 정제의 수준, 깊이 상이 (개인정보, 민감한 정보)

- 전각 문자 제거
  - 중국어, 일본어 문서는 대부분 전각 문자로 표기
  - 한국어 문서의 일부는 전각 문자로 표기된 기호, 숫자 등 사용
  - 데이터 처리는 반각 문자를 기준으로 하므로 전각 문자를 반각 문자로 변형하는 작업요함    

- 대소문자 통일
  - 일부 영어 코퍼스에서는 약자 등에서 대소문자 표현이 통일 x
  - 데이터를 하나의 형태로 통일해 희소성 줄이는 효과 기대
  - 딥러닝 모델 확산하며 중요도가 매우 떨어짐

- 정규 표현식을 사용한 정제
  - 다량의 코퍼스는 특수문자, 기호 등의 노이즈가 많음(크롤릴 많이 필요)
  - 웹 사이트 성격에 따라 일정한 패턴을 가지는 경우 많음
![image](https://github.com/user-attachments/assets/3d94ced7-94f5-4a34-9cbc-102151bd2f83)

- 지정 문자 사용
![image](https://github.com/user-attachments/assets/783373a4-f074-41de-bdeb-9156f946bdf8)

![image](https://github.com/user-attachments/assets/11d0c6c2-12af-4573-9bff-e416106a0e61)

- 그룹 사용
  - 정규 표현식에서 괄호는 그룹을 의미
  - 예) 전화번호 패턴
 ![image](https://github.com/user-attachments/assets/6f8d90d4-65f5-476c-9e27-64a530e6f4d6)

- 정규 표현식 적용 예제
![image](https://github.com/user-attachments/assets/c9cc9c84-c5e8-4eca-9686-654b92cd1b00)

- 정제 작업
  - 마지막 줄에 전화번호가 있다. -> 삭제 -> 마지막 줄에 전화 번호 없는 경우 ->X
  - 다양한 상황을 고려해야함
![image](https://github.com/user-attachments/assets/404827af-c690-4f12-aff6-b7e26ad9bb26)

- 정제 작업 결과
![image](https://github.com/user-attachments/assets/0ab297d4-c372-46a4-91d3-f1d3bc8020cf)

https://regexper.com/ 
![image](https://github.com/user-attachments/assets/eaf30592-e79f-45c9-937c-c95b6145040f)

## 분절
- 자연어 처리에 사용되는 데이터
  - 입력단위 : 기본적으로 문장 단위
  - 한 줄에 한 문장 요구
  - 여러 문장이 한줄에 있거나, 한 문장이 여러 줄에 걸쳐 있는 경우 -> 문장 단위 분절이 요구됨 

- 문장 단위 분절의 기준
  - 마침표를 기준으로 잡으면 U.S와 같은 약자, 3.14와 같은 소수점 등의 문제 발생
  - 적절한 알고리즘 또는 모델이 필요함
    - 직접 구현 어려우므로 잘 알려진 자연어 처리 툴킷, 패키지 등의 활용 권장

- 단어 단위 분절:토크나이징(형태소 분석)
  - 풀고자 하는 문제에 따라 형태소 분석 또는 단순 분절을 통한 정규화 수행
  - 가장 기본적인 기준은 띄어쓰기(한국: 근대 이후 도입, 중, 일 : 사용 x, (중국어의 경우 문자단위로 처리되므로 차이 없는데, 일본은 띄어쓰기가 요구됨), 영어의 경우 띄어쓰기 필수, 대부분 경우 규칙이 잘 지켜짐

- NLTK(Natural Language Toolkit)
  - 자연어 처리 기능을 제공하는 파이썬 라이브러리
  - 텍스트로부터 단어 개수, 출현 빈도, 어휘 다양도 같은 통계적 정보 쉽게 얻을 수 있음
  - 손 쉽게 이용 가능하도록 모듈 형식 기능 제공

  - 전처리 모듈 : 분류 토큰화, 스테밍
  - 분석 모듈 : 구문분석, 클러스터링, 감정분석, 태깅
  - 추론 모듈 : 시맨틱 추론
  - 각 모듈은 최소 2개 이상의 알고리즘을 제공해 사용자가 원하는 알고리즘을 선택할 수 있도록 지원
  - 문서화가 잘 되어 있음
  - 포럼이 활성화 되어 개발 지원 받을 수 있고, 많은 레퍼런스를 포함하고 있음
  - 속도가 다소 느림

  - 원래 한국어 지원 x였기에 국내 연구자들이 KoNLTK와 같은 모듈을 개발해 사용함
  - 최근 한국어도 지원 언어에 포함됨

- 각 언어별 주요 자연어 처리 지원 패키지
![image](https://github.com/user-attachments/assets/90bb0f1a-a694-48aa-9c2b-6f447b1f9dba)

### 병렬 코퍼스
- 동일한 내용을 가진 복수 언어 말뭉치(코퍼스)
- 데이터 자체가 복수 언어를 대상으로 만들기 때문에 언어의 대조 연구나 기계번역, 대역 사전구축, 언어 교육(작문 교육, 회화 교육)등에 유용하게 사용

- 병렬 코퍼스 정렬
  - 대부분 병렬 코퍼스들은 여러 문장 단위로 정렬
  - 병렬 코퍼스 제작 프로세스
  ![image](https://github.com/user-attachments/assets/df6e3920-7668-4ec6-a1c3-3ccca4eadc59)

- 사전 생성
  - 단어 사전 구축에 많은 비용이 요구됨
    - 기 구축된 사전 활용 또는 MUSE를 이용한 단어 사전 자동 생성(구축) 활용
    - MUSE : 병렬 코퍼스가 없는 상황에서 사전 구축을 위한 방법, 코드 제공
      - 각 단일 언어 코퍼스를 통해 구축한 언어별 단어 임베딩 벡터에 대해 다른 언어의 임베딩 벡터와 매핑을 통하여 단어간 번역 수행
      - 비지도 학습을 적용한 모델
    - 구축된 사전은 CTK의 입력으로 사용됨
![image](https://github.com/user-attachments/assets/873a3f19-ce3b-4da3-bb90-87cd0286a4e0)

- MUSE를 통한 비지도 학습의 결과인 단어사전 예
![image](https://github.com/user-attachments/assets/1068f34e-6eb3-487c-b96e-74f1a898f977)
- 비지도 학습이지만 꽤 정확한 단어간 번역을 확인할 수 있음
- <>을 구분 문자로 사용

- CTK를 활용한 정렬
- 구축된 단어사전을 이용하여 여러 라인으로 구성된 언어별 문서에 대해 문장 정렬 수행
- 정렬 결과 예시
- 일대일, 일대다, 다대다 매핑가능, 버려지는 경우도 가능
![image](https://github.com/user-attachments/assets/afbfe4fe-4826-4605-a04e-9897ded4fdd4)

### 서브워드 분절
- 단어는 의미를 가진 더 작은 서브워드들의 조합으로 이루어진다는 가정 하에 적용되는 알고리즘
- BPE 알고리즘을 기반으로함
- 현재 시점 필수 전처리 방법으로 꼽힘
![image](https://github.com/user-attachments/assets/6bf72487-8b02-4948-bb01-37b38feeb10b)

- 적절하게 분절하면 어휘 수 감소 및 희소성의 효과적 감소 가능
- Unknown 토큰에 대한 효율적인 대처 가능
  - 자연어 처리에서는 문장을 입력 받을 때 단어들의 시퀀스로 받아들임
  - Unknown이 발생하면 언어모델의 확률이 크게 하락하고 적절한 문장 생성이 어려워짐

## Tokenizing(형태소 분석) 개요

- 자연어 : 우리가 일상 생활에서 사용하는 언어
- 기본적으로 컴퓨터는 자연어를 이해하지 못한다
- 컴퓨터에게 자연어를 이해하게 하려면 토크나이징과 임베딩 기반으로 벡터화 하는게 일반적, 이후 텍스트 유사도를 이용해 문맥 분류하는 방법을 사용

- Tokenizing이란?
  - 주어진 문장에서 토큰 단위로 정보를 나누는 작업
  - 문장 형태 데이터 처리시 제일 처음 수행하는 기본 작엄, 주로 텍스트 전처리 과정 사용
  - 토큰 : 일정한 의미 있는 가장 작은 정보 단위

- Tokenizing 과정
- 어던 문장을 일정한 의미가 있는 가장 작은 단어로 나눔
- 나눠진 단어를 이용해 의미 분석

- 토큰화의 기본적 방식
- 단어 단위 토큰화 : 단어(어절) 단위로 토큰화 (영어)
- 문자 단위 토큰화 : 문자 단위로 토큰화 (중국)
- 서브 워드 단위 토큰화 : 서브 워드 단위로 토큰화
- 문장 단위 토큰화 or 문장 단위 임베딩 

###### 단어단위 토큰화 : 공백으로 분리(띄어쓰기, 별도의 토크나이저 없어도 무방)
  - 단점 : 어휘 집합의 크기가 매우 커질 수 있음
![image](https://github.com/user-attachments/assets/609638df-7ad6-43a8-a122-37b981e768dc)


- 사전 학습된 토크나이저(형태소 분석기)를 사용하면 어휘 집합의 비대화를 다소 완화 가능
![image](https://github.com/user-attachments/assets/67e95e5c-65ae-4a06-aab4-575dba2f5102)
- 하나의 언어로 모델을 구축시, 어휘 집합의 크기는 10만개를 가뿐히 넘어감
- 어휘 집합의 크기가 커질 수록 모델의 학습은 어려워짐

###### 문자 단위 토큰화
- 한글 경우 표현 가능한 글자는 11,172개(알파벳, 숫자, 기호를 모두 고래해도 어휘 집합의 크기는 15,000개 정도)<그당시 조합형으로 한글 사용해서 유니코드 상 가장 많은 문자 등록되어 있는 것이 한글임(너무 많으니 조합형 포기하고, 현재 한글에서 사용하는 모든 글자를 그냥 다 넣음:완성형 기반)>
- 해당 언어의 모든 문자를 어휘 집합에 포함 -> 미등록 토큰 문제 없음
- 단점
  - 각 토큰은 의미있는 단어가 될 수 없음
  - 어미에 따른 변화, 조사의 사용 등 한글의 특징이 모두 사라짐
  - 분석 결과인 토큰 시퀀스의 길이가 단어 단위 토큰화 결과보다 상대적으로 길어짐
  - 언어 모델에 입력할 토큰 시퀀스가 길면 모델의 학습이 어려워지고 결과적으로 성능 하락

- 서브 워드 단위 토큰화
  - 단어 단위 토큰화와 문자 단위 토큰화의 중간 형태
  - 두 토큰화 방식의 장점만 적용 (어휘 집합의 크기가 지나치게 커지지 않음, 미등록 토큰 문제 회피, 분석 려과의 토큰 시퀀스가 너무 길어지지 않음)
  - 대표적인 서브워드 단위 토큰화 기법 : 바이트 페어 인코딩(BPE)

- BPE(Byte Pair Encoding)
  - 1994년 제안된 정보 압축 알고리즘
  - 데이터에 가장 많이 등장한 문자열을 병합해 데이터를 압축하는 기법
  - 데이터에 등장한 글자를 초기 사전으로 구성해 연속된 두 글자를 한 글자로 병합하는 방식 적용
  - 최근 자연어 처리 모델에 널리 쓰이는 토큰화 기법 (GPT 모델)

- 초기 사전 : (a,b,c,d) -> 4개 문자열 : aaabdaaabac -> 11자
  - aaabdaaabac → aa를 Z로 병합 → ZabdZabac
  - ZabdZabac → ab를 Y로 병합 → ZYdZYac
  - ZYdZYac → ZY를 X로 병합 → XdXac
- BPE 수행 이후
  - 사전 : (a,b,c,d,z,y,x) 7개
  - 결과 문자열: XdXac 5자

- BPE기반 토큰화 기법
  - 사전 크기의 증가를 억제하면서도 정보를 효율적으로 압축할 수 있는 알고리즘
  - BPE 어휘 집합은 고빈도 바이그램 쌍을 병합하는 방식으로 구축함

- BPE 알고리즘 특징
  - 분석 대상 언어에 대한 지식이 필요하지 않음
  - 말뭉치(코퍼스)에 자주 나타나는 문자열(서브워드)를 토큰으로 분석
  - 자연어 처리에서 BPE가 처음 적용된 분야는 기계번역 분야
 
- BPE 활용 토큰화 절차
  - 1. 어휘집합 구축 : 자주 등장하는 문자열 병합 후 어휘집합 추가 반복
    2. 토큰화 : 문장의 각 어절에서 어휘 집합에 있는 서브 워드를 어절에서 분리 

- BPE 어휘 집합 구축
![image](https://github.com/user-attachments/assets/2be9f190-839d-4db2-853b-14456d8aa54c)

- BPE 토큰화
- 어휘 집합과 병합 우선순위를 기준으로 토튼화 수행
![image](https://github.com/user-attachments/assets/792555c5-fa37-44be-a44c-803a0037ed28)

- BPE 토큰화 예시
![image](https://github.com/user-attachments/assets/79e89963-3271-4b95-9647-64e6ae1dc66a)


### 한국어 토크나이징(형태소 분석)
- 한국어 토크나이징 구현하기 위해서는 한국어 문법에 대한 깊은 이해 필수
- 비전공자는 한국어 토크나이징 지원하는 파이썬 라이브러리 사용(KoNLPy)

- 토큰 단위를 어떻게 정의하느냐가 자연어 처리 성능에 큰 영향
- 한국어 분석에서 사용하는 토큰의 단위는 형태소
- 형태소는 언어학에서 사용되는 용어, 일정한 의미가 있는 가장 작은 말의 단위
- 형태소를 토큰 단위로 사용하면, 단어와 품사 정보를 같이 활용할 수 있다.(효과적 처리 가능)

- 영어의 경우 토크나이징 쉽다.(단어의 변화가 크지 않고, 띄어쓰기 단어 구분, 공백 기준으로 토크나이징 수행도 문제 없다.)
- 한글의 경우 토크나이징 어려움
  - 한국어는 명사와 조사를 띄어 쓰지 않는다 → 공백을 기준으로 토크나이징 수행 불가능
  - 용언에 따라 여러 가지 어미가 붙는다 → 일정한 기준을 찾기 어렵다
  - 따라서 복잡한 특성을 고려하여 문장에서 형태소를 분석할 수 있는 “형태소 분석기“ 가 필수
  - 다양한 문법적 특징을 반영하고 언어적 속성의 구조를 파악할 수 있어야 함

![image](https://github.com/user-attachments/assets/b0b7b515-9d0d-44f6-b084-a9bf350a1226)


- 형대소 분석기
  - 복잡한 한국어 문법때문에 형태소 분석기의 개발은 매우 어렵다
  - KoNLPy 등의 라이브러리 사용은 필수(내부에 다양한 형태소 분석기 통합해 라이브러리 형태로 제공, 기본 성능이 뛰어난 편)

- KoNLPy가 제공하는 대표적인 형태소 분석기
  - Kkma (꼬꼬마, 서울대에서 개발, GPL2 라이센스), Komoran (코모란, Shineware에서 자바로 개발, Apache 2.0 라이센스), Okt (트위터에서 개발, Apache 2.0 라이센스)
  - 통합제공하기 때문에 5~6 종류 모두 사용법이 거의 동일함

![image](https://github.com/user-attachments/assets/dc0d487d-cd97-4b9b-83f5-33a2387b585f)

- 사용자 사전 구축
  - 챗봇의 데이터 입력단은 인터넷 구어체와 관련이 많음
  - → 일반적으로 딱딱한 구어체, 문어체 등을 사용하지 않음
  - 새롭게 생겨나는 단어나 문장은 형태소 분석기가 인식하지 못하는 경우 많음
    - 기존의 많은 문장을 이용하여 형태소 분석기 모델이 개발되었으므로 새로운 형태의 단어, 문장은 학습 데이터에 포 함되어 있지 않음 → 인식률 저하의 원인
  - 문제의 해결을 위해 대부분의 형태소 분석기는 사용자 사전을 추가할 수 있도록 구성됨
























```











