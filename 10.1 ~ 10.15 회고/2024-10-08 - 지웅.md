# 📝 2024.10.08 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 임베딩
- 단어나 문장을 수치화 하여 벡터공간으로 표현하는 과정
  - 컴퓨터는 자연어를 직접적으로 처리할 수 없다
  - 컴퓨터는 수치 연산만 가능하다
  - 따라서 자연어를 숫자나 벡터 형태로 변환할 필요가 있다
  - 이때 사용되는 일련의 과정을 “임베딩”이라고 한다
- 임베딩된 결과는 딥러닝 모델의 입력값으로 사용된다.

###### 임베딩 기법 종류
- 문장 임베딩
  - 문장 전체를 벡터로 표현하는 방법
  - 전체 문장 흐름을 파악해 벡터로 변환(통으로) → 문맥적 의미를 지님
  - 단어 임베딩에 비해 품질이 좋음, 상용 시스템중 품질이 좋아야하거나 번역이 좋아야하는 경우 사용
  - 학습을 위한 수 많은 문장 데이터가 필요하며 학습 비용이 매우 높다.
 
###### 단어 임베딩
  - 개별 단어를 벡터로 표현하는 방법
  - 동음어 구분 하지 않음 → 의미가 달라도 단어의 형태가 같으면 동일한 벡터값을 가짐(단점)
  - 문장 임베딩에 비하면 학습 방법이 간단함 → 성능은 떨어지지만 실무에서 많이 사용(비용)

- 단어 임베딩
  - 말뭉치에서 각각 단어를 벡터로 변환
  - 의미와 문법적 정보를 지님
  - 단어를 표현하는 방법에 따라 다양한 모델 존재
  - 토크나이징을 통해 문장에서 토큰 단위로 추출하는 경우, 추출된 토큰은 형태소 기반이므로 단어 임베딩에 효과적이다.

- 원핫 인코딩
  - 단어를 숫자 벡터로 변환하는 가장 기본적인 방법
  - 요소들 중 단 하나의 값만이 1이고 나머지 요소들의 값은 0인 인코딩 방식
![image](https://github.com/user-attachments/assets/f7ce4efe-3fdf-46f2-937e-9a2de2005222)

- 원핫 인코딩 벡터의 차원은 전체 어휘의 개수 → 매우 큰 차원이 됨
![image](https://github.com/user-attachments/assets/a5eaec03-e20e-489a-aabd-f603fb6b5325)

- 단어는 불연속적인 심볼이며 이산 확률 변수로 나타남 → 원핫 벡터는 이산 확률 분포에서 추출한 샘플 → 불연속적인 값을 가진다.
![image](https://github.com/user-attachments/assets/5ef3be6a-1d3f-4696-8f53-558bcdcae487)


- 희소표현
  - 원-핫 인코딩: 표현하고자 하는 단어의 인덱스 요소만 1이고 나머지 요소는 모두 0으로 표현되는 희소 벡터(또는 희소 행렬) → 단어가 희소 벡터로 표현되는 방식: 희소 표현
  - 희소표현 : 각각의 차원이 독립적인 정보를 지님
  - 자연어처리를 잘 하려면 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야함(희소 표현은 이런 조건을 만족하지 못함)

- 분산 표현
  - 희소 표현의 단점 해결을 위해 고안됨
  - 각 단어간 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법
  - 한 단어 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현됨
  - 비유를 통한 예시(색상을 표시하는 RGB 모델 → 3차원 벡터, 분산 표현 방식)
  - 희소 표헨에 비해 많은 장점을 가지므로 단어 임베딩 기법에서 많이 사용됨
- 분산표현을 그림으로 이해하면 아래와 같다.

![image](https://github.com/user-attachments/assets/61311291-5405-4513-827e-f47612a0b859)

- 신경망에서는 분산 표현을 학습하는 과정에서 임베딩 벡터의 모든 차원에 의미있는 데이터를 고르게 밀집시 킴 → 희소표현과 반대로 데이터 손실을 최소화 하면서 벡터 차원이 압축되는 효과를 가짐


- 희소 표현 대신 분산 표현을 사용하면
  - 임베딩 벡터의 차원을 데이터 손실을 최소화하면서 압축할 수 있다.
  - 임베딩 벡터에 단어의 의미, 주변 단어와의 관계 등 많은 정보가 내포되어 있다.(일반화 능력이 뛰어남)

- 분산 표현 방식의 벡터 공간
![image](https://github.com/user-attachments/assets/de60fd9d-135e-4c2b-9a0a-87e80f7eeacd)


### 단어의 의미 파악
- 단어의 의미를 파악하기 위한 기법들
  - 시소러스 활용 기법
  - 통계 기반 기법
  - 추론 기반 기법

##### 시소러스 활용 기법
- 사람의 경우
  - 단어의 이해를 위해 사전 활용 (사전에 단어의 의미를 정의)
    - 컴퓨터도 이렇게 할 수 있지 않을까? 라는 데에서 나온 아이디어

- 시소러스 : 유의어 사전, 어휘 분류 사전
  - 뜻이 같은 단어(동의어), 뜻이 비슷한 단어(유의어)가 한 그룹으로 분류되어 있는 사전

- 단어들을 의미의 상/하위 관계에 따라 그래프로 표현, 처리
  - 단어들의 의미의 상, 하위 관계에 기초해 그래프로 계층화 시키고 코드화 시킴
  - 모든 단어에 대한 유의어 집합을 만들고 단어들의 관계를 그래프로 표현한 뒤, 단어 사이의 연결을 정의하고, 단어의 네트워크를 이용해 컴퓨터에게 단어 사이의 관계를 주입해 컴퓨터에게 단어의 의미를 이해시킨 것으로 간주함

- 시소러스의 문제점
  - 시대 변화에 대응하기 어려움(언어는 변한다, 신조어, 사멸어, 의미의 변화, 활용의 변화 -> 사람이 꾸준히 갱신, 관리)
  - 시소러스를 만들고 관리하기 위한 고가의 인적비용이 발생한다.(WordNet에 등록된 단어는 20만개)
  - 단어의 미묘한 차이를 표현할 수 없다.

- 문제 해결을 위해 통계 기반 기법, 신경망을 이용한 추론 기반 기법 등이 제안됨

- 시소러스의 대표적 모델 : WordNet

##### WordNet
  - 프린스턴 대학교 심리학 교수 “조지 아미티지 밀러”의 지도 하에 1985년부터 개발
  - 기계번역을 돕기 위한 목적으로 개발됨
  - 동의어 집합, 상위어, 하위어에 대한 정보가 잘 구축되어 있음
  - 유향 비순환 그래프(Directed Acyclic Graph, DAG)로 구성됨
  - (트리구조가 아닌 이유: 하나의 노드가 여러 개의 상위 노드를 가질 수 있음)

- WordNet 내의 단어 별 top-1 의미의 top-1 상위어만 선택하여 트리 구조로 나타낸 경우
![image](https://github.com/user-attachments/assets/f59a154e-f9f4-4563-89f2-1e791d26da95)

- 표제어 읽는 방법
![image](https://github.com/user-attachments/assets/adcceed0-8a08-48ca-a55c-2fdd42efb0ef)

- 한국어 WordNet
  - KorLex 부산대학교 http://korlex.pusan.ac.kr
  - Korean WordNet(KWN) KAIST http://wordnet.kaist.ac.kr
  
- 특징
  - 단어들은 그 내용을 설명할 수 있는 다양한 특징을 가짐
  - 특징벡터 : 이러한 특징 별 수치를 모아 벡터로 표현한 것

- 단어의 특징 벡터를 구성하기 위한 가정
  - 의미가 비슷한 단어라면 쓰임새가 비슷할 것
  - 쓰임새가 비슷하므로 비슷한 문장 안에서 비슷한 역할로 사용될 것
  - 따라서 함께 나타나는 단어들이 유사할 것

- 특징 추출하기
  - TF-IDF(Term Frequency-Inverse Document Frequency)
  ![image](https://github.com/user-attachments/assets/9aa70c65-dcb2-492e-ad33-5c8f3dc8de6d)


### 단어의 의미 파악 : 통계 기반 기법
- 통계 기반 기법
  - 통계를 기반으로 어떤 데이터가 많이 사용되었는지, 어떻게 분포되 있는지, 분포에 따라 어떤 연관성과 의미를 갖는지 등의 정보를 추출하여 자연어를 처리하는 기법
  - 통계를 사용하므로 통계 결과를 계산히기 위한 충분한 크기를 가진 대규모 텍스트 데이터가 요구됨

- 말뭉치를 이용함
  - 말뭉치 : 자연어 처리에 대한 연구, 애플리케이션의 개발 등을 염두에 두고 수집된 대량의 텍스트 데이터
  - 말뭉치 자체는 단순한 텍스트 데이터이지만 텍스트에 담긴 문장은 사람이 쓴 글이다.
  - 말뭉치에서 자동으로, 효율적으로 핵심을 추출하는 것(통계 기반의 자연어 처리 기법)

- 분포 가설
  - 단어의 의미는 주변 단어에 의해 형성
  - 단어 자체에는 별 의미가 없고 그 단어가 사용된 맥락이 의미를 형성
  - 자연어 처리에 관한 중요한 기법은 대부분 분포가설을 기반으로 한다.

![image](https://github.com/user-attachments/assets/0b05d113-5c56-4f01-9f41-ba9877ae9b8e)

![image](https://github.com/user-attachments/assets/b142231e-aeab-47ba-b2a5-6e8b708a3451)

- 분포 가설을 기초로 통계적으로 분석
  - 일단 어떤 단어가 몇 번이나 나오는지 세어보는 것이 핵심
  - 동시발생 행렬 : 주어진 단어의 맥락으로써 동시에 발생하는 단어의 출현빈도
    - 맥락의 크기를 윈도우라하고, 윈도우가 1인 경우(주어진 단어에서 한 단어까지 처리)
![image](https://github.com/user-attachments/assets/30142f5f-e9ed-4d79-b85e-d74296e55e95)

- 맥락이란 특정 단어를 중심에 둔 주변 단어를 말함
  - 맥락의 크기 : 윈도우
![image](https://github.com/user-attachments/assets/4eb99229-0cb2-44b1-ab4a-b37c9f5add26)

- 동시 발생 행렬
  - 분포 가설에 기초해 단어를 벡터로 바꾸는 방법
  - 단어가 몇개 나오는지 세어봄(통계 기반 기법)+
![image](https://github.com/user-attachments/assets/d6a39f4a-075d-4e47-8052-962dbb1e3b93)

![image](https://github.com/user-attachments/assets/549478da-b331-41df-aded-54eb49556437)

![image](https://github.com/user-attachments/assets/b4ede6c8-6de0-4a19-bd67-7ce108d0d911)


- 벡터 간 유사도
  - 동시발생 행렬을 이용해 단어를 벡터로 표현
  - 벡터 사이 유사도를 계산하여 해당 단어 간의 유사도, 관련성을 확인 가능
  - 벡터 간 유사도 계산 방법
    - 벡터의 내적
    - 유클리드 거리
    - 코사인 유사도: 단어 벡터 간의 유사도를 나타낼 때는 코사인 유사도를 많이 사용(효율 정확도 좋음)
![image](https://github.com/user-attachments/assets/e7d42fc1-6030-4078-8a0a-7ae2483833ee)

- 코사인 유사도 → 두 벡터가 가리키는 방향이 얼마나 비슷한가?를 확인, 벡터의 방향이 완전히 같다면 코사인 유사도 1, 완전히 반대라면 -1

- 유사 단어의 랭킹 표시
  - 유사도 높은 순으로 순위 매김
  - 입력 데이터인 말뭉치(텍스트 데이터)가 커질수록 랭킹이 정확해짐
- 이러한 과정을 거쳐 도출된 각 단어의 유사도 랭킹(순위)에 따라 단어, 문맥(맥락), 문장에 해당하는 의미를 선택하고 그 결과를 활용하는 것이 통계 기반의 기법

- 통계 기반 기법에서는 단어의 동시발생 행렬을 이용
- 발생횟수, 단어의 빈도수는 많이 활용되는 특성이지만 좋은 특성은 아님
- 전문적인 내용의 말뭉치일 경우, 해당 주제에 관련된 말이 비정상적으로 많이 나올 수 있음
- 영어권 언어에서는 관사 등 실제 의미와 무관한 단어가 최고 빈도수를 가진다.


- 점별 상호 정보량(Pointwise Mutual Information, PMI)
![image](https://github.com/user-attachments/assets/30e6768e-476b-4b41-9366-3c3f2ba203a6)
- 𝑃(𝑥): 𝑥가 일어날 확률, 𝑃(𝑦): 𝑦가 일어날 확률, 𝑃(𝑥, 𝑦): 𝑥와 y가 동시에 일어날 확률
- 𝑃𝑀𝐼 값이 높을수록 관련성이 높음

- 말뭉치에 적용시
  - 𝑃(𝑥): 단어 𝑥가 말뭉치에 등장할 확률
  - 𝑃(𝑥, 𝑦): 단어 𝑥와 y가 말뭉치에 동시에 등장할 확률 
![image](https://github.com/user-attachments/assets/2576cb7e-f86e-4caf-9cde-5c9babb2ba5e)

- 동시 발생 행렬을 사용해 다시 정리하면
![image](https://github.com/user-attachments/assets/b0ec06e4-2733-4919-b178-ff24110d9c35)

- 예) 말뭉치 단어 수가 10,000개, “the” 1,000번, “car” 20번, “drive” 10번 등장했고, “the”, ”car”의 동시발생 수 는 10회, “car”, “drive”의 동시발생 수는 5회라고 가정

![image](https://github.com/user-attachments/assets/1dcd04d8-ac72-46e3-b103-aa58fcfbb9ec)

- car , drive 의 관련성이 더 높은 것을 확인할 수 있음

- PMI 문제점
  - 두 단어의 동시발생 횟수가 0이면 𝑙𝑜𝑔20 = −∞가 됨
  - 문제 해결을 위해 실제 구현 시 양의 상호 정보량(PPMI)사용
![image](https://github.com/user-attachments/assets/9772b96d-a503-434b-8990-dfca4ae8b0c5)
- 𝑃𝑀𝐼 / 𝑃𝑃𝑀𝐼 모두 말뭉치의 어휘 수 증가에 따라 단어벡터의 차원 수가 증가 → 차원 감소를 위한 방안이 요구됨

#### 차원 감소
- 벡터의 차원을 줄이는 기법
- 단순한 차원 감소가 아닌 중요한 정보는 최대한 유지하며 줄임
- 단어 벡터는 주로 원핫 인코딩 같은 희소벡터(희소행렬)을 사용하므로 희소벡터에서 중요한 축을 찾아내 더 적은 차원으로 다시 표현하는 것이 핵심
- 차원 감소 결과는 원소 대부분이 0이 아닌 값으로 구성된 밀집벡터가 됨(단어의 분산 표현)
- 다양한 차원감소 기법 중 단어 처리에서 주로 사용되는것(특잇값 분해 Singular Value Decomposition(SVD))

- 특잇값 분해(SVD)
  - 주어진 행렬(X)을 3개의 행렬(U,S,V)의 곱으로 분해 → 𝑿 = 𝑼𝑺𝑽^𝑻
  - U, V: 직교행렬(열 벡터가 서로 직교함) → 어떤 공간의 축(기저)을 형성. U행렬은 단어 공간
  - S: 대각행렬(대각성분 외에는 모두 0) → 대각성분은 특잇값(=해당 축의 중요도) → 중요도가 낮은 원소(특잇값이 작은 원소)를 깎아내어 차원 축소에 반영
![image](https://github.com/user-attachments/assets/3fd3e8b7-71d2-4803-9c60-3cef924a6654)

- PTB(Penn Treebank) 데이터 셋
  - 펜실베이니아 대학교에서 관리하는 데이터 셋
  - NLP(자연어 처리) 연구를 위한 기계 학습에 널리 사용되는 데이터 셋
  - 대부분의 최신 데이터 집합에 비해 상대적으로 작아서 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용됨
  - 대문자, 숫자 및 문장 부호가 포함되어 있지 않으며 어휘는 10k 고유 단어로 제한됨
  - 스피치 조각, 구문 및 의미론적 골격과 같은 다양한 종류의 주석으로 나뉨

- 통계 기반 기법에서는 주변 단어의 빈도를 기초로 단어를 표현함
- 단어의 동시발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집벡터(분산표현)를 획득 → 대규모의 말뭉
치를 다룰 때 문제 발생

![image](https://github.com/user-attachments/assets/553f0439-f407-4370-8ea3-e3c204b45383)

- 특히 통계 기반 기법은 말뭉치 전체의 통계를 이용하여 단 1회의 처리만에 단어의 분산 표현을 확보(시스템의 부하가 큼)

### 추론 기반 기법
- 신경망을 이용한 추론을 기반으로 단어의 분산 표현을 얻는 기법
- 통계 기반 기법과 마찬가지로 분포 가설을 기초로 함

  - 통계 기반 기법: 학습 데이터를 한꺼번에 처리함(배치 학습)
  - 추론 기반 기법: 학습 데이터의 일부를 사용하여 순차적으로 학습함(미니배치 학습)
    - 말뭉치의 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도 학습 가능
    - GPU를 이용한 병렬 계산 가능 → 학습 속도 향상  

- 추론 기반 기법에서 추론
- 주변 단어들을 맥락으로 하여 들어갈 단어를 추측
- 이러한 추론 문제를 반복 수행해 단어의 출현 패턴을 학습

![image](https://github.com/user-attachments/assets/cc7ca349-4936-487b-a0f1-85ab66b2dae2)

- 신경망에서의 단어 처리
  - 단어를 고정 길이의 벡터로 변환(원핫 벡터)
    - 원핫 벡터 : 벡터의 원소 중 1개만 1이고 나머지는 모두 0인 벡터
![image](https://github.com/user-attachments/assets/038c4af1-be05-41c1-9a6c-94201b9488a0)

![image](https://github.com/user-attachments/assets/c7157128-af89-4a1b-b648-1667aa618285)

![image](https://github.com/user-attachments/assets/0b3aeee6-856c-4c86-b52b-4daf7c5b4d56)

- Word2Vec
  - 2013년 Google에서 발표, 현재 가장 많이 사용되고 있는 단어 임베딩 모델
  - 기존 신경망 기반의 단어 임베딩과 구조의 차이는 크지 않으나 계산량을 획기적으로 줄여 빠른 학습을 가능하게 하였음
  - CBOW(Continuous Bag-of-Words) 모델과 skip-gram 모델이 있음
    - CBOW: 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정하여 학습된 가중치 데이터를 임베딩 벡터로 활용
    - skip-gram: 하나의 타깃 단어를 이용하여 주변 단어들을 예측하는 모델(CBOW와 반대)

![image](https://github.com/user-attachments/assets/ad16a628-13b5-498a-8d73-692611602aa5)


- CBOW 모델
  - 맥락으로부터 타겟을 추측하기 위한 신경망 (타겟: 중앙 단어, 맥락: 주변 단어)

![image](https://github.com/user-attachments/assets/76eba5f6-814a-4833-98e1-8a53591443ae)

![image](https://github.com/user-attachments/assets/5263dfa8-e521-4179-8eeb-a72d7be94d03)

![image](https://github.com/user-attachments/assets/1a182fc7-27a8-4970-bdd8-0c984f06ccca)


- skip-gram 모델
  - CBOW 모델에서 맥락과 타겟을 역전시킨 모델
  - 사용하긴 어렵지만 더 성능이 잘나옴

![image](https://github.com/user-attachments/assets/e5f6cf7a-77cd-42fa-aff6-9fcc62033c07)

![image](https://github.com/user-attachments/assets/6fc90a80-d270-4ad8-a35c-4ede627e0bc6)

- skip-gram의 상세 훈련 방식
  - 최대우도법(Maximum Likelihood Estimation, MLE)을 통해 수식을 최대로 만드는 𝜃를 찾는다
![image](https://github.com/user-attachments/assets/3262db48-0d5f-403b-a0f0-1348145d44a3)

![image](https://github.com/user-attachments/assets/43b611b6-f727-4504-977e-865e106c83c8)

- 구조
![image](https://github.com/user-attachments/assets/87b550aa-cc41-4fd6-aa57-1d6615d88b91)


- CBOW 모델 vs. skip-gram 모델
  - Skip-gram 모델: 맥락의 수 만큼 추측 → 손실함수는 각 맥락에서 구한 손실의 총합
  - CBOW 모델: 타겟 하나의 손실
- 정밀도 면에서 skip-gram 모델이 우세
- 말뭉치가 커질수록 저빈도 단어, 유추문제 성능에서 skip-gram 모델이 뛰어남
- 단, 학습 속도는 CBOW 모델이 빠름
- Skip-gram 모델은 맥락의 수 만큼 손실을 구해야 하므로 계산비용이 높아짐

![image](https://github.com/user-attachments/assets/a5bdcbf1-3dce-4951-9f5b-76d82629ebea)

- word2vec 모델의 독특한 성능
- 언어(단어) 사이의 유추 문제를 연산을 통해서 풀 수 있다

![image](https://github.com/user-attachments/assets/14f66aa0-b223-4dde-8d00-38d5a0e2c384)

#### 확률의 관점에서 보는 word2vec
- CBOW 모델
  - 맥락의 단어로부터 타깃 단어를 추측함
  - CBOW 모델의 목적인 “맥락으로부터 타깃을 추측하는 것”은 어디에 활용하이다.

- CBOW 모델을 직접 언어 모델로 적용하면
  - 적당한 규모의 데이터에서는 활용 가능함
  - 그러나 CBOW 모델은 맥락 안의 단어의 순서가 무시된다는 한계를 가지고 있음
  - 따라서 CBOW 모델의 결과물을 제대로 된 언어 모델에 적용하는 것이 더 좋음
  
### 통계 기반 기법 vs 추론 기반 기법

- 새로운 단어가 생겨서 말뭉치에 추가해야 한다?
  - 통계 기반 기법: 처음부터 모두 다시 시작
  - 추론 기반 기법: 현재 상황에서 추가로 학습 가능

- 단어의 분산 표현의 성격이나 정밀도는?
  - 통계 기반 기법: 주로 단어의 유사성이 인코딩 됨
  -  추론 기반 기법: 단어의 유사성, 단어 사이의 패턴도 인코딩 됨

- 실제로는 단어의 유사성을 정량평가 해 보면 둘 다 비슷하다.
- 통계 기반 기법과 추론 기반 기법은 내부적인 계산을 기준으로 비교해보면 서로 연관된 관계 에 있음

## 임베딩
- 생각할 수 있는 가장 간단한 형태의 임베딩(단어의 빈도를 그대로 벡터로 사용하는 것)

- 임베딩의 역할
  - 단어/문장 간 관련도 계산
  - 의미적/문법적 정보 함축
  - 전이 학습

- 단어/문장 간 관련도 계산
  - 단어-문서 행렬 (가장 단순한 형태의 임베딩)
  - Word2Vec(단어를 벡터로 바꾸는 방법, 모델)

- 한국어 위키백과, KorQuAD, 네이버 영화 리뷰 등의 말뭉치를 형태소 분석 후 100차원으로 학습한 Word2Vec 임베딩 한 결과 중
  - 희망이라는 단어를 기준으로 각 쿼리 단어별로 코사인 유사도를 적용한 결과 상위 5개의 단어를 나열하면
![image](https://github.com/user-attachments/assets/d63eb3f8-0056-4530-8a3b-93a6fd724188)

- 차원 축소 후 시각화
  - 100차원의 단어 벡터를 2차원으로 축소하여 시각화한 경우
  - 관련성이 높은 단어들이 주변에 몰려 있음을 확인할 수 있음
![image](https://github.com/user-attachments/assets/da5497e4-99a4-430a-a597-172899e23072)

- 의미적/문법적 정보 함축
  - 임베딩은 벡터로 표현되므로 사칙연산이 가능(단어 벡터 간 덧/뺄셈을 통해, 단어들 사이의 의미적, 문접적 관계를 도출할 수 있음)
    - 첫 번째 단어 벡터 – 두 번째 단어 벡터 + 세 번째 단어 벡터
    - 아들 – 딸 + 소녀 = 소년 → 아들 – 딸 = 소년 – 소녀 (이항 연산을 통해서도 가능)
    -  → 아들-딸 사이의 관계와 소년-소녀 사이의 관계, 의미 차이가 임베딩에 함축되어 있다면
    - → 품질이 좋은 임베딩이라고 할 수 있음

![image](https://github.com/user-attachments/assets/6c8ea422-4c6c-44e7-89fe-b34535d12965)

- 이렇게 단어 임베딩을 평가하는 방법: 단어 유추 평가(Word Analogy Test)

#### 전이학습
- 특정 분야에서 학습된 신경망의 일부 능력을 유사하거나 전혀 새로운 분야에서 사용되는 신경망의 학습에 이용하는 것
- 사람의 경우 : 무엇인가를 배울때 (기존에 쌓아온 지식을 바탕으로 새로운 사실만을 추가로 이해 > 전이학습과 동일한 개념)
- 임베딩에서도 동일하게 활용 가능
  - 대규모 말뭉치 활용해 학습된 임베딩 데이터 확보 (의미적, 문법적 정도 내포)
  - 임베딩 데이터를 입력 값으로 사용하는 전이 학습 모델 (문서 분류 등 태스크를 빠르게 적용 가능)

- 전이 학습의 예
  - 문장의 극성 예측 모델(Bidirectional LSTM 모델 + Attention 적용) 개발
  - 모델의 입력값 : FastText임베딩(100차원)
    - FastText: Word2Vec의 개선된 버전. 59만건의 한국어 문서 사전학습 완료 모델

- 전이 학습 모델에 적용
  - → 모델은 문장을 입력 받으면 해당 문장이 긍정인지 부정인지 출력함
    - 이 영화 꿀잼 + 긍정(Positive)  /  이 영화 노잼 + 부정(Negative)
- → 문장을 형태소 분석한 후 각각의 형태소에 해당하는 FastText 단어 임베딩이 모델의 입력값이 됨
![image](https://github.com/user-attachments/assets/c999cca4-5d17-4853-92ed-63ef00c607b3)


#### 임베딩 기법의 역사와 종류
- 초기 임베딩 : 말뭉치의 통계량을 직접적으로 활용하는 경향 (대표적 : 잠재 의미 분석)

![image](https://github.com/user-attachments/assets/6d82b51b-0970-44eb-a521-269676a86d65)

- 뉴럴 네트워크(신경망 기반 임베딩)
  - 요수아 벤지오 “Neural Probabilistic Language Model” 발표 후부터 주목
  - 이전 단어들이 주어졌을 때 다음 단어가 무엇이 될지 예측, 문장 내 일부분을 비워놓고 해당 단어가 무엇일지 맞추는 과정에서 학습 수행

![image](https://github.com/user-attachments/assets/fe011fd3-f3f6-4211-9783-f432a80f06b8)
- 신경망은 구조가 유연하고 표현력이 풍부 → 자연어의 문맥을 상당 부분 학습 가능

- 단어 수준에서 문장 수준으로
  - 2017년 이전의 임베딩 기법은 대체로 단어 수준 모델
  - NPLM, Word2Vec, GloVe, FastText, Swivel 등
  - 단어 수준 임베딩
    - 각각 벡터에 해당 단어의 문맥적 의미 함축
    - 단점 : 동음이의어 분간이 어려움 (단어의 형태가 같음 동일한 단어로 보고 모든 문맥 정보를 해당 단어 벡터에 투영)

- 2018년 초, ELMo(Embeddings from Language Models) 발표 후, 문장 수준 임베딩 기법이 주목받기 시작
  - BERT(Bidirectional Encoder Representations from Transformer)
  - GPT(Generative Pre-Training) 모델 등
  - 개별 단어가 아닌 단어 시퀀스 전체의 문맥적 의미를 함축 (단어 임베딩 기법보다 전이 학습이 효과가 좋음)

![image](https://github.com/user-attachments/assets/06878c31-8832-4e7a-a74e-e0a963a81eda)
- “배”의 BERT 임베딩 시각화

- Rule → End to End → [Pre-Train(사전학습) / Fine-Tuning(미세조정)](전이학습)
  - 1990년대
    - 대부분의 자연어 처리 모델은 사람이 Feature(모델의 입력값)를 직접 추출
    - Feature 추출 시 언어학적 지식 활용 → Rule 정의
  - 2000년대 중반 ~
    - 자연어 처리 분야에 딥러닝 모델 적용
    - 딥러닝 모델: 입력, 출력 사이의 관계를 잘 근사(Approximation) → 사람이 Rule을 알려주지 않아도 됨
    - End to End 모델 : 데이터를 통째로 모델에 넣고 입출력 사이의 관계를 사람의 개입 없이 모델 스스로 처음부터 끝까지 처리하도록 유도(대표모델: Sequence to Sequence)

- 2018년 이후 ELMo 모델 제안 이후
  - End to End 모델에서 벗어나 Pre-Train, Fine-Tuning 방식으로 발전
  - 대규모 말뭉치로 임베딩 만들기(Pre-Train) → 임베딩에 말뭉치의 의미적, 문법적 맥락 포함
  - 임베딩을 입력으로 하는 새로운 딥러닝 모델 작성
  - 풀고자 하는 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체 업데이트 → Fine-Tuning, 전이학습(Transfer Learning)

- Down-Stream Task
  - 해결하고자 하는 자연어 처리의 구체적인 문제
    - 개체명 인식(Named Entity Recognition, NER)
    - 품사 판별(=품사 태깅)   나는 네가 지난 여름에 한 [일]을 알고 있다. → 일: 명사
    - 문장 성분 분석    [나는 네가 지난 여름에 한 일]을 알고 있다. → 네가 지난 여름에 한 일: 명사구
    - 의존 관계 분석    [자연어 처리는] 늘 그렇듯이 [재미있다]. → 자연어 처리는, 재미있다: 주격 명사구
    - 의미역 분석   [나는 네가 지난 여름에 한 일]을 알고 있다.  → 네가 지난 여름에 한 일: 피행위(Patient)
    - 상호 참조 해결    나는 어제 [성빈이]를 만났다. [그]는 스웨터를 입고 있었다 → 그 : 성빈이

- Up-Stream Task
  - Down-Stream Task에 앞서 해결해야 할 과제 (단어/문제 임베딩의 Pre-Train 작업)

### 임베딩의 종류
  - 행렬 분해 기반 방법
  - 예측 기반 방법
  - 토픽 기반 방법
  - 임베딩 성능 평가

- 행렬 분해(factorization) 기반 방법
  - 말뭉치 정보 들어있는 원리 행렬을 두 개 이상의 작은행렬로 쪼개는것
  - 분해 후 둘 중 하나의 행렬만 쓰거나, 둘을 더하거나 이어 붙여 임베딩으로 사용
  - GloVe, Swivel등

![image](https://github.com/user-attachments/assets/ca031da8-7402-4ff0-95f0-8ce81c3bb05d)

- 예측 기반 방법
  - 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습하는 방법
  - 뉴럴 네트워크 방법들이 예측 기반 방법에 속함
  - Word2Vec, FastText, BERT, ELMo, GPT 등

- 토픽 기반 방법
  - 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행하는 방법
  - 대표적인 기법: 잠재 디히클레 할당(Latent Dirichlet Allocation, LDA)
  - 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환 → 임베딩 기법의 일종으로 받아들여짐


- 임베딩 성능 평가
  - Down-Stream Task 에 대한 임베딩 종류별 성능 분석
  - 성능 측정 대상 Down-Stream Task (형태소분석, 문장 성분 분석, 의존 관계 분석, 의미역 분석, 상호 참조 해결)
  - 파인튜닝 모델의 구조를 고정한 뒤 각각의 임베딩을 전이학습 시키는 형태로 정확도 측정
  - 문장 임베딩 기법(ELMo, GPT, BERT 등)이 단어 임베딩 기법(GloVe 등)을 크게 앞섬
  - 한국어는 공개된 데이터가 적어 측정하기 어려움

### 벡터는 어떻게 의미를 가지게 되는가?
- 컴퓨터가 수행하는 자연어 이해의 본직은 계산
- 임베딩에서 자연어의 의미는 어떻게 함축하는가?
  - 자연어 통계적 패턴 정보를 통쨰로 임베딩에 넣는 것
  - 자연어의 의미는 해당 언어의 화자들이 실제 사용하는 일상 언어에서 드러남

- 임베딩에 사용되는 통계 정보
  - 문장에 어떤 단어가 많이 쓰였는가
  - 단어가 어떤 순서로 등장하는가
  - 문장에 어떤 단어가 같이 나타나는가
![image](https://github.com/user-attachments/assets/b9f6dd0b-7dcd-44b0-b816-0f0cd7b96d99)

- 각 방법의 연관성
  - 언어모델은 단어의 등장 순서, 분포 가정은 이웃 단어(문맥)을 우선
  - 단어가 문장에서 주로 나타나는 순서는 해당 단어의 주변 문맥과 밀접한 관계
  - PMI는 어떤 단어 쌍이 얼마나 자주 같이 나타나는지 정보를 수치화, 그를 위해 개별 단어, 단어쌍의 빈도 정보를 활용
  - BOW, 언어모델, 분포가정은 말뭉치의 통계적 패턴을 서로 다른 각도에서 분석해 상호 보완적으로 동작
 
- BOW(Bag of Words) : 자연어 처리분야에서 BOW란 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 말함 (수학에서는 중복 원소를 허용한 집합을 의미, 원소 순서는 고려x : 어떤 단어가 많이 쓰였는가?)
![image](https://github.com/user-attachments/assets/7003a666-7a73-4460-8ba2-0b7cd754a51d)

- BOW의 가정
  - 저자가 생각한 주제가 문서에서의 단어 사용에 녹아있음
  - 주제가 비슷한 문저라면 단어의 빈도 또는 단어의 등장여부도 비슷할 것이므로 BOW 임베딩 역시 유사
  - 빈도를 그대로 BOW로 쓴다면 많이 쓰인 단어가 주제와 더 강한 관련을 맺고 있음

- BOW의 단점
  - 단어의 빈도, 등장 여부를 그대로 임베딩으로 쓰는 경우
  - 해당 단어가 어떤 문서에서든 많이 쓰이는 경우, 문서의 주제를 가늠하기 어려움
- BOW 단점을 개선하기 위해 나온것이 TF-IDF(Term Frequency-Inverse Document Frequency) 기법

- TF-IDF
![image](https://github.com/user-attachments/assets/23f0ee0c-0af5-4c53-9a59-c8771fb7950d)

- 위 가중치에서
  - TF: 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지에 대한 빈도 수 (많이 쓰인 단어가 중요하다는 가정을 전제로 한 수치)
  - DF: 특정 단어가 나타난 문서의 수(DF가 클 수록 다수 문서에 사용되는 범용적인 단어)

- TF는 같은 단어라도 문서마다 다른 값, DF는 문서가 달라져도 단어가 같다면 동일한 값
- IDF : 전체 문서 수(N)을 해당 단어의 DF로 나눈 뒤 로그 취한 값
  - 값이 클수록 특이한 단어
  - 단어의 주제 예측 능력(해당 단어만 보고 문서의 주제를 가늠해 볼 수 있는 정도)와 직결

- TF-IDF의 지향점
  - 어떤 단어의 주제 예측 능력이 강할수록 가중치가 커지고, 그 반대의 경우는 작아지는 것을 목표로 학습 진행
![image](https://github.com/user-attachments/assets/3971be5d-6cc2-44ef-9c3f-fde4bd08a03b)

- BOW의 경우
  - 문장 내에 어떤 단어가 쓰였는지, 쓰였다면 얼마나 많이 쓰였는지에 대한 빈도만 계산
  - 해당 문서가 어떤 범주인지 분류(classification)하는 문제에 대해서는 간단한 아키텍처임에도 불구하고 성능이 좋음 → 해당 문제에서는 현업에서도 자주 활용됨

- 단어가 어떤 순서로 쓰였는가
  - 통계 기반 언어 모델
    - 단어 시퀀스에 확률을 부여하는 모델
    - 단어의 등장 순서를 무시하는 BOW와 달리 시퀀스 정보를 명시적으로 학습
    - BOW의 반대편에 있는 모델이라고 볼 수 있음
- n 개의 단어가 주어졌다면 언어 모델은 n개 단어가 동시에 나타날 확률, P(w1,w2,...,wn)을 반환
- 통계 기반의 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습

- 잘 학습된 언어 모델이 있으면  
  - 어떤 문장이 활률값이 높은지
  - 주어진 단어 시퀀스 다음 단어는 무엇이 오는 것이 자연스러운지 알 수 있음
![image](https://github.com/user-attachments/assets/ce8653e7-01e8-48a0-95f5-37bdf595825e)

- n-gram
  - n개의 단어를 뜻함
  - 난폭, 운전 : 2-gram (=bigram)
  - 눈, 뜨다 : 2-gram (=bigram)
  - 누명, 을, 쓰다 : 3-gram (=trigram)
  - 바람, 잘, 날, 없다 : 4-gram
- 경우에 따라서 n-gram은 n-gram에 기반한 언어모델을 의미
  - 말뭉치 내 단어들을 n개씩 묶어서 그 빈도를 학습했다는 의미

- 네이버 영화 리뷰 말뭉치에서
  - 각 표현이 등장한 횟수
  - 띄어쓰기 단위인 어절을 하나의 단어로 보고 빈도 계산
  - “내 마음 속에 영원히 기억될 최고의 명작이다.” 의 경우, 말뭉치에서 한 번도 등장하지 않음 → 언어모델은 해당 표현이 나타날 확률을 0으로 부여
  - 문법적, 의미적으로 결함이 없는 훌륭한 한국어 문장이지만 언어모델 은 해당 표현을 말이 되지 않는 문장으로 취급함

![image](https://github.com/user-attachments/assets/b18e1ad6-4f45-4ba9-90f3-6e3c22253ebb)
- 네이버 영화 말뭉치의 각 표현 별 등장 횟수

- 내 마음 속에 영원히 기억될 최고의” 다음에 “명작이다”가 나타날 확률의 계산
  - 조건부 확률(conditional probability)의 정의를 활용하여 최대우도추정법(Maximum Likelihood Estimation)으로 유도할 수 있음

![image](https://github.com/user-attachments/assets/31b3413b-825a-4d01-89ed-ddb823c3457b)

  - 우변의 분자가 0이 되므로 전체 값은 0
- 이 문제를 n-gram 모델을 이용하여 해결 가능함
  - 직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스의 등장 확률을 근사
  - “한 상태의 확률은 그 직전 상태에만 의존한다”라는 Markov Assumption(마르코프 가정)에 기반

![image](https://github.com/user-attachments/assets/f3481de4-3ce4-44ad-9507-fe30a528518b)

- 바이그램 모델에서 “내 마음 속에 영원히 기억될 최고의 명작이다”라는 단어 시퀀스가 나타날 확률은?

![image](https://github.com/user-attachments/assets/979c4967-2a59-4553-b34a-66a3da53380a)
- 단어를 하나씩 이어가면서 끝까지 계산한 결과가 원하는 확률이 됨

- 바이그램 모델의 일반화 수식
![image](https://github.com/user-attachments/assets/e0947e0b-8bf4-4441-84c7-ea47eefb3670)

- 데이터에 한 번도 등장하지 않는 n-gram이 존재한다면 예측 단계에서 문제 발생 
  - 또바기: “언제나 한결같이 꼭 그렇게”라는 뜻을 가진 한국어 부사
  - 학습 데이터에서 “아이는” 다음에 “또바기”라는 단어가 한 번도 등장하지 않았다면 → 언어 모델은 예측 단계에서 “그 아이는 또바기 인사를 잘한다”라는 문장이 등장할 확률을 0으로 부여
 
![image](https://github.com/user-attachments/assets/0b7030c1-3690-4223-91e5-578a75b6a831)
- n=gram 모델의 한계

- 한계 해결을 위해 Back-Off, Smoothing 등의 방식 제안

- Back off
  - n-gram 등장 빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식
  - n을 크게 하면 할수록 등장하지 않는 케이스가 많아질 가능성이 높다

- Smoothing(각진 부분 부드럽게 만드는것)
  - 등장 빈도 표에 모두 k 만큼을 더하는 기법
  - K=1인 경우, Laplace Smoothing(라플라스 스무딩)
  - Smoothing을 시행하면 높은 빈도를 가진 문자열 등장 확률을 일부 깎고, 학 습 데이터에 전혀 등장하지 않는 케이스들에는 작은 값이지만 일부 확률을 부여함

- 뉴럴 네트워크 기반 언어 모델
- 뉴럴 네트워크
  - 입력과 출력 사이의 관계를 유연하게 포착
  - 그 자체로 확률모델로 기능(통계 기반 언어 모델 대신)
- 주어진 단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습 수행
- 학습이 완료되면 모델의 중간 혹 말단 계산 결과물을 단어나 문장의 임베딩으로 활용
- ELMo, GPT 등 모델이 해당
- 단어를 순차적으로 입력받아 다음 단어 맞춰야함(일방향)

- 마스크 언어 모델
  - 문장 중간에 마스크를 씌우고 해당 마스크 위치에 어떤 단어가 올지 예측하는 과정에서 학습 수행
  - 문장 전체를 다 보고 중간 단어를 예측하므로 양방향 학습이 가능
  - BERT 모델이 여기 해당


### 어떤 단어가 같이 쓰였는가

#### 분포 가정
- (자연어 처리에서의) 분포: 특정 범위 → 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 가리킴
- 개별 단어의 분포는 그 단어가 문장 내 주로 어느 위치에 나타나는지, 이웃한 위치에 어떤 단어가 자주 나타나는지에 따라 달라짐
- 어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장하면, 그 의미 또한 유사
- 문맥을 살핌으로 그 단어의 의미를 밝힐 수 있다는 주장
- 빨래, 세탁이라는 단어 모를때 둘을 타겟 단어로 잡고, 청소, 물 등을 주위에 등장하는 문맥 단어로 잡음
- 이를 통해 의미의 추측을 진행
- 각 단어의 의미는 추측이 가능해 졌지만 개별 단어의 분포, 의미 사이의 논리적 연관성이 드러나지 않음 -> “분포 정보가 곧 의미”라는 분포 가설에 의문 제기 가능

- 형태소
  - 의미를 가지는 최소 단위 → 더 쪼개면 의미가 사라짐. ‘의미’에는 어휘적, 문법적 의미 포함
  - 언어학자들은 형태소를 계열 관계를 바탕으로 분석함( 언어학자들은 특정 타겟 단어 주변의 문맥 정보를 바탕으로 형태소를 확인, 말뭉치의 분포 정보와 형태소가 밀접한 관계를 이룸)
  
- 품사
  - 품사: 문법적 성질의 공통성에 따라 언어학자들이 각 단어를 몇 갈래로 묶어 놓은 것
  - 품사의 분류 기준: 기능, 의미 , 형식
    - 기능: 한 단어가 문장 가운데서 다른 단어와 맺는 관계
    - 의미: 단어의 형식적인 의미
    - 형식: 단어의 형태적 특징

![image](https://github.com/user-attachments/assets/2c64f5ee-d51f-4380-a415-53a6ef0090f4)

- 품사에서 가장 중요한 것은 기능
  - 한국 비롯한 많은 언어에서 어떤 단어의 기능이 그 단어의 분포와 매우 밀접
  - 기능: 특정 단어가 문장 내에서 어떤 역할을 하는지, 분포: 그 단어가 어느 자리에 있는지
  - 개념적으로는 엄밀히 다르지만 밀접한 관련이 있다.
- 형태소의 경계를 정하거나 품사를 나누는 등의 언어학적 문제는 말뭉치의 분포 정보와 깊은 관계 존재
- 임베딩에 분포 정보를 함축하면 해당 벡터에 해당 단어의 의미를 내재 시킬 수 있다

- 한국어 품사 분류의 일반적 기준
![image](https://github.com/user-attachments/assets/63ba074d-cb5b-4e38-bcc6-e7f930b8de76)

- 단어 문맥 행렬 구축 과정
![image](https://github.com/user-attachments/assets/dbff0c39-34ff-4f8b-bd9f-0cfe88ff1fd1)










```
