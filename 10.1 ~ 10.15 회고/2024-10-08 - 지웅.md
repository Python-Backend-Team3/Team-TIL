# 📝 2024.10.08 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 임베딩
- 단어나 문장을 수치화 하여 벡터공간으로 표현하는 과정
  - 컴퓨터는 자연어를 직접적으로 처리할 수 없다
  - 컴퓨터는 수치 연산만 가능하다
  - 따라서 자연어를 숫자나 벡터 형태로 변환할 필요가 있다
  - 이때 사용되는 일련의 과정을 “임베딩”이라고 한다
- 임베딩된 결과는 딥러닝 모델의 입력값으로 사용된다.

###### 임베딩 기법 종류
- 문장 임베딩
  - 문장 전체를 벡터로 표현하는 방법
  - 전체 문장 흐름을 파악해 벡터로 변환(통으로) → 문맥적 의미를 지님
  - 단어 임베딩에 비해 품질이 좋음, 상용 시스템중 품질이 좋아야하거나 번역이 좋아야하는 경우 사용
  - 학습을 위한 수 많은 문장 데이터가 필요하며 학습 비용이 매우 높다.
 
###### 단어 임베딩
  - 개별 단어를 벡터로 표현하는 방법
  - 동음어 구분 하지 않음 → 의미가 달라도 단어의 형태가 같으면 동일한 벡터값을 가짐(단점)
  - 문장 임베딩에 비하면 학습 방법이 간단함 → 성능은 떨어지지만 실무에서 많이 사용(비용)

- 단어 임베딩
  - 말뭉치에서 각각 단어를 벡터로 변환
  - 의미와 문법적 정보를 지님
  - 단어를 표현하는 방법에 따라 다양한 모델 존재
  - 토크나이징을 통해 문장에서 토큰 단위로 추출하는 경우, 추출된 토큰은 형태소 기반이므로 단어 임베딩에 효과적이다.

- 원핫 인코딩
  - 단어를 숫자 벡터로 변환하는 가장 기본적인 방법
  - 요소들 중 단 하나의 값만이 1이고 나머지 요소들의 값은 0인 인코딩 방식
![image](https://github.com/user-attachments/assets/f7ce4efe-3fdf-46f2-937e-9a2de2005222)

- 원핫 인코딩 벡터의 차원은 전체 어휘의 개수 → 매우 큰 차원이 됨
![image](https://github.com/user-attachments/assets/a5eaec03-e20e-489a-aabd-f603fb6b5325)

- 단어는 불연속적인 심볼이며 이산 확률 변수로 나타남 → 원핫 벡터는 이산 확률 분포에서 추출한 샘플 → 불연속적인 값을 가진다.
![image](https://github.com/user-attachments/assets/5ef3be6a-1d3f-4696-8f53-558bcdcae487)


- 희소표현
  - 원-핫 인코딩: 표현하고자 하는 단어의 인덱스 요소만 1이고 나머지 요소는 모두 0으로 표현되는 희소 벡터(또는 희소 행렬) → 단어가 희소 벡터로 표현되는 방식: 희소 표현
  - 희소표현 : 각각의 차원이 독립적인 정보를 지님
  - 자연어처리를 잘 하려면 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야함(희소 표현은 이런 조건을 만족하지 못함)

- 분산 표현
  - 희소 표현의 단점 해결을 위해 고안됨
  - 각 단어간 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법
  - 한 단어 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현됨
  - 비유를 통한 예시(색상을 표시하는 RGB 모델 → 3차원 벡터, 분산 표현 방식)
  - 희소 표헨에 비해 많은 장점을 가지므로 단어 임베딩 기법에서 많이 사용됨
- 분산표현을 그림으로 이해하면 아래와 같다.

![image](https://github.com/user-attachments/assets/61311291-5405-4513-827e-f47612a0b859)

- 신경망에서는 분산 표현을 학습하는 과정에서 임베딩 벡터의 모든 차원에 의미있는 데이터를 고르게 밀집시 킴 → 희소표현과 반대로 데이터 손실을 최소화 하면서 벡터 차원이 압축되는 효과를 가짐


- 희소 표현 대신 분산 표현을 사용하면
  - 임베딩 벡터의 차원을 데이터 손실을 최소화하면서 압축할 수 있다.
  - 임베딩 벡터에 단어의 의미, 주변 단어와의 관계 등 많은 정보가 내포되어 있다.(일반화 능력이 뛰어남)

- 분산 표현 방식의 벡터 공간
![image](https://github.com/user-attachments/assets/de60fd9d-135e-4c2b-9a0a-87e80f7eeacd)


### 단어의 의미 파악
- 단어의 의미를 파악하기 위한 기법들
  - 시소러스 활용 기법
  - 통계 기반 기법
  - 추론 기반 기법

##### 시소러스 활용 기법
- 사람의 경우
  - 단어의 이해를 위해 사전 활용 (사전에 단어의 의미를 정의)
    - 컴퓨터도 이렇게 할 수 있지 않을까? 라는 데에서 나온 아이디어

- 시소러스 : 유의어 사전, 어휘 분류 사전
  - 뜻이 같은 단어(동의어), 뜻이 비슷한 단어(유의어)가 한 그룹으로 분류되어 있는 사전

- 단어들을 의미의 상/하위 관계에 따라 그래프로 표현, 처리
  - 단어들의 의미의 상, 하위 관계에 기초해 그래프로 계층화 시키고 코드화 시킴
  - 모든 단어에 대한 유의어 집합을 만들고 단어들의 관계를 그래프로 표현한 뒤, 단어 사이의 연결을 정의하고, 단어의 네트워크를 이용해 컴퓨터에게 단어 사이의 관계를 주입해 컴퓨터에게 단어의 의미를 이해시킨 것으로 간주함

- 시소러스의 문제점
  - 시대 변화에 대응하기 어려움(언어는 변한다, 신조어, 사멸어, 의미의 변화, 활용의 변화 -> 사람이 꾸준히 갱신, 관리)
  - 시소러스를 만들고 관리하기 위한 고가의 인적비용이 발생한다.(WordNet에 등록된 단어는 20만개)
  - 단어의 미묘한 차이를 표현할 수 없다.

- 문제 해결을 위해 통계 기반 기법, 신경망을 이용한 추론 기반 기법 등이 제안됨

- 시소러스의 대표적 모델 : WordNet

##### WordNet
  - 프린스턴 대학교 심리학 교수 “조지 아미티지 밀러”의 지도 하에 1985년부터 개발
  - 기계번역을 돕기 위한 목적으로 개발됨
  - 동의어 집합, 상위어, 하위어에 대한 정보가 잘 구축되어 있음
  - 유향 비순환 그래프(Directed Acyclic Graph, DAG)로 구성됨
  - (트리구조가 아닌 이유: 하나의 노드가 여러 개의 상위 노드를 가질 수 있음)

- WordNet 내의 단어 별 top-1 의미의 top-1 상위어만 선택하여 트리 구조로 나타낸 경우
![image](https://github.com/user-attachments/assets/f59a154e-f9f4-4563-89f2-1e791d26da95)

- 표제어 읽는 방법
![image](https://github.com/user-attachments/assets/adcceed0-8a08-48ca-a55c-2fdd42efb0ef)

- 한국어 WordNet
  - KorLex 부산대학교 http://korlex.pusan.ac.kr
  - Korean WordNet(KWN) KAIST http://wordnet.kaist.ac.kr
  
- 특징
  - 단어들은 그 내용을 설명할 수 있는 다양한 특징을 가짐
  - 특징벡터 : 이러한 특징 별 수치를 모아 벡터로 표현한 것

- 단어의 특징 벡터를 구성하기 위한 가정
  - 의미가 비슷한 단어라면 쓰임새가 비슷할 것
  - 쓰임새가 비슷하므로 비슷한 문장 안에서 비슷한 역할로 사용될 것
  - 따라서 함께 나타나는 단어들이 유사할 것

- 특징 추출하기
  - TF-IDF(Term Frequency-Inverse Document Frequency)
  ![image](https://github.com/user-attachments/assets/9aa70c65-dcb2-492e-ad33-5c8f3dc8de6d)


### 단어의 의미 파악 : 통계 기반 기법
- 통계 기반 기법
  - 통계를 기반으로 어떤 데이터가 많이 사용되었는지, 어떻게 분포되 있는지, 분포에 따라 어떤 연관성과 의미를 갖는지 등의 정보를 추출하여 자연어를 처리하는 기법
  - 통계를 사용하므로 통계 결과를 계산히기 위한 충분한 크기를 가진 대규모 텍스트 데이터가 요구됨

- 말뭉치를 이용함
  - 말뭉치 : 자연어 처리에 대한 연구, 애플리케이션의 개발 등을 염두에 두고 수집된 대량의 텍스트 데이터
  - 말뭉치 자체는 단순한 텍스트 데이터이지만 텍스트에 담긴 문장은 사람이 쓴 글이다.
  - 말뭉치에서 자동으로, 효율적으로 핵심을 추출하는 것(통계 기반의 자연어 처리 기법)

- 분포 가설
  - 단어의 의미는 주변 단어에 의해 형성
  - 단어 자체에는 별 의미가 없고 그 단어가 사용된 맥락이 의미를 형성
  - 자연어 처리에 관한 중요한 기법은 대부분 분포가설을 기반으로 한다.

![image](https://github.com/user-attachments/assets/0b05d113-5c56-4f01-9f41-ba9877ae9b8e)

![image](https://github.com/user-attachments/assets/b142231e-aeab-47ba-b2a5-6e8b708a3451)

- 분포 가설을 기초로 통계적으로 분석
  - 일단 어떤 단어가 몇 번이나 나오는지 세어보는 것이 핵심
  - 동시발생 행렬 : 주어진 단어의 맥락으로써 동시에 발생하는 단어의 출현빈도
    - 맥락의 크기를 윈도우라하고, 윈도우가 1인 경우(주어진 단어에서 한 단어까지 처리)
![image](https://github.com/user-attachments/assets/30142f5f-e9ed-4d79-b85e-d74296e55e95)

- 맥락이란 특정 단어를 중심에 둔 주변 단어를 말함
  - 맥락의 크기 : 윈도우
![image](https://github.com/user-attachments/assets/4eb99229-0cb2-44b1-ab4a-b37c9f5add26)

- 동시 발생 행렬
  - 분포 가설에 기초해 단어를 벡터로 바꾸는 방법
  - 단어가 몇개 나오는지 세어봄(통계 기반 기법)+
![image](https://github.com/user-attachments/assets/d6a39f4a-075d-4e47-8052-962dbb1e3b93)

![image](https://github.com/user-attachments/assets/549478da-b331-41df-aded-54eb49556437)

![image](https://github.com/user-attachments/assets/b4ede6c8-6de0-4a19-bd67-7ce108d0d911)


- 벡터 간 유사도
  - 동시발생 행렬을 이용해 단어를 벡터로 표현
  - 벡터 사이 유사도를 계산하여 해당 단어 간의 유사도, 관련성을 확인 가능
  - 벡터 간 유사도 계산 방법
    - 벡터의 내적
    - 유클리드 거리
    - 코사인 유사도: 단어 벡터 간의 유사도를 나타낼 때는 코사인 유사도를 많이 사용(효율 정확도 좋음)
![image](https://github.com/user-attachments/assets/e7d42fc1-6030-4078-8a0a-7ae2483833ee)

- 코사인 유사도 → 두 벡터가 가리키는 방향이 얼마나 비슷한가?를 확인, 벡터의 방향이 완전히 같다면 코사인 유사도 1, 완전히 반대라면 -1

- 유사 단어의 랭킹 표시
  - 유사도 높은 순으로 순위 매김
  - 입력 데이터인 말뭉치(텍스트 데이터)가 커질수록 랭킹이 정확해짐
- 이러한 과정을 거쳐 도출된 각 단어의 유사도 랭킹(순위)에 따라 단어, 문맥(맥락), 문장에 해당하는 의미를 선택하고 그 결과를 활용하는 것이 통계 기반의 기법

- 통계 기반 기법에서는 단어의 동시발생 행렬을 이용
- 발생횟수, 단어의 빈도수는 많이 활용되는 특성이지만 좋은 특성은 아님
- 전문적인 내용의 말뭉치일 경우, 해당 주제에 관련된 말이 비정상적으로 많이 나올 수 있음
- 영어권 언어에서는 관사 등 실제 의미와 무관한 단어가 최고 빈도수를 가진다.


- 점별 상호 정보량(Pointwise Mutual Information, PMI)
![image](https://github.com/user-attachments/assets/30e6768e-476b-4b41-9366-3c3f2ba203a6)
- 𝑃(𝑥): 𝑥가 일어날 확률, 𝑃(𝑦): 𝑦가 일어날 확률, 𝑃(𝑥, 𝑦): 𝑥와 y가 동시에 일어날 확률
- 𝑃𝑀𝐼 값이 높을수록 관련성이 높음

- 말뭉치에 적용시
  - 𝑃(𝑥): 단어 𝑥가 말뭉치에 등장할 확률
  - 𝑃(𝑥, 𝑦): 단어 𝑥와 y가 말뭉치에 동시에 등장할 확률 
![image](https://github.com/user-attachments/assets/2576cb7e-f86e-4caf-9cde-5c9babb2ba5e)

- 동시 발생 행렬을 사용해 다시 정리하면
![image](https://github.com/user-attachments/assets/b0ec06e4-2733-4919-b178-ff24110d9c35)

- 예) 말뭉치 단어 수가 10,000개, “the” 1,000번, “car” 20번, “drive” 10번 등장했고, “the”, ”car”의 동시발생 수 는 10회, “car”, “drive”의 동시발생 수는 5회라고 가정

![image](https://github.com/user-attachments/assets/1dcd04d8-ac72-46e3-b103-aa58fcfbb9ec)

- car , drive 의 관련성이 더 높은 것을 확인할 수 있음

- PMI 문제점
  - 두 단어의 동시발생 횟수가 0이면 𝑙𝑜𝑔20 = −∞가 됨
  - 문제 해결을 위해 실제 구현 시 양의 상호 정보량(PPMI)사용
![image](https://github.com/user-attachments/assets/9772b96d-a503-434b-8990-dfca4ae8b0c5)
- 𝑃𝑀𝐼 / 𝑃𝑃𝑀𝐼 모두 말뭉치의 어휘 수 증가에 따라 단어벡터의 차원 수가 증가 → 차원 감소를 위한 방안이 요구됨

#### 차원 감소
- 벡터의 차원을 줄이는 기법
- 단순한 차원 감소가 아닌 중요한 정보는 최대한 유지하며 줄임
- 단어 벡터는 주로 원핫 인코딩 같은 희소벡터(희소행렬)을 사용하므로 희소벡터에서 중요한 축을 찾아내 더 적은 차원으로 다시 표현하는 것이 핵심
- 차원 감소 결과는 원소 대부분이 0이 아닌 값으로 구성된 밀집벡터가 됨(단어의 분산 표현)
- 다양한 차원감소 기법 중 단어 처리에서 주로 사용되는것(특잇값 분해 Singular Value Decomposition(SVD))

- 특잇값 분해(SVD)
  - 주어진 행렬(X)을 3개의 행렬(U,S,V)의 곱으로 분해 → 𝑿 = 𝑼𝑺𝑽^𝑻
  - U, V: 직교행렬(열 벡터가 서로 직교함) → 어떤 공간의 축(기저)을 형성. U행렬은 단어 공간
  - S: 대각행렬(대각성분 외에는 모두 0) → 대각성분은 특잇값(=해당 축의 중요도) → 중요도가 낮은 원소(특잇값이 작은 원소)를 깎아내어 차원 축소에 반영
![image](https://github.com/user-attachments/assets/3fd3e8b7-71d2-4803-9c60-3cef924a6654)

- PTB(Penn Treebank) 데이터 셋
  - 펜실베이니아 대학교에서 관리하는 데이터 셋
  - NLP(자연어 처리) 연구를 위한 기계 학습에 널리 사용되는 데이터 셋
  - 대부분의 최신 데이터 집합에 비해 상대적으로 작아서 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용됨
  - 대문자, 숫자 및 문장 부호가 포함되어 있지 않으며 어휘는 10k 고유 단어로 제한됨
  - 스피치 조각, 구문 및 의미론적 골격과 같은 다양한 종류의 주석으로 나뉨

- 통계 기반 기법에서는 주변 단어의 빈도를 기초로 단어를 표현함
- 단어의 동시발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집벡터(분산표현)를 획득 → 대규모의 말뭉
치를 다룰 때 문제 발생

![image](https://github.com/user-attachments/assets/553f0439-f407-4370-8ea3-e3c204b45383)

- 특히 통계 기반 기법은 말뭉치 전체의 통계를 이용하여 단 1회의 처리만에 단어의 분산 표현을 확보(시스템의 부하가 큼)

### 추론 기반 기법
- 신경망을 이용한 추론을 기반으로 단어의 분산 표현을 얻는 기법
- 통계 기반 기법과 마찬가지로 분포 가설을 기초로 함

  - 통계 기반 기법: 학습 데이터를 한꺼번에 처리함(배치 학습)
  - 추론 기반 기법: 학습 데이터의 일부를 사용하여 순차적으로 학습함(미니배치 학습)
    - 말뭉치의 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도 학습 가능
    - GPU를 이용한 병렬 계산 가능 → 학습 속도 향상  

- 추론 기반 기법에서 추론
- 주변 단어들을 맥락으로 하여 들어갈 단어를 추측
- 이러한 추론 문제를 반복 수행해 단어의 출현 패턴을 학습

![image](https://github.com/user-attachments/assets/cc7ca349-4936-487b-a0f1-85ab66b2dae2)

- 신경망에서의 단어 처리
  - 단어를 고정 길이의 벡터로 변환(원핫 벡터)
    - 원핫 벡터 : 벡터의 원소 중 1개만 1이고 나머지는 모두 0인 벡터
![image](https://github.com/user-attachments/assets/038c4af1-be05-41c1-9a6c-94201b9488a0)

![image](https://github.com/user-attachments/assets/c7157128-af89-4a1b-b648-1667aa618285)

![image](https://github.com/user-attachments/assets/0b3aeee6-856c-4c86-b52b-4daf7c5b4d56)

- Word2Vec
  - 2013년 Google에서 발표, 현재 가장 많이 사용되고 있는 단어 임베딩 모델
  - 기존 신경망 기반의 단어 임베딩과 구조의 차이는 크지 않으나 계산량을 획기적으로 줄여 빠른 학습을 가능하게 하였음
  - CBOW(Continuous Bag-of-Words) 모델과 skip-gram 모델이 있음
    - CBOW: 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정하여 학습된 가중치 데이터를 임베딩 벡터로 활용
    - skip-gram: 하나의 타깃 단어를 이용하여 주변 단어들을 예측하는 모델(CBOW와 반대)

![image](https://github.com/user-attachments/assets/ad16a628-13b5-498a-8d73-692611602aa5)


- CBOW 모델
  - 맥락으로부터 타겟을 추측하기 위한 신경망 (타겟: 중앙 단어, 맥락: 주변 단어)

![image](https://github.com/user-attachments/assets/76eba5f6-814a-4833-98e1-8a53591443ae)

![image](https://github.com/user-attachments/assets/5263dfa8-e521-4179-8eeb-a72d7be94d03)

![image](https://github.com/user-attachments/assets/1a182fc7-27a8-4970-bdd8-0c984f06ccca)







```
