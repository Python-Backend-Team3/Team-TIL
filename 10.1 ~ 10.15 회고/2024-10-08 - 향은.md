## 2024-10-08 회고

### 임베딩이란?
- 컴퓨터는 자연어를 이해하지 X
  - 수차 데이터를 계산하여 사람에게 그럴듯하게 보이도록 표현
- 컴퓨터의 자연어 이해와 생성은 **연산과 처리의 한 부분**

> **대전제 : 언어의 표현력은 무한하다.**
- 컴퓨터가 무한한 언어의 표현력을 연산할 수 있도록 수치화가 가능한가?
- 가능하다면 말과 글을 숫자로 변환할 때 어떤 정보를 함축시킬 것인가?
- 정보 압축 과정에서 손실이 발생하지 않을까?
- 그 손실은 어떻게 줄일 수 있을 것인가?

#### 이러한 문제를 해결해 나가는 과정이 자연어 이해의 연구 과정

### 자연어 처리에서의 임베딩이란?
사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 벡터로 바꾼 결과

단어나 문장 각각을 벡터로 변환하여 벡터 공간으로 끼워 넣는다는 의미를 가짐

### 임베딩의 역할
- 단어/문장 간 관련도 계산
  - "희망"이라는 단어를 기준으로, 100차원으로 임베딩 -> 벡터 요소는 100개가 됨
  - 단어 벡터를 봐서는 의미를 이해할 수 X -> 단어를 벡터로 표현 -> 벡터 사이의 유사도 계산이 가능해짐
  - 각 쿼리 단어별로 코사인 유사도를 적용한 결과 상위 5개의 단어를 나열
- 의미적/문법적 정보 함축
- 전이 학습
  - 모델은 문장을 입력 받으면 해당 문장이 긍정인지 부정인지 출력
  - 형태소를 분석한 후 각각의 형태소에 해당하는 FastText 단어 임베딩이 모델의 입력값이 됨

  ![image](https://github.com/user-attachments/assets/8759b596-1376-41db-893d-190535f1be0a)

### 신경망 기반 임베딩 - 뉴럴 네트워크
- 이전 단어들이 주어졌을 때, 다음 단어가 무엇이 될지 예측 or 맞추는 과정에서 학습 수행
- 신경망은 구조가 유연하고 표현력이 풍부 -> 자연어의 문맥을 상당 부분 학습 가능

![image](https://github.com/user-attachments/assets/920e7367-0202-41ae-a1e6-24e72385d0e2)


### 임베딩 종류
- 행렬 분해 기반 방법
- 예측 기반 방법
- 토픽 기반 방법
- 임베딩 성능 평가


### 행렬 분해(factoriazation) 기반 방법
- 말뭉치 정보가 들어있는 원리 행렬을 두 개 이상의 작은 행렬로 쪼개는 방식
- 분해 -> 둘 중 하나의 행렬만 씀 or 둘을 더하거나 or 이어 붙여 임베딩으로 사용

![image](https://github.com/user-attachments/assets/37693d6a-6cba-47d6-823b-325c40f38ec8)

### 예측 기반 방법
- 어떤 단어 주변에 특정 단어가 나타날지 예측 or
- 이전 단어들이 주어졌을 때 다음단어가 무엇일지 예측 or
- 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맟주는 과정에서 학습하는 방법
- 뉴럴 네트워크 방법들이 예측 기반 방법에 속함

### 토픽 기반 방법
- 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행하는 방법
- 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환

### 임베딩 성능 평가
- Down-Stream Task 에 대한 임베딩 종류별 성능 분석
- 형태소 분석, 문장 성분 분석, 의존 관계 분석, 의미역 분석, 상호 참조 해결 등
- 파인튜닝 모델의 구조를 고정한 뒤 각각의 임베딩을 전이학습 시키는 형태로 정확도 측정

### BOW(Bag of Words)
- 수학에서 중복 원소를 허용한 집합을 의미
- 원소의 순려는 고려 x
- 자연어 처리 분야에서 BOW란 단어의 등장 순서에 관계없이 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 말함

![image](https://github.com/user-attachments/assets/b32f999e-7af6-4673-87ed-b7a6563c562c)

#### BOW의 가정
- 저자가 생각한 주제가 문서에서의 단어 사용에 녹아 있다
- 주제가 비슷한 문서라면 단어의 빈도 또는 단어의 등장여부도 비슷할 것이므로 BOW 임베딩 역시 유사할 것이다
- 빈도를 그대로 BOW로 쓴다면 많이 쓰인 단어가 주제와 더 강한 관련을 맺고 있을 것이다

#### BOW의 단점
- 단어의 빈도, 등장 여부를 그대로 임베딩으로 쓰는 경우, 문서의 주제를 가늠하기 어려움
  -> 단점 개선을 위해 TF-IDF기법 제안

###  TF-IDF(Term Frequency-Inverse Document Frequency)
: 단어-문서 행렬에 TF-IDF(w) = TF(w) x log(N/DF(w)과 같이 가중치를 계산하여 행렬의 원소를 바꿈
- TF : 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지에 대한 빈도 수
- DF : 특정 단어가 나타난 문서의 수
- IDF : 전체 문서 수(N)를 해당 단어의 DF로 나눈 뒤 로그를 취한 값 -> 값이 클수록 특이한 단어
  -> 단어의 주제 예측 능력(해당 단어만 보고 문서의 주제 가늠할 정도)과 직결
- TF는 같은 단어라도 문서마다 다른 값, DF는 문서가 달라져도 단어가 같다면 동일한 값을 가짐

![image](https://github.com/user-attachments/assets/dece02df-704a-4422-863e-be01623ad4f4)

- 어떤 단어의 주제 예측 능력이 강할수록 가중치가 커지고, 그 반대의 경우는 작아짐
- 어떤 단어의 TF가 높으면 TF-IDF값도 커짐 -> 단어 사용 빈도는 저자가 상정한 주제와 관련을 맺고 있을 것이라는 가정에 기초

### 통계 기반 언어 모델
- 단어 시퀀스에 확률을 부여하는 모델
- 단어의 등장 순서를 무시하는 BOW와 달리 시퀀스 정보를 명시적으로 학습
- n개의 단어가 주어졌다면 언어 모델은 n개 단어가 동시에 나타날 확률 P(w_1,w_2,...,w_n)을 반환
- 통계 기반의 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습

![image](https://github.com/user-attachments/assets/a4f26a5e-5bbc-4a26-8945-048b9ce281f3)

#### n-gram
- n개의 단어를 뜻함
- 난폭, 운전 : 2-gram(=bigram)
- 눈, 뜨다 : 2-gram(=bigram)
- 누명, 을, 쓰다 : 3-gram(=trigram)
- 바람, 잘, 날, 없다 : 4-gram

#### 네이버 영화 리뷰 말뭉치
- 띄어쓰기 단위인 어절을 하나의 단어로 보고 빈도 계산
- "내 마음 속에 영원히 기억될 최고의 명작이다"
- 문접적, 의미적으로 결함이 없는 문장이지만 언어모델은 해당 표현을 말이 되지 않는 문장으로 취급함
![image](https://github.com/user-attachments/assets/58c620e3-00ca-4054-a1e1-64ea50e642c6)

### "내 마음 속에 영원히 기억될 최고의" 다음에 "명작이다"가 나타날 확률의 계산
조건부확률의 정의 활용 -> 최대우도추정법으로 유도 가능

![image](https://github.com/user-attachments/assets/30721352-3b58-41f1-87f9-48de58621d6e)

- 우변의 분자가 0이 되므로 전체 값은 0
- 이러한 문제를 n-gram모델을 이용하여 해결 가능함
  - 직전 n-1개 단어의 등장 확률로 전체 단어 시퀀스의 등장 확률을 근사

![image](https://github.com/user-attachments/assets/d394495f-7c45-4553-a010-a93c8ccc6b70)

#### 바이그램 모델에서 " 내 마음속 ... 최고의 명작이다"라는 단어 시퀀스가 나타날 확률은?

![image](https://github.com/user-attachments/assets/e96d9571-06aa-46dd-a10b-a0cba8fd18ec)

- 바이그램 모델의 일반화 수식

![image](https://github.com/user-attachments/assets/e4a4a723-86da-4137-a3da-09eb1da4cac1)

- n-gram 모델의 한계 예시

![image](https://github.com/user-attachments/assets/53b3ae7f-ec5c-4c3e-8bac-41898722e1e3)

### Back-Off
- n-gram 등장 빈도를 n보다 작은 범위의 단어 시쿠너스 빈도로 근사하는 방식
- n을 크게하면 할수록 등장하지 않는 케이스가 많아질 가능성이 높음

- 7-gram 모델 적용시 " 그 문장"의 등장 빈도는 0
- Back-Off방식으로 n을 4로 줄여서 7-gram빈도를 근사하면
![image](https://github.com/user-attachments/assets/aa3b3114-1fa8-4791-bbee-03152d8f01c0)

### Smoothing
- 등장 빈도 표에 모두 k만큼을 더하는 기법
- 그 문장의 등장 빈도는 k(=0+k)가 됨 -> Add-k Smoothing
- k=1인 경우, Laplace Smoothing
- Smoothing을 시행하면 높은 빈도를 가진 문자열 등장 확률을 일부 깎고, 학습 데이터에 전혀 등장하지 않는 케이스들에는 작은 값이지만 일부 확률을 부여함

![image](https://github.com/user-attachments/assets/08501fc3-ec6c-4659-ab9b-098286817296)

### 뉴럴 네트워크 기반 언어 모델
- 뉴럴 네트워크
  - 입력과 출력 사이의 관계를 유연하게 포착 가능
  - 그 자체로 확률모델로 가능 -> 통계 기반 언어 모델을 대신할 수 있다
- 주어진 단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습을 수행
- 학습이 완료되면 모델의 중간 or 말단 계산 결과물을 단어나 문장의 임베딩으로 활용
- 단어를 순차적으로 입력 받아 다음 단어를 맞춰야 함 -> 일방향

### 마스크 언어 모델
- 기본 뉴럴 네트워크 기반 언어 모델과 큰 틀에서 유사하지만 디테일에서 차이를 보임
- 문장의 중간에 마스크를 씌우고 해당 마스크 위치에 어떤 단어가 올지 예측하는 과정에서 학습 수행
- 문장 전체를 다 보고 중간에 있는 단어를 예측하므로 양방향 학습이 가능함
- 이런 이유로 마스크 언어 모델 기반의 방법이 기존 언어 모델 기법과 비교하여 임베딩 품질 뛰어남
