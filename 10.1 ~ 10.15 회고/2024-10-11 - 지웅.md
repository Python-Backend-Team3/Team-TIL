# 📝 2024.10.11 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 언어모델이란?
- 문장의 확률을 나타내는 모델
- 문장은 단어들로 이루어진 시퀀셜 데이터이다.
- 언어모델이란 단어 시퀀스에 확률을 부여하는 모델로, 단어 시퀀스를 입력받아서 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 것
- 한국어 말뭉치로 학습된 언어모델은 자연스러운 한국어 문장에 높은 확률값을 부여한다(한국어 모델이 필요한 이유)

- 자연스러운 표현 = 가장 많이 사용된 표현 = 카운트 값이 가장 높은것 = 확률이 가장 높은것 (확률을 부여하는 것이 언어모델로서의 기능이 높다)
- 문장을 구성하는 단어의 조합은 매우 다양하지만 우리가 자연스럽게 받아들이는 조합은 동등한 확률보다는 평소에 자주 사용되는 단어나 표현의 조합이 훨씬 높은 확률로 발생한다.

### 인간과 같은 능력의 언어 모델을 만들기 위해
- 다양한 경로로 문장을 수집하고, 단어와 단어 사이 출현 빈도를 세어 확률을 계산, 특정분야 문장 분포 파악을 위해 전문 분야에 대한 코퍼스 수집
- 일상 생활에서 사용하는 실제 언어(문장)의 분포를 정확하게 근사하는 것이 목표이다.

### 언어 모델의 기술적 분류
- 확률에 기초한 통계적 언어 모델
- 신경망에 기초한 딥러닝 언어모델
  - 두 모델 기본적으로
  - 주어진 단어를 바탕으로 다음 단어, 혹은 단어들의 조합을 예측하며
  - 이를 바탕으로 문장 생성, 기계 번역, 음성 인식, 문서 요약 등과 같은 다양한 자연어 처리 문제를 해결한다
 
### 통계적 언어 모델
- 단어열이 가지는 확률 분포를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델
- 실제로 많이 사용하는 단어열(문장)의 분포를 정확하게 근사하는 것
- 확률 기반으로 단어의 조합을 예측하는 것
  - 주어진 단어를 통해 다음 단어로 돌 확률이 가장 높은 단어를 예측하는 일련의 과정
  - 조건부 확률을 언어 현상에 적용해 보는 것에서 출발

### 문장의 확률 표현
- 문장에서 𝒊 번째로 등장하는 단어를 𝒘𝒊 로 표시한다면
- 𝒏개의 단어로 구성된 문장이 해당 언어에서 등장할 확률(=언어모델의 출력)

- 결과가 되는 사건을 앞에 조건이 되는 사건을 뒤에 표기함

![image](https://github.com/user-attachments/assets/40baa83e-5891-4cfa-b286-85ccd056325b)


- 결합 확률과 조건부 확률
  - 3개 단어가 동시에 등장할 결합 확률

![image](https://github.com/user-attachments/assets/28baa61b-4fbd-4337-bc17-2dd8537d94cb)

- 조건부 확률로 다시 쓰면

![image](https://github.com/user-attachments/assets/ad2ac468-2c81-44cf-aad5-b667f6523e8e)

- 임의의 단어 시퀀스가 해당 언어에서 얼마나 자연스러운지를 이해하고 있는 언어 모델 구축을 하고자하는데
- 조건부 확률 정의에 따라 수식의 좌변 우변이 같음

![image](https://github.com/user-attachments/assets/d8ebb24b-fda1-4aad-a0c6-f6fd19f8a7b6)

- 이전 단어(컨텍스트)들이 주어졌을 때 다음 단어를 맞추는 문제로도 목표를 달성할 수 있다.

### 언어 모델의 형태적 분류
- 순방향 언어 모델(Forward Language Model)
  - 문장의 앞에서 뒤로, 사람이 이해하는 순서대로 계산하는 모델
    - 어제 카페 갔었어 거기 사람 많더라: 어제→카페→갔었어→거기→사람→많더라  
    - GPT, ELMo등

- 역방향 언어 모델(Backward Language Model)
  - 문장의 뒤부터 앞으로 계산하는 모델
    - 어제 카페 갔었어 거기 사람 많더라: 많더라→사람→거기→갔었어→카페→어제
  - ELMo등(ELMo는 순방향, 역방향을 붙여서 만든 모델)

#### 넓은 의미의 언어 모델
- 전통적 의미의 언어 모델은 조건부확률의 정의를 따르는 수식 표현

   ![image](https://github.com/user-attachments/assets/33cbbace-0e02-46c1-be25-32819aafd0fb)

- 컨텍스트(주변 맥락 정보)가 전제된 상태에서 특정 단어(𝒘)가 나타날 조건부 확률로 표현함


#### 잘 학습된 언어 모델
- 어떤 문장이 자연스러운지 가려낼 수 있음으로 그 자체가 가치가 있음
- 학습 대상 언어의 풍부한 맥락을 표현하고 있음(기계번역, 문법 교정, 문장 생성 등 다양한 테스크 수행 가능)

## 언어모델
- 언어를 이루는 구성 요소(글자, 형태소, 단어, 단어열(문장), 문단 등)에 확률 값을 부여하여
- 이를 바탕으로 다음 구성요소를 예측하거나 생성하는 모델

#### 언어 모델의 변화 추세
- 전통 언어처리 연구에서 딥러닝 패러다임으로 전환
- 다양한 모델 조합(단어의 문자를 인식하는 CNN 모델 + 시퀀스 처리를 위한 LSTM(RNN) + MLP를 이용한 LSTM의 출력 분류)
- 시퀀스를 위한 합성곱 연산(학습속도 향상)
- 어텐션 모델(주의모델)의 주도
- 전이학습

## Sequence To Sequence 모델
#### 번역
- 어떤 언어(seq)로 된 글을 다른 언어(seq)의 글로 옮기는 것
- 번역의 궁극적 목표(사람의 목표가 아닌 컴퓨터의 목표)

![image](https://github.com/user-attachments/assets/27c97314-39e2-48a1-b96d-387ad73f7cda)

#### 번역이 어려운 이유
- 인간의 언어(자연어)는 컴퓨터 프로그래밍 언어처럼 명확하지 않다(모호성)
- 자연어는 그 활용에 있어 효율을 극대화 하는 쪽으로 흘러간다(정보나 단어를 생략하고, 문장을 짧게 만들며, 동일한 단어와 어절을 상황에 따라서 다른 의미로 사용)
- 언어는 문화를 내포하고 있으므로 수천 년간 쌓여온 사람의 의식, 철학 등이 녹아 들어가 있어서 그러한 문 화의 차이가 번역을 더욱 어렵게 만든다

#### 기계 번역의 역사
- 규칙 기반 기계 번역
- 통계 기반 기계 번역
- 딥러닝 이전의 신경망 기계 번역
- 딥러닝 이후의 신경망 기계 번역

#### 규칙 기반 기계 번역(Rule-Based Machine Translation, RBMT)
- 가장 전통적 번역 방식
- 주어진 문장의 구조를 분석, 그 분석에 맞춰 규칙을 세운 후 분류를 나눠 정해진 규칙에 따라 번역
- 사람의 경우 일반화 능력이 뛰어나므로 몇 가지 규칙으로 번역을 수행할 수 있지만 컴퓨터는 이게 매우 어려움
- 규칙이 잘 만들어지면 통계 기반 기계 번역보다 자연스러운 표현이 가능
- 규칙을 사람이 일일이 만들어야 함 → 자원과 시간 등 소요 비용 높음

#### 통계 기반 기계 번역(Statistical Machine Translation, SMT)
- 신경망 기계 번역 이전에 세상을 지배하던 번역 방식
- 대량의 양방향 코퍼스에서 통계를 얻어내어 번역 시스템 구성
- 많은 모델로 구성되므로 매우 복잡함, 통계 기반 방식이므로 언어쌍의 확장 시, 대부분의 알고리즘, 시스템이 유지되므로 기존의 규칙 기반 기계 번역에 비해 비용적으로 유리했음

#### 딥러닝 이전의 신경망 기계 번역(Neural Machine Translation, NMT)
- 현재의 언어모델에 사용된 인코더-디코더(Encoder-Decoder) 형태를 가지고 있었으나 컴퓨터의 성능
부족, 데이터의 부족 등의 이유로 제대로 된 성능을 발휘하지 못함(지금은 핵심 모델, Transformer 모델, 생성모델)

![image](https://github.com/user-attachments/assets/8d397cbf-affc-4483-986f-81d3ea40b8d7)

#### 딥러닝 이후의 신경망 기계 번역

- 기존 통계 기반 기계 번역 방식을 순식간에 앞질러버림
- 구글 번역기를 포함하여 대부분의 상용 번역기가 딥러닝 기술로 대체됨
- 신경망 기반 기계번역의 장점

![image](https://github.com/user-attachments/assets/bb7895cd-8b51-4b4d-8ceb-d4e13563d23b)


## Seq2Seq (Sequence to sequence) 모델
- 한 도메인(예, 영어 문장)에서 다른 도메인(예, 한국어 문장)으로 시퀀스(Sequence)를 변환하는 모델
- 시퀀스를 다루는 모델이므로 당연히 순차데이터를 대상으로 함. 주로 시계열 데이터에 대하여 많이 활용됨
- 대표적인 시퀀스 모델인 RNN 모델을 기반으로 하고 있음(초기에는 RNN쓰다 LSTM으로 변경)
- 2개의 RNN 모델을 이용하여 인코더와 디코더를 구현하였고, 이 때문에 Encoder-Decoder 모델이라고 도 부름
- 시퀀스에 대한 예측을 목표로함(자연어 모델링: 각 타임 스텝에서 주어진 시퀀스를 기반으로 다음 단어 예측
  ,품사 태깅: 단어의 문법 품사 예측 ,개체명 인식: 단어가 사람, 위치, 제품, 회사 같은 개체명에 속하는지 예측 에 활용)

#### Seq2Seq 모델의 목적은
- 모델 구조를 이용하여 MLE를 수행, 주어진 데이터를 가장 잘 설명하는 파라미터 𝜽를 찾는 것
  - MLE(Maximum Likelihood Estimation, 최대 우도법)
  - 주어진 데이터를 토대로 확률 변수의 모수를 구하는 방법
  - 모수가 주어졌을 때, 원하는 값들이 나올 가능도 함수를 최대로 만드는 모수를 선택하는 방법

![image](https://github.com/user-attachments/assets/a05bdf1e-ce2b-44d3-9eac-8d198434f355)


- seq2seq 모델의 목적을 수식으로 나타내면
![image](https://github.com/user-attachments/assets/624f426c-b4d8-4daf-a7ca-1bb43d9f4013)

- 이 계산을 위해 seq2seq는 크게 3개의 서브모듈(인코더, 디코더, 생성자)로 구성됨

![image](https://github.com/user-attachments/assets/7dc415d3-988e-45cb-b030-ccf60e06cf80)

## Seq2Seq 모델: 인코더
- 주어진 소스 문장인 여러 개의 벡터를 입력으로 받아 문장을 함축하는 문장 임베딩 벡터 생성 (𝑷(𝒛|𝑿)의 모 델링)
- RNN 분류 모델과 거의 같음
- 𝑷(𝒛|𝑿)를 모델링하고, 주어진 문장을 매니폴드를 따라 차원 축소하여 해당 도메인의 잠재 공간(Latent Space)에 있는 어떤 하나의 점에 투영하는 작업

- 인코더는 문장을 하나의 벡터로 압축하여 표현함
![image](https://github.com/user-attachments/assets/e85e3d99-5d92-4af7-a62a-d75a1ebb4b75)

- 기존의 텍스트 분류 문제에서는 모든 정보(특징, Feature)가 필요하지는 않음
- → 벡터 변환 시, 많은 정보를 간직할 필요는 없음
- 예: 나는.. 과 같은 중립적인 단어는 감성 분류에 불필요
- 그러나, 기계 번역을 위한 문장 임베딩 벡터에서는 최대한 많은 정보를 요구

- 인코더를 수식으로 나타내면
![image](https://github.com/user-attachments/assets/cd3ac815-3cf1-4cc0-bd50-63289821b657)

- 인코더를 실제 구현할 때는 전체 time-step을 병렬로 한 번에 처리
![image](https://github.com/user-attachments/assets/0007d723-e479-4dd9-a4e6-ff941afaa34e)

## Seq2Seq 모델: 디코더

- 인코더와 반대의 역할을 수행
- 앞에서 살펴 본 seq2seq의 수식을 time-step에 관해 풀어서 나타내면

![image](https://github.com/user-attachments/assets/677fddfd-5c54-4529-8935-616b66836e0d)

- 조건부 확률 변수에 𝑿 가 추가됨

- seq2seq 식에서 조건부 확률에 𝑿가 추가된 것은
- 인코더의 결과인 문장 임베딩 벡터와 이전 time-step까지 번역하여 생성한 단어들에 기반하여 현재 time-step의 단어를 생성함을 의미
- 수식으로 표현하면
![image](https://github.com/user-attachments/assets/94c9ec17-a905-410c-abe8-13f26e25a295)
- 디코더 자체만으로도 신경망 언어모델에 속함 → 디코더 입력의 초깃값으로 𝑦0 에 BOS 토큰을 입력으로 줌

### 생성자(Generator)
![image](https://github.com/user-attachments/assets/99961e56-2de1-4d77-8e2b-a45a5f0fb3e5)

- Encoder와 Decoder가 번역을 수행하는 예

![image](https://github.com/user-attachments/assets/a8fcc752-b556-423d-b0b7-903f2e3ba23b)

- 인코딩(부호화) : 정보를 어떤 규칙에 따라 변환하는 것
- 디코딩(복호화) : 인코딩된 정보를 원래의 정보로 되돌리는 것

#### Encoder를 구성하는 계층

![image](https://github.com/user-attachments/assets/17a12aea-4eb0-40c3-b0ee-1ba8ea254cf0)

- Encoder는 문장을 고정 길이 벡터로 인코딩한다.

![image](https://github.com/user-attachments/assets/66eefbc3-a15d-4783-aee8-317513254e63)

- Decoder를 구성하는 계층

![image](https://github.com/user-attachments/assets/3d99127a-9db6-4b02-9a86-e21c8ec84cd1)


#### seq2seq의 활용 분야

- 특정 도메인의 시계열 데이터 또는 시퀀스 데이터 입력을
- 다른 도메인의 시계열 또는 시퀀스 데이터로 출력하는데 탁월한 능력을 보임

![image](https://github.com/user-attachments/assets/8bbcf09e-f235-46bd-8ff4-2a5d5fb690fa)

#### Seq2Seq 모델:한계점
- seq2seq는 오토인코더(AutoEncoder)의 일종으로 특히 시계열 또는 시퀀스 데이터에 강점이 있는 모 델이라고 볼 수 있음
- 장기 기억력 문제 
  - 신경망 모델은 차원축소를 통한 데이터 압축에 탁월한 성능을 보이지만
  - 정보를 무한하게 압축할 수는 없음 → 압축 가능한 정보량의 한계 → 문장(또는 time-step)이 길어질수록 압축 성능 하락
  - LSTM, GRU 등을 사용하여 RNN보다는 높은 성능을 낼 수 있지만 역시 한계가 존재함
- 구조 정보의 부재
  - 현재 딥러닝 자연어 추세에
  - 문장을 이해할 때 구조 정보를 사용하기 보다 단순히 시퀀스 데이터로 다루는 경향
  - 아직까지는 성공적으로 사용되지만 다음 단계로 나아가려면 구조 정보가 필요할 것으로 추정
- 챗봇 또는 QA봇
  - 시퀀스 데이터 입력을 다른 도메인의 시계열/시퀀스 데이터로 출력하는데 뛰어남
  - 많은 사람이 seq2seq를 학습 시키면 더욱 뛰어난 성능을 보일 것으로 기대
  - 그러나 지속적인 데이터, 정보 추가가 필요하지 않은 번역, 요약과 달리 대화의 경우 지속적인 관리가 요구되지만 해당하는 구조적 기능이 부족함 → 발전된 구조 요구


## Attention 언어 모델
- 딥러닝 초창기의 기계번역 기술의 주요 방식은 Sequence 방식
- 데이터를 토큰으로 나눠 순차적으로 입력 데이터를 준 다음 순차적으로 출력을 뽑아내는 방식(seq2seq) : Encoder-Decoder 방식

- 인코더에서 입력 시퀀스를 컨텍스트 벡터라는 하나의 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 컨텍스트 벡터를 통해 출력 시퀀스를 만들어 내는 모델(문장이 길어지면:30~40단어 이상 성능이 떨어짐)

##### 문제점 정리
- 하나의 고정된 크기의 벡터로 모든 정볼르 압축하다보니 정보 손실이 발생
- 문제 해결을 위해 Attention모델 기법을 제안함

### Attention 기법의 아이디어
- 사람은 번역시 모든 문장을 듣지만 각 단어를 번역할 때마다 모든 문장의 정보를 이용하지 않는다.
- Encoder-Decoder 방식에서는
  - 다음 단어를 번역할 때마다 C 라는 문맥 벡터를 전달하는데, 
  - 이 고정된 길이의 벡터에 그 동안 본 모든 단어에 대한 정보가 축약되어 있음 → 문장이 길어지면 효율 저하 → 이게 문제가됨
  - 이에 고정된 길이의 C 벡터에 모든 정보를 축약하지 말고 매 Step마다 필요한 정보를 새로 만들자!  해결/개선안 제시 → Attention 모델

- 디코더에서 출력 단어를 예측하는 매 시점마다 인코더에서의 전체 입력 문장을 다시 한 번 참고하자는 것
- 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라 해당 시점에서 예측해야 할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중해서 참고하는것

#### RNN 모델에 대한 Attention 기법 적용

- 기존의 encode는 word-for-word translation, 즉 단어 하나를 번역하면 그 단어를 넘어가는 방식

![image](https://github.com/user-attachments/assets/f99b95c6-893b-4641-a8e6-ceee9cf55905)

- 번역하고자 하는 내용과 문제점

![image](https://github.com/user-attachments/assets/6f048661-18d1-4cba-8436-b17842372015)

- RNN 모델에서의 번역(예시: 프랑스어를 영어로 변환시)

![image](https://github.com/user-attachments/assets/62925aa6-d561-40d2-a6bf-d95a12533da8)

![image](https://github.com/user-attachments/assets/cd348676-6782-492f-b054-4a9976b3715d)

![image](https://github.com/user-attachments/assets/9029ba8a-796d-470d-8fdf-a22ccd436e78)

![image](https://github.com/user-attachments/assets/4fd60492-9fff-4ff1-acc9-3efc7d332cb5)

![image](https://github.com/user-attachments/assets/01906809-1f68-400c-aa06-a4998aea4ddb)

![image](https://github.com/user-attachments/assets/6433b855-7b84-44ef-a3a4-e831bb00d348)

![image](https://github.com/user-attachments/assets/b5af41ab-2926-4d8f-b553-ab3a46401dc8)


### Attention은
- Query와 비슷한 값을 가진 키(Key)를 찾아서 그 값(Value)을 얻는 과정이다.
  - → 일반적인 Key-Value 방식과 비교해 볼 수 있다.
#### Key-Value 자료형
- 컴퓨터 공학의 많은 분야에서 사용되고 있는 자료형
- 파이썬의 경우 Dictionary 자료형을 예로 들 수 있음

- 키(Key)와 값(Value)이라는 두 개의 쌍으로 구성되며, 키를 통해서 맵핑된 값을 찾아낼 수 있다는 특징을 가짐
![image](https://github.com/user-attachments/assets/0472a42f-b00b-49ae-b0f9-5acfae3b9db6)

- Key-Value 자료형을 기준으로 Attention 함수를 살펴보면
![image](https://github.com/user-attachments/assets/67a61fd8-2f3c-4b5f-8b90-8fa6b0226278)


- Dot-Product Attention은
  - 다양한 어텐션 종류 중에서 가장 수식적으로 이해하기 쉽게 적용된 모델
  - seq2seq에서 사용되는 어텐션 중에서 Dot-Product Attention과 다른 어텐션들은 주로 중간 수식만
  다를 뿐 메커니즘은 거의 유사함

![image](https://github.com/user-attachments/assets/6c3058dc-c49e-4647-9c00-261f7b9dfcf8)
-<그림은 디코더의 세번째 LSTM에서 출력 단어를 예측할 때, 어텐션 메커니즘을 사용하는 모습을 보여줌>

## Dot-Product Attention

- 처리 과정
  - 1. 어텐션 스코어(Attention Score)를 구한다.

![image](https://github.com/user-attachments/assets/927675a8-327f-499e-9544-24c41d844784)

  - 2. 소프트맥스(softmax) 함수를 통해 어텐션 분포(Attention Distribution) 계산

![image](https://github.com/user-attachments/assets/38a71419-db9b-480e-b4ae-e0f9d29a116c)

  - 3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값 계산
지금까

![image](https://github.com/user-attachments/assets/cd0c654a-20e2-446c-80b8-5cf04368b788)

  - 4. 어텐션 값과 디코더의 t 시점의 은닉 상태를 연결(Concatenate)

![image](https://github.com/user-attachments/assets/37797a62-9c29-4b31-907a-8bc1b4f83d17)

![image](https://github.com/user-attachments/assets/913e13eb-d932-4b68-9a64-95048fbe599d)

- 다양한 어텐션의 차이는 중간 수식(어텐션 스코어 함수)

![image](https://github.com/user-attachments/assets/dc46b115-0e7f-4c3c-b517-7d1b3573b6ac)

- 어텐션은 RNN 기반의 seq2seq 성능 보정을 목적으로 제안되었지만
- 현재에 이르러서는 어텐션 스스로가 기존의 seq2seq를 대체하는 방법이 되고 있음 → Transformer

- https://arxiv.org/pdf/1409.0473.pdf (Attention 소개 논문)

### 바나다우 어텐션(Bahdanau Attention)

![image](https://github.com/user-attachments/assets/e55b4fcd-9696-4c92-9e21-fe37533e55cc)

- 어텐션 함수의 Query가 디코더 셀의 t 시점의 은닉 상태가 아니라 t-1 시점의 은닉 상태임

##### 처리 과정
- 1. 어텐션 스코어(Attention Score) 계산

![image](https://github.com/user-attachments/assets/4cb044d4-9f8b-45da-88ea-bc136ae740dc)

![image](https://github.com/user-attachments/assets/02f05e2f-e6b6-4270-b632-5a2d6c35f939)

![image](https://github.com/user-attachments/assets/d8bce117-9e52-4c4f-979f-706e4920f51f)

- 2. 소프트맥스(softmax) 함수를 통해 어텐션 분포 계산

![image](https://github.com/user-attachments/assets/76d3def7-18a3-4ef5-8ef5-e114e132d8b3)

- 3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값 계산

![image](https://github.com/user-attachments/assets/a2162409-4eb4-4da7-becd-ecae8e517e09)

- 4. 컨텍스트 벡터로부터 𝒔t 계산

![image](https://github.com/user-attachments/assets/be653a5a-48f1-473b-9cc0-b39e7aaed237)

#### 실습

## 백준
