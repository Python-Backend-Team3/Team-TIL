# 📝 2024.10.11 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 언어모델이란?
- 문장의 확률을 나타내는 모델
- 문장은 단어들로 이루어진 시퀀셜 데이터이다.
- 언어모델이란 단어 시퀀스에 확률을 부여하는 모델로, 단어 시퀀스를 입력받아서 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 것
- 한국어 말뭉치로 학습된 언어모델은 자연스러운 한국어 문장에 높은 확률값을 부여한다(한국어 모델이 필요한 이유)

- 자연스러운 표현 = 가장 많이 사용된 표현 = 카운트 값이 가장 높은것 = 확률이 가장 높은것 (확률을 부여하는 것이 언어모델로서의 기능이 높다)
- 문장을 구성하는 단어의 조합은 매우 다양하지만 우리가 자연스럽게 받아들이는 조합은 동등한 확률보다는 평소에 자주 사용되는 단어나 표현의 조합이 훨씬 높은 확률로 발생한다.

### 인간과 같은 능력의 언어 모델을 만들기 위해
- 다양한 경로로 문장을 수집하고, 단어와 단어 사이 출현 빈도를 세어 확률을 계산, 특정분야 문장 분포 파악을 위해 전문 분야에 대한 코퍼스 수집
- 일상 생활에서 사용하는 실제 언어(문장)의 분포를 정확하게 근사하는 것이 목표이다.

### 언어 모델의 기술적 분류
- 확률에 기초한 통계적 언어 모델
- 신경망에 기초한 딥러닝 언어모델
  - 두 모델 기본적으로
  - 주어진 단어를 바탕으로 다음 단어, 혹은 단어들의 조합을 예측하며
  - 이를 바탕으로 문장 생성, 기계 번역, 음성 인식, 문서 요약 등과 같은 다양한 자연어 처리 문제를 해결한다
 
### 통계적 언어 모델
- 단어열이 가지는 확률 분포를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델
- 실제로 많이 사용하는 단어열(문장)의 분포를 정확하게 근사하는 것
- 확률 기반으로 단어의 조합을 예측하는 것
  - 주어진 단어를 통해 다음 단어로 돌 확률이 가장 높은 단어를 예측하는 일련의 과정
  - 조건부 확률을 언어 현상에 적용해 보는 것에서 출발

### 문장의 확률 표현
- 문장에서 𝒊 번째로 등장하는 단어를 𝒘𝒊 로 표시한다면
- 𝒏개의 단어로 구성된 문장이 해당 언어에서 등장할 확률(=언어모델의 출력)

- 결과가 되는 사건을 앞에 조건이 되는 사건을 뒤에 표기함

![image](https://github.com/user-attachments/assets/40baa83e-5891-4cfa-b286-85ccd056325b)


- 결합 확률과 조건부 확률
  - 3개 단어가 동시에 등장할 결합 확률

![image](https://github.com/user-attachments/assets/28baa61b-4fbd-4337-bc17-2dd8537d94cb)

- 조건부 확률로 다시 쓰면

![image](https://github.com/user-attachments/assets/ad2ac468-2c81-44cf-aad5-b667f6523e8e)

- 임의의 단어 시퀀스가 해당 언어에서 얼마나 자연스러운지를 이해하고 있는 언어 모델 구축을 하고자하는데
- 조건부 확률 정의에 따라 수식의 좌변 우변이 같음

![image](https://github.com/user-attachments/assets/d8ebb24b-fda1-4aad-a0c6-f6fd19f8a7b6)

- 이전 단어(컨텍스트)들이 주어졌을 때 다음 단어를 맞추는 문제로도 목표를 달성할 수 있다.

### 언어 모델의 형태적 분류
- 순방향 언어 모델(Forward Language Model)
  - 문장의 앞에서 뒤로, 사람이 이해하는 순서대로 계산하는 모델
    - 어제 카페 갔었어 거기 사람 많더라: 어제→카페→갔었어→거기→사람→많더라  
    - GPT, ELMo등

- 역방향 언어 모델(Backward Language Model)
  - 문장의 뒤부터 앞으로 계산하는 모델
    - 어제 카페 갔었어 거기 사람 많더라: 많더라→사람→거기→갔었어→카페→어제
  - ELMo등(ELMo는 순방향, 역방향을 붙여서 만든 모델)

#### 넓은 의미의 언어 모델
- 전통적 의미의 언어 모델은 조건부확률의 정의를 따르는 수식 표현

   ![image](https://github.com/user-attachments/assets/33cbbace-0e02-46c1-be25-32819aafd0fb)

- 컨텍스트(주변 맥락 정보)가 전제된 상태에서 특정 단어(𝒘)가 나타날 조건부 확률로 표현함


#### 잘 학습된 언어 모델
- 어떤 문장이 자연스러운지 가려낼 수 있음으로 그 자체가 가치가 있음
- 학습 대상 언어의 풍부한 맥락을 표현하고 있음(기계번역, 문법 교정, 문장 생성 등 다양한 테스크 수행 가능)

## 언어모델
- 언어를 이루는 구성 요소(글자, 형태소, 단어, 단어열(문장), 문단 등)에 확률 값을 부여하여
- 이를 바탕으로 다음 구성요소를 예측하거나 생성하는 모델

#### 언어 모델의 변화 추세
- 전통 언어처리 연구에서 딥러닝 패러다임으로 전환
- 다양한 모델 조합(단어의 문자를 인식하는 CNN 모델 + 시퀀스 처리를 위한 LSTM(RNN) + MLP를 이용한 LSTM의 출력 분류)
- 시퀀스를 위한 합성곱 연산(학습속도 향상)
- 어텐션 모델(주의모델)의 주도
- 전이학습

## Sequence To Sequence 모델
#### 번역
- 어떤 언어(seq)로 된 글을 다른 언어(seq)의 글로 옮기는 것
- 번역의 궁극적 목표(사람의 목표가 아닌 컴퓨터의 목표)

![image](https://github.com/user-attachments/assets/27c97314-39e2-48a1-b96d-387ad73f7cda)

#### 번역이 어려운 이유
- 인간의 언어(자연어)는 컴퓨터 프로그래밍 언어처럼 명확하지 않다(모호성)
- 자연어는 그 활용에 있어 효율을 극대화 하는 쪽으로 흘러간다(정보나 단어를 생략하고, 문장을 짧게 만들며, 동일한 단어와 어절을 상황에 따라서 다른 의미로 사용)
- 언어는 문화를 내포하고 있으므로 수천 년간 쌓여온 사람의 의식, 철학 등이 녹아 들어가 있어서 그러한 문 화의 차이가 번역을 더욱 어렵게 만든다

#### 기계 번역의 역사
- 규칙 기반 기계 번역
- 통계 기반 기계 번역
- 딥러닝 이전의 신경망 기계 번역
- 딥러닝 이후의 신경망 기계 번역

#### 규칙 기반 기계 번역(Rule-Based Machine Translation, RBMT)
- 가장 전통적 번역 방식
- 주어진 문장의 구조를 분석, 그 분석에 맞춰 규칙을 세운 후 분류를 나눠 정해진 규칙에 따라 번역
- 사람의 경우 일반화 능력이 뛰어나므로 몇 가지 규칙으로 번역을 수행할 수 있지만 컴퓨터는 이게 매우 어려움
- 규칙이 잘 만들어지면 통계 기반 기계 번역보다 자연스러운 표현이 가능
- 규칙을 사람이 일일이 만들어야 함 → 자원과 시간 등 소요 비용 높음

#### 통계 기반 기계 번역(Statistical Machine Translation, SMT)
- 신경망 기계 번역 이전에 세상을 지배하던 번역 방식
- 대량의 양방향 코퍼스에서 통계를 얻어내어 번역 시스템 구성
- 많은 모델로 구성되므로 매우 복잡함, 통계 기반 방식이므로 언어쌍의 확장 시, 대부분의 알고리즘, 시스템이 유지되므로 기존의 규칙 기반 기계 번역에 비해 비용적으로 유리했음

#### 딥러닝 이전의 신경망 기계 번역(Neural Machine Translation, NMT)
- 현재의 언어모델에 사용된 인코더-디코더(Encoder-Decoder) 형태를 가지고 있었으나 컴퓨터의 성능
부족, 데이터의 부족 등의 이유로 제대로 된 성능을 발휘하지 못함(지금은 핵심 모델, Transformer 모델, 생성모델)

![image](https://github.com/user-attachments/assets/8d397cbf-affc-4483-986f-81d3ea40b8d7)

#### 딥러닝 이후의 신경망 기계 번역

- 기존 통계 기반 기계 번역 방식을 순식간에 앞질러버림
- 구글 번역기를 포함하여 대부분의 상용 번역기가 딥러닝 기술로 대체됨
- 신경망 기반 기계번역의 장점

![image](https://github.com/user-attachments/assets/bb7895cd-8b51-4b4d-8ceb-d4e13563d23b)


## Seq2Seq (Sequence to sequence) 모델
- 한 도메인(예, 영어 문장)에서 다른 도메인(예, 한국어 문장)으로 시퀀스(Sequence)를 변환하는 모델
- 시퀀스를 다루는 모델이므로 당연히 순차데이터를 대상으로 함. 주로 시계열 데이터에 대하여 많이 활용됨
- 대표적인 시퀀스 모델인 RNN 모델을 기반으로 하고 있음(초기에는 RNN쓰다 LSTM으로 변경)
- 2개의 RNN 모델을 이용하여 인코더와 디코더를 구현하였고, 이 때문에 Encoder-Decoder 모델이라고 도 부름
- 시퀀스에 대한 예측을 목표로함(자연어 모델링: 각 타임 스텝에서 주어진 시퀀스를 기반으로 다음 단어 예측
  ,품사 태깅: 단어의 문법 품사 예측 ,개체명 인식: 단어가 사람, 위치, 제품, 회사 같은 개체명에 속하는지 예측 에 활용)

#### Seq2Seq 모델의 목적은
- 모델 구조를 이용하여 MLE를 수행, 주어진 데이터를 가장 잘 설명하는 파라미터 𝜽를 찾는 것
  - MLE(Maximum Likelihood Estimation, 최대 우도법)
  - 주어진 데이터를 토대로 확률 변수의 모수를 구하는 방법
  - 모수가 주어졌을 때, 원하는 값들이 나올 가능도 함수를 최대로 만드는 모수를 선택하는 방법

![image](https://github.com/user-attachments/assets/a05bdf1e-ce2b-44d3-9eac-8d198434f355)


- seq2seq 모델의 목적을 수식으로 나타내면
![image](https://github.com/user-attachments/assets/624f426c-b4d8-4daf-a7ca-1bb43d9f4013)

- 이 계산을 위해 seq2seq는 크게 3개의 서브모듈(인코더, 디코더, 생성자)로 구성됨

![image](https://github.com/user-attachments/assets/7dc415d3-988e-45cb-b030-ccf60e06cf80)

## Seq2Seq 모델: 인코더
- 주어진 소스 문장인 여러 개의 벡터를 입력으로 받아 문장을 함축하는 문장 임베딩 벡터 생성 (𝑷(𝒛|𝑿)의 모 델링)
- RNN 분류 모델과 거의 같음
- 𝑷(𝒛|𝑿)를 모델링하고, 주어진 문장을 매니폴드를 따라 차원 축소하여 해당 도메인의 잠재 공간(Latent Space)에 있는 어떤 하나의 점에 투영하는 작업

- 인코더는 문장을 하나의 벡터로 압축하여 표현함
![image](https://github.com/user-attachments/assets/e85e3d99-5d92-4af7-a62a-d75a1ebb4b75)

- 기존의 텍스트 분류 문제에서는 모든 정보(특징, Feature)가 필요하지는 않음
- → 벡터 변환 시, 많은 정보를 간직할 필요는 없음
- 예: 나는.. 과 같은 중립적인 단어는 감성 분류에 불필요
- 그러나, 기계 번역을 위한 문장 임베딩 벡터에서는 최대한 많은 정보를 요구

- 인코더를 수식으로 나타내면
![image](https://github.com/user-attachments/assets/cd3ac815-3cf1-4cc0-bd50-63289821b657)

- 인코더를 실제 구현할 때는 전체 time-step을 병렬로 한 번에 처리
![image](https://github.com/user-attachments/assets/0007d723-e479-4dd9-a4e6-ff941afaa34e)

## Seq2Seq 모델: 디코더

- 인코더와 반대의 역할을 수행
- 앞에서 살펴 본 seq2seq의 수식을 time-step에 관해 풀어서 나타내면

![image](https://github.com/user-attachments/assets/677fddfd-5c54-4529-8935-616b66836e0d)

- 조건부 확률 변수에 𝑿 가 추가됨

- seq2seq 식에서 조건부 확률에 𝑿가 추가된 것은
- 인코더의 결과인 문장 임베딩 벡터와 이전 time-step까지 번역하여 생성한 단어들에 기반하여 현재 time-step의 단어를 생성함을 의미
- 수식으로 표현하면
![image](https://github.com/user-attachments/assets/94c9ec17-a905-410c-abe8-13f26e25a295)
- 디코더 자체만으로도 신경망 언어모델에 속함 → 디코더 입력의 초깃값으로 𝑦0 에 BOS 토큰을 입력으로 줌
















































































