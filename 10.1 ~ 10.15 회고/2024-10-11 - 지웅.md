# 📝 2024.10.11 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

## 언어모델이란?
- 문장의 확률을 나타내는 모델
- 문장은 단어들로 이루어진 시퀀셜 데이터이다.
- 언어모델이란 단어 시퀀스에 확률을 부여하는 모델로, 단어 시퀀스를 입력받아서 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력하는 것
- 한국어 말뭉치로 학습된 언어모델은 자연스러운 한국어 문장에 높은 확률값을 부여한다(한국어 모델이 필요한 이유)

- 자연스러운 표현 = 가장 많이 사용된 표현 = 카운트 값이 가장 높은것 = 확률이 가장 높은것 (확률을 부여하는 것이 언어모델로서의 기능이 높다)
- 문장을 구성하는 단어의 조합은 매우 다양하지만 우리가 자연스럽게 받아들이는 조합은 동등한 확률보다는 평소에 자주 사용되는 단어나 표현의 조합이 훨씬 높은 확률로 발생한다.

### 인간과 같은 능력의 언어 모델을 만들기 위해
- 다양한 경로로 문장을 수집하고, 단어와 단어 사이 출현 빈도를 세어 확률을 계산, 특정분야 문장 분포 파악을 위해 전문 분야에 대한 코퍼스 수집
- 일상 생활에서 사용하는 실제 언어(문장)의 분포를 정확하게 근사하는 것이 목표이다.

### 언어 모델의 기술적 분류
- 확률에 기초한 통계적 언어 모델
- 신경망에 기초한 딥러닝 언어모델
  - 두 모델 기본적으로
  - 주어진 단어를 바탕으로 다음 단어, 혹은 단어들의 조합을 예측하며
  - 이를 바탕으로 문장 생성, 기계 번역, 음성 인식, 문서 요약 등과 같은 다양한 자연어 처리 문제를 해결한다
 
### 통계적 언어 모델
- 단어열이 가지는 확률 분포를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델
- 실제로 많이 사용하는 단어열(문장)의 분포를 정확하게 근사하는 것
- 확률 기반으로 단어의 조합을 예측하는 것
  - 주어진 단어를 통해 다음 단어로 돌 확률이 가장 높은 단어를 예측하는 일련의 과정
  - 조건부 확률을 언어 현상에 적용해 보는 것에서 출발

### 문장의 확률 표현
- 문장에서 𝒊 번째로 등장하는 단어를 𝒘𝒊 로 표시한다면
- 𝒏개의 단어로 구성된 문장이 해당 언어에서 등장할 확률(=언어모델의 출력)

- 결과가 되는 사건을 앞에 조건이 되는 사건을 뒤에 표기함

![image](https://github.com/user-attachments/assets/40baa83e-5891-4cfa-b286-85ccd056325b)


- 결합 확률과 조건부 확률
  - 3개 단어가 동시에 등장할 결합 확률

![image](https://github.com/user-attachments/assets/28baa61b-4fbd-4337-bc17-2dd8537d94cb)

- 조건부 확률로 다시 쓰면

![image](https://github.com/user-attachments/assets/ad2ac468-2c81-44cf-aad5-b667f6523e8e)

- 임의의 단어 시퀀스가 해당 언어에서 얼마나 자연스러운지를 이해하고 있는 언어 모델 구축을 하고자하는데
- 조건부 확률 정의에 따라 수식의 좌변 우변이 같음

![image](https://github.com/user-attachments/assets/d8ebb24b-fda1-4aad-a0c6-f6fd19f8a7b6)

- 이전 단어(컨텍스트)들이 주어졌을 때 다음 단어를 맞추는 문제로도 목표를 달성할 수 있다.

### 언어 모델의 형태적 분류
- 순방향 언어 모델(Forward Language Model)
  - 문장의 앞에서 뒤로, 사람이 이해하는 순서대로 계산하는 모델
    - 어제 카페 갔었어 거기 사람 많더라: 어제→카페→갔었어→거기→사람→많더라  
    - GPT, ELMo등

- 역방향 언어 모델(Backward Language Model)
  - 문장의 뒤부터 앞으로 계산하는 모델
    - 어제 카페 갔었어 거기 사람 많더라: 많더라→사람→거기→갔었어→카페→어제
  - ELMo등(ELMo는 순방향, 역방향을 붙여서 만든 모델)

#### 넓은 의미의 언어 모델
- 전통적 의미의 언어 모델은 조건부확률의 정의를 따르는 수식 표현

   ![image](https://github.com/user-attachments/assets/33cbbace-0e02-46c1-be25-32819aafd0fb)

- 컨텍스트(주변 맥락 정보)가 전제된 상태에서 특정 단어(𝒘)가 나타날 조건부 확률로 표현함


#### 잘 학습된 언어 모델
- 어떤 문장이 자연스러운지 가려낼 수 있음으로 그 자체가 가치가 있음
- 학습 대상 언어의 풍부한 맥락을 표현하고 있음(기계번역, 문법 교정, 문장 생성 등 다양한 테스크 수행 가능)

## 언어모델
- 언어를 이루는 구성 요소(글자, 형태소, 단어, 단어열(문장), 문단 등)에 확률 값을 부여하여
- 이를 바탕으로 다음 구성요소를 예측하거나 생성하는 모델

#### 언어 모델의 변화 추세
- 전통 언어처리 연구에서 딥러닝 패러다임으로 전환
- 다양한 모델 조합(단어의 문자를 인식하는 CNN 모델 + 시퀀스 처리를 위한 LSTM(RNN) + MLP를 이용한 LSTM의 출력 분류)
- 시퀀스를 위한 합성곱 연산(학습속도 향상)
- 어텐션 모델(주의모델)의 주도
- 전이학습

## Sequence To Sequence 모델
#### 번역
- 어떤 언어(seq)로 된 글을 다른 언어(seq)의 글로 옮기는 것
- 번역의 궁극적 목표(사람의 목표가 아닌 컴퓨터의 목표)

![image](https://github.com/user-attachments/assets/27c97314-39e2-48a1-b96d-387ad73f7cda)

#### 번역이 어려운 이유
- 인간의 언어(자연어)는 컴퓨터 프로그래밍 언어처럼 명확하지 않다(모호성)
- 자연어는 그 활용에 있어 효율을 극대화 하는 쪽으로 흘러간다(정보나 단어를 생략하고, 문장을 짧게 만들며, 동일한 단어와 어절을 상황에 따라서 다른 의미로 사용)
- 언어는 문화를 내포하고 있으므로 수천 년간 쌓여온 사람의 의식, 철학 등이 녹아 들어가 있어서 그러한 문 화의 차이가 번역을 더욱 어렵게 만든다

#### 기계 번역의 역사
- 규칙 기반 기계 번역
- 통계 기반 기계 번역
- 딥러닝 이전의 신경망 기계 번역
- 딥러닝 이후의 신경망 기계 번역

#### 규칙 기반 기계 번역(


























































































































