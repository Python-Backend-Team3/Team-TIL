## 2024-10-01 회고
### 회고 목표: AI 내용 정리

그동안 백준 문제를 푸느라 AI 내용을 복습만 하였는데 이번에 정리를 해보려고 함!

### 인공지능
- 다양한 기술을 이용하여 **사람이 하는 일을 흉내 내어 처리**하는 시스템
- 기호 주의 : 규칙기반 전문가 시스템 중심으로 발전
- 연결 주의 : 신경망 모델을 통한 학습 중심으로 발전
- 진화 주의 : 유전자 알고리즘을 주축으로 발전
- 베이즈 주의 : 베이즈 추론 기반의 통계 모델을 중심으로 발전
- 유추 주의 : 사물, 현상에 대한 유추를 기반으로 학습 진행

### 머신러닝
- 지도학습
  - 분류 :  KNN, SVM, 의사결정트리, 신경망
  - 예측 : 회귀
- 비지도학습 : 군집화, 독립성분분석
- 강화학습 : 마르코프 결정과정

### 딥러닝
- 신경망 모델 : 신경세포의 간단하고 효과적인 처리 방식에 착안해 구현된 머신 러닝 모델의 한 종류
  - 다량의 뉴런들이 층으로 연결되어 간단한 계산과 연결방식을 통해 복잡한 문제를 해결하는 모델 

#### 단층 퍼셉트론(SLP)

![image](https://github.com/user-attachments/assets/042e4d86-5a13-4579-852e-eb7764244d30)

- 다수의 퍼셉트론이 하나의 층을 이루고 있는 형태
- 센서 데이터 등 다양한 데이터를 각 퍼셉트론의 입력으로 전달
- 각 퍼셉트론은 입력된 데이터를 모아 합산
- 합산 결과가 임계 값을 넘으면 1, 넘지 않으면 0 출력
- 입력층에서 각 퍼셉트론으로 진행되는 통로에는 **가중치** 적용

#### 다층 퍼셉트론(MLP)

![image](https://github.com/user-attachments/assets/66269588-212a-4d9d-8b57-6fd37e6b69c8)

- SLP의 한계였던 XOR문제 해결 가능
- 처리방식은 동일함

그러나 가중치를 변경할 수 없다는 문제점은 여전함

다층 퍼셉트론 같은 깊은 신경망 -> 선형으로 분류가 가능하지 않은 데이터를 분류하는 것이 가능

하지만 모델의 깊이가 깊어질수록(=모델의 층이 많아질수록) 모델의 가중치의 수는 기하급수적으로 증가함

따라서 경사하강법(기울기를 이용하여 가중치를 업데이트하는 방식)을 사용하지만, 이는 많은 연산량과 메모리를 요구함

이를 해결하고자 고안된 알고리즘이 바로 **역전파 알고리즘**

여태 신경망은 모두 연산이 입력층 -> 출력층으로 이동하였음

이를 **순방향 신경망** 이라고 함.

또 연산이 앞으로 이동하는 과정을 **순전파**라고 함

### 역전파 알고리즘
- 출력층에서 입력층 방향으로 오차를 전파시켜 각 층의 가중치를 업데이트 함

![image](https://github.com/user-attachments/assets/d6443bef-6561-4f3c-bf24-849d65cba736)

- 검은색으로 칠해진 조그마한 원은 **활성화 함수**
- 처음 입력값이 모델에 주입되면 순전파를 통해 각 노드의 값들을 계산함
- 그 후 출력층의 노드에 있는 값과 타깃(y)값의 차이를 이용하여 **전체 손실(L)** 을 구함
- L은 출력증의 각 노드와 그에 대응하는 타깃 값과의 차이를 누적하여 더한 값임
- 각 손실을 L1, L2라고 하고, 선형 방정식을 통과한 값은 z, 활성화 함수를 통과한 z값을 a라고 하면

![image](https://github.com/user-attachments/assets/6ebafcc2-aa98-47c5-8c03-77d08a4c4deb)

- 출력층과 가까운 가중치부터 살펴보자(역전파 알고리즘 이므로)

![image](https://github.com/user-attachments/assets/e2e263e5-1629-44fc-9956-78c97ba6d785)

- i,j는 같은 층에 작용하는 가중치, k는 가중치의 층을 구별하기 위함

![image](https://github.com/user-attachments/assets/f30e69dc-62f8-4165-986a-3519caf81c24)

- 역전파 알고리즘은 출력층부터 시작 -> 입력층으로 **오차**를 전파
- 이 오차가 전파되면서 도달한 층의 가중치들을 업데이트 함
- L, a, z의 값들은 다음과 같음

![image](https://github.com/user-attachments/assets/09a4cd6e-bf84-4da6-a3be-23fd95c7f9a8)

- L1을 결정하는데에 관여한 가중치들은 각각 위 첨자 (1)을 달고있는 w11,w21,w31임
- 기울기를 구할 떄는 연쇄 법칙이 적용되어 다음의 기울기들을 곱함

![image](https://github.com/user-attachments/assets/8e09b14f-1184-4ac1-b08c-dfbe0802bfa2)

- 따라서 위 식에 학습률(learning rate)를 곱하여 가중치에서 뺴준다면 가중치를 업데이트 할 수 있음
- L2도 마찬가지


- 복잡해보이지만 중요한 점은 이미 계산된 값이 또 사용되고 있다는 것
- 역전파가 일어날 때 어떤 층에서 계산한 기울기(오차)가 다음 층에서도 사용된다는 뜻
- 그래서 오차가 역방향으로 전파되는 것처럼 보임(아래 사진 참고)

![image](https://github.com/user-attachments/assets/c2ca23de-de68-48bf-87e8-a7fe16e79a1f)

### 활성화 함수
- 인공신경망 내부에서 입력받은 데이터를 근거로 다음 계층으로 출력할 값을 결정하는 기능을 수행함
- 가중치(w)가 달린 입력 신호(x)와 편향(b)의 총합을 계산하고 함수f에 넣어 출력하는 흐름(아래 사진 참고)
![image](https://github.com/user-attachments/assets/f13a43ca-6f96-4eb1-8e37-4bcac27f5ea2)

- 신경망의 활성화 함수는 **비선형 함수**를 사용해야 함
  - 선형 함수 : 출력이 입력의 상수 배만큼 변하는 함수
  - 비선형 함수: 직선 하나로는 그릴 수 없는 함수
  - 선형 함수를 이용하면 신경망의 층을 깊게하는 의미가 없어짐

#### sigmoid 함수

![image](https://github.com/user-attachments/assets/403df57a-82bf-43ca-85cf-1fc4c222b0ef)

- 실수 값을 입력받아 0~1 사이의 값으로 압축
- 기울기 소멸 문제 -> 학습 능력이 제한되는 포화가 발생함
- 0이 중심이 아님 -> 학습 속도가 느려짐
- exp 연산으로 자원과 시간이 많이 소모됨
```
import numpy as np
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

#### tanh 함수(하이퍼볼릭탄젠트)

![image](https://github.com/user-attachments/assets/fb34be62-a460-459b-bc7c-d5f12423e7da)

- 실수값을 입력 받아 -1~1 사이의 값으로 압축
- 시그모이드를 두배하고 -1 한 값과 같음
- 시그모이드에 비해 최적화를 잘하지만 기울기 소실 문제를 갖고 있음

```
def tanh(x):
    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
```

#### ReLU(Rectified Linear Unit) 함수

![image](https://github.com/user-attachments/assets/cb1a61a1-937d-4341-a468-07622d11f29a)

- 입력이 0을 넘으면 그 입력 그대로 출력, 0이하면 0 출력
```
def relu(x):
    return np.maximum(0,x)
```
#### Softmax 함수
- 세 개 이상으로 분류하는 다중 클래스 분류에서 출력층에 사용되는 활성화 함수
- 입력 받은 값을 출력으로 0~1사이의 값으로 모두 정규화
- 출력값들의 총합은 항상 1이 되는 특성을 가짐


|사용되는 층|활성화 함수|용도|
|----------|-----------|-----------------------|
|은닉층|ReLU|기울기 소실 문제를 줄이고 다음 층으로 신호 전달|
|출력층|Sigmoid|이진 분류|
|      |SoftMax|다중 분류|
|      |X|회귀|

#### 과적합(Overfitting)
- 학습 데이터를 과하게 학습하는 것을 뜻함
- 학습 데이터 셋에서는 **높은 정확도**
- 테스트 데이터 셋에서는 **낮은 정확도**
- 조기 종료, 정규화, Drop Out(랜점으로 일부 노드 배제하고 학습)

#### 앙상블
- 여러 개의 model을 조합하여 하나의 최종 결과를 도출하는 방식
- 성능 향상, 과적합 방지, 안전성 및 다양성
- 금융에서 신용 점수 예측, 의학에서 질병 진단, 마케팅에서 고객 분석 등에 활용됨
- 그러나 계산이 복잡하고 해석이 어려울 수 있음
- Bagging
  - 여러 데이터 서브셋을 구성
  - 각각의 서브셋에서 독립적으로 모델을 학습시킴 -> 모든 모델의 예측을 집계
  - 랜덤 포레스트
- Boosting
  - 연속적인 모델 학습을 통해 이전 모델의 오류를 보완하는 방법
  - XGBoost, LightGBM
- Stacking
  - 여러 모델의 예측값을 취하여 새로운 '메타 모델'을 학습시키는 전략
