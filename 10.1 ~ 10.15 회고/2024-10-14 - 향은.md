## 2024-10-14 회고 : 오늘 배운 내용 정리


### Transfomer란?
- 2017, 구글이 제안한 Seq2Seq 모델 중 하나
- 기존의 구조인 Encoder-Decoder방식을 따름+Attention만으로 구현한 모델
- RNN모델 사용X, encoder-decoder를 설계하였음에도 성능에서 RNN보다 우수함

### 기존의 Seq2Seq모델의 한계
- 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축
- 디코더는 이 벡터 표현을 통해 출력 시퀀스를 만들어냄
- 이러한 구조는
  - 인코더의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실된다는 단점이 발생
  - 이를 보정하기 위해 Attention모델 사용됨
 
- 그런데 어텐션을 RNN의 보정을 위한 용도로 사용하는 것이 아닌
- 어텐션만으로 인코더와 디코더를 만들어보자는 의견 -> 트랜스포머 등장

### 기계 번역에서의 seq2seq작업 예시

![image](https://github.com/user-attachments/assets/57c3c515-db2e-45b8-8e79-524556244e5e)

- 소스 시퀀스의 길이(토큰 6개)와 타깃 시퀀스의 길이(10개)가 다름
- 그러나 seq2seq에서는 인코딩 길이 고정 -> Attention으로 극복
-> **제대로 된 seq2seq는 소스,타깃의 길이가 달라도 과제 수행에 문제가 없어야 함**

### 트랜스포머의 encoder-decoder 구조
- 인코더 : 소스 시퀀스의 정보 압축 -> 디코더로 보냄
- 디코더 : 인코더가 보내준 소스 시퀀스의 정보를 받아 타겟 시퀀스 생성

![image](https://github.com/user-attachments/assets/f54cd716-71ba-4496-86e4-ce10604bfb25)

### 트랜스포머의 학습 방법
1. 'I'를 예측하는 학습
   - 인코더 입력 : 어제, 카페, 갔었어, 거기, 사람, 많더라 -> 소스 시퀀스 전체
   - 디코더 입력 : <S.>
   - 디코더의 최종 출력 : 타깃 언어의 어휘 수만큼의 차원으로 구성된 벡터 -> 요소의 값이 모두 확률 값

![image](https://github.com/user-attachments/assets/86b262ba-3f66-401b-a71f-d96e3398facd)

2. 트랜스포머의 학습 진행
   - 인코더와 디코더의 입력이 주어졌을 때, 정답에 해당하는 단어의 확률값을 높이는 방식으로 학습함
   - 모델은 이번 시점의 정답인 I에 해당하는 확률은 높이고, 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신

![image](https://github.com/user-attachments/assets/da4c5469-abe3-4aad-910f-c3fd1c347e6f)

3. 'went'를 예측
   - 인코더 입력 : 어제, 카페, 갔었어, 거기, 사람, 많더라 -> 소스 시퀀스 전체
   - 디코더 입력 : <S.> I
   - 특이 사항
     - 학습 중의 디코더 입력과 학습을 마친 후, 모델을 실제 기계번역에 사용할 때의 디코더 입력이 다름
     - 학습 중 : 디코더 입력에 예측해야 할 단어(went) 이전의 정답 타깃 시퀀스(<S.> I)를 넣어줌
     - 학습 후(인퍼런스 때) : 현재 디코더 입력에 직전 디코딩 결과를 사용
     - 예) 인퍼런스 떄 직전 디코더 출력이 I 대신 YOU가 나오면 다음 디코더 입력는 <S.> YOU

4. 'went'확률 높이기
   - 이번 시점의 정답인 went에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 전체 갱신

...

### 트랜스포머 블록
- 트랜스포머는 블록 형태로 구성된 인코더, 디코더 수십개가 반복적으로 쌓여 있는 상태
- 이런 구조를 블록 or 레이어 라고 부름
- 인코더 블록 :  3가지 요소로 구성
  - 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 & 레이어 정규화
- 디코더 블록 : 인코더 블록이 변형된 형태
  - 마스크 멀티 헤드 어텐션 추가됨
 
![image](https://github.com/user-attachments/assets/bdf0cc53-e29b-4dfc-82a4-4de80ba17045)

### 멀티 헤드 어텐션 (= 셀프 어텐션)
- 어텐션
  - 시퀀스 입력에 수행하는 기계학습 방법의 일종
  - 시퀀스 요소 가운데 중요한 요소에 집중, 그렇지 않은 요소는 무시 -> 테스크 수행 성능 올림
  - 디코딩 시 소스언어의 단어 시퀀스 중에서 디코딩에 도움이 되는 단어 위주로 취사 선택 -> 번역 품질 올림
- 셀프 어텐션은 자신에게 수행하는 어텐션 기법
- 트랜스포머 경쟁력의 원천으로 평가

#### CNN과 비교하면
- CNN : 합성곱 필터를 이용 > 시퀀스의 지역적 특징을 잡아냄
  - 자연어는 기본적으로 시퀀스
  - 특정 단어를 기준으로 한 주변 문맥이 의미 형성에 중요한 역할 수행 -> 자연어 처리에 CNN도 사용되기 시작
  - 합성곱 필터 크기를 넘어서는 문맥은 읽어내기 어려움

![image](https://github.com/user-attachments/assets/ec9393f7-81af-4631-be06-88465fec950e)

#### RNN과 비교하면
- RNN : 시퀀스 정보 압축에 강점이 있는 구조
  - 소스 시퀀스를 차례대로 처리
  - 시퀀스 길이가 길어질수록 정보 압축에 문제 발생

#### 어텐션과 비교하면

![image](https://github.com/user-attachments/assets/86c2b134-ec26-4d54-818d-1c4fd14b98b9)

- 그림을 보면, cafe에 대응하는 소스 언어의 단어 : 카페 -> 소스 시퀀스의 초반부에 등장
- cafe라는 단어를 디코딩 할 때, 카페를 반드시 참고
- 어텐션이 없는 단순 RNN이라면, 워낙 초반에 입력된 단어라 잊었을 가능성 큼 -> 번역 품질이 낮아질 수 있음

#### 셀프 어텐션과 어텐션의 주요 차이
- 어텐션은 소스 시퀀스 전체 단어들과 타깃 시퀀스 단어 하나 사이를 연결하는데 사용
- 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결

![image](https://github.com/user-attachments/assets/3798d392-4bd2-4f15-85b5-e178d0800ced)

- 어텐션은 RNN구조 위에서 동작하지만 셀프 어텐션은 없이 동작함
- 타깃 언어의 단어를 1개 생성할 때, 어텐션은 1회 수행, 셀프 어텐션은 인코더, 디토더 블록의 개수만큼 반복 수행


### 하이퍼 파라미터
- 머신러닝에서 모델링 시에 사용자가 직접 설정해 주는 값
- 학습률 등 다양한 종류

#### 하이퍼 파라미터의 정의 및 특성
- 모델 하이퍼파라미터는 모델 외부에 있고 데이터에서 값을 추정할 수 없는 구성
  - 하이퍼 파라미터 : 모델 외부에서 사용자가 직접 지정하는 값
  - 모델 매개 변수를 추정하는 데 도움이 되는 프로세스에서 자주 사용
  - 파라미터 : 모델 내부에서 결정되는 매개변수, 데이터로부터 결정
 

### 트랜스포머의 주요 하이퍼 파라미터

![image](https://github.com/user-attachments/assets/26d5e387-f3f2-4caa-85f4-7c2f48660fe0)

- 트랜스포머의 인코더, 디코더에서의 정해진 입력과 출력의 크기를 의미

![image](https://github.com/user-attachments/assets/8187edce-0b9b-43c2-bfc5-ecb6b0f62b80)

- 트랜스포머 모델에서 인코더, 디코더가 총 몇 층으로 구성되었는지 의미

![image](https://github.com/user-attachments/assets/8f2ec4ef-e522-4b1e-9fb1-f90bbddbd73a)

- 어텐션을 사용할 때, 여러 개로 분할해서 병렬로 수행하고 결과값을 다시 하나로 합치는 방식, 이때 이 병렬의 개수를 의미

![image](https://github.com/user-attachments/assets/5eb2825e-fa56-4cd3-9e8c-f3039a70401f)

- 트랜스포머 내부에 존재하는 피드 포워드 신경망의 은닉층의 크기를 의미

### 인코더-디코더 구조
![image](https://github.com/user-attachments/assets/df3c9116-f055-470b-ac86-32cb46665422)

- seq2seq : 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점을 가지는 구조
- 트랜스포머 : 인코더와 디코더라는 단위가 N개로 구성되는 구조

### 트랜스포머의 동작 형태
![image](https://github.com/user-attachments/assets/1defe1d1-208f-4f9b-a3ca-d04005099312)

- 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 구조
- 디코더는 seq2seq처럼 시작 심볼 sos를 입력받아 종료 심볼 eos가 나올 때까지 연산 진행
- RNN사용 x, 구조는 유지
  - RNN이 자연어 처리에서 유용한 이유
    - 단어의 위치에 따라 단어를 순차적으로 입력받아 처리하는 RNN은 각 단어의 우치 정보를 가질 수 있음
  - 트랜스포머의 경우
    - 단어 입력을 순차적으로 받지 X -> 단어의 위치 정보를 알려줄 방법 필요
    - 포지셔널 인코딩(Positional Encoding) 적용
      - 단어의 위치 정보를 얻기 위해
      - 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용
     

### 포지셔널 인코딩
- 입력되는 임베딩 벡터들은 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 추가됨

![image](https://github.com/user-attachments/assets/86510cff-d740-4795-9dca-0392de36fabd)

- 임베딩 벡터에 포지셔널 인코딩값이 더해지는 과정

![image](https://github.com/user-attachments/assets/037ab576-b149-4975-8d84-b62476d382b2)

- 포지셔널 인코딩 값의 계산
  - 트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래의 두 개의 함수를 사용
 
![image](https://github.com/user-attachments/assets/0c7cdda9-ab22-4654-a7e8-5d6a7c8c5568)

![image](https://github.com/user-attachments/assets/daaab597-ada3-42f6-825b-573ba9d2edf4)

- 짝수인 경우 사인 함수 값 사용
- 홀수인 경우 코사인 함수 값 사용


### 트랜스포머에서 사용되는 어텐션
- Encoder Self-Attention
- Masked Decoder Self-Attention
- Encoder-Decoder Attention

![image](https://github.com/user-attachments/assets/051198e0-f327-47b1-8a34-810ca5494f88)

![image](https://github.com/user-attachments/assets/e30bb699-b53a-4b28-8f06-260fce95f6ee)


### 인코더의 구조
![image](https://github.com/user-attachments/assets/a6f5a573-68a4-46bf-8c2a-0a6ea04acbab)

- 트랜스포머는 하이퍼 파라미터인 num_layers개의 인코더 층이 쌓여서 구성됨

#### 셀프 어텐션
- 기존 어텐션의 개념
  - 어텐션 함수는 주어진 쿼리에 대해 모든 키와 유사도를 각각 계산
  - 계산된 유사도를 가중치로 하여 키와 맵핑되어있는 각각의 값에 반영
  - 유사도가 반영된 값을 모두 가중합하여 리턴

    ![image](https://github.com/user-attachments/assets/209066e7-e52f-4044-82d8-ab801644b61e)

- 셀프 어텐션
  - 어텐션을 자기 자신에게 수행한다는 의미
  - Q = 쿼리 : 모든 시점의 디코더 셀에서의 은닉 상태
  - K = 키 : 모든 시점의 인코더 셀의 은닉 상태들
  - V = values : 모든 시점의 인코더 셀의 은닉 상태들
  - 전부 동일함

  - 단어 벡터를 Q,K,V의 벡터로 변환하는 과정

  ![image](https://github.com/user-attachments/assets/9e802a6a-cddf-4786-94c7-01fee5ff7e63)

  - 기존의 벡터로부터 더 작은 벡터는 가중치 행렬을 곱해서 만듦
  - 가중치는 훈련 과정에서 학습됨
  - 각 가중치 행렬의 크기는 아래와 같음
 
    ![image](https://github.com/user-attachments/assets/1380c001-3a24-4e71-9c11-df24161602be)


### 인코더 -> 디코더
- 인코더는 총 num_layers 만큼의 층 연산을 순차적으로 한 후
- 마지막 층의 인코더의 출력을 디코더에 전달
- 디코더도 num_layers만큼의 연산 수행
- 매 연산마다 인코더가 보낸 출력을 각 디코더 층 연산에 사용

![image](https://github.com/user-attachments/assets/dc708372-693a-49e4-8bb8-882b9174cdc3)

