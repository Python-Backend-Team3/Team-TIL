# 🚩 10월 14일 회고
# ⛱️ 언어 모델: Transformer 아키텍처
## 1. Transformer 개요
### 1.1 정의
- 2017년, 구글이 제안한 Sequence-to-Sequence 모델의 하나
- 기존의 seq2seq 구조인 Encoder-Decoder 방식을 따르면서도 논문 제목처럼 Attention 만으로 구현한 모델임
- RNN 모델을 사용하지 않고 Encoder-Decoder를 설계하였음에도 불구하고 번역 등의 성능에서 RNN 모델보다 우수한 성능을 보임
- RNN에서 사용한 순환방식을 사용하지 않고 순수하게 어텐션만 사용한 모델
- 기존에 사용되었던 RNN, LSTM, GRU 등은 점차 트랜스포머로 대체되기 시작
- GPT, BERT, T5 등과 같은 다양한 자연어 처리 모델에 트랜스포머 아키텍처가 적용됨
    
### 1.2 기존의 seq2seq 모델의 한계
- 기존의 seq2seq 모델은 인코더-디코더 구조로 구성

- 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 통해서 출력 시퀀스를 만들어 냄
  - 이러한 구조는 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실된다는 단점이 발생하였고, 이를 보정하기 위해 어텐션(Attention) 모델이 사용됨

- 예시
  ![image](https://github.com/user-attachments/assets/7248aef2-d72d-4b5d-bf7c-3a18d5ba781a)

### 1.3 트랜스포머의 인코더-디코더 구조
- 인코더 : 소스 시퀀스의 정보를 압축해 디코더로 보냄
- 디코더 : 인코더가 보내 준 소스 시퀀스의 정보를 받아서 타겟 시퀀스 생성

  ![image](https://github.com/user-attachments/assets/905c2425-38d3-45b7-9427-50c2dfe6f5b9)

### 1.4 트랜스포머의 학습 방법

![image](https://github.com/user-attachments/assets/3c3b55df-026b-4445-b6a1-1a7da65646c1)

- ① ‘I’를 예측하는 학습
  - 인코더 입력: 어제, 카페, 갔었어, 거기, 사람, 많더라 → (소스 시퀀스 전체)
    - 소스 시퀀스를 압축하여 디코더로 보냄

   - 디코더 입력: ![image](https://github.com/user-attachments/assets/c83ebdd4-3bcd-4c39-beee-2ebba9cb6e6b)

     - 인코더에서 보내온 정보와 현재 디코더 입력을 모두

  - 디코더의 최종 출력
    - 타깃 언어의 어휘 수만큼의 차원으로 구성된 벡터 → 이 벡터는 요소의 값이 모두 확률 값

      (예: 타깃 언어의 어휘가 총 3만개라면 디코더 출력은 3만 차원의 벡터, 3만개 각각은 확률값)
    
- ② 트랜스포머의 학습 진행
  - 인코더와 디코더의 입력이 주어졌을 때, 정답에 해당하는 단어의 확률값을 높이는 방식으로 학습함
    - 모델은 이번 시점의 정답인 I에 해당하는 확률은 높이고, 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신

    ![image](https://github.com/user-attachments/assets/e64a4b37-99c9-4ad9-b038-e60620f87054)

- ③ ‘went’를 예측할 차례
  - 인코더 입력: 어제, 카페, 갔었어, 거기, 사람, 많더라 → (소스 시퀀스 전체)

  - 디코더 입력: ![image](https://github.com/user-attachments/assets/50e903f5-0733-47f3-b424-b4781099b918)

  - 특이 사항
    - 학습 중의 디코더 입력과 학습을 마친 후 모델을 실제 기계번역에 사용할 때(인퍼런스)의 디코더 입력이 다름
      - 학습 중: 디코더 입력에 예측해야 할 단어(went) 이전의 정답 타깃 시퀀스(![image](https://github.com/user-attachments/assets/aa03ee73-cace-48cd-90e8-590a2b407c7f)
)를 넣어줌
      - 학습 후(인퍼런스 때): 현재 디코더 입력에 직전 디코딩 결과를 사용
(예: 인퍼런스 때 직전 디코더 출력이 I 대신 you가 나오면 다음 디코더 입력은 ![image](https://github.com/user-attachments/assets/75a904cf-ebc3-489f-bb93-39f766331f19)
 you)

- ④ ‘went’확률 높이기
  - 학습 과정 중 인코더, 디코더의 입력이 아래 그림과 같으면 모델은 이번 시점의 정답인 went에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신함
 
    ![image](https://github.com/user-attachments/assets/14d18b82-f20e-4fe9-9103-c28819637260)
    
- ⑤ ‘to’ 예측하기
  
  ![image](https://github.com/user-attachments/assets/dfe0682c-7206-400b-b849-aaece6998f7f)

  - 인코더 입력은 소스 시퀀스 전체

  - 디코더 입력은 정답인 ![image](https://github.com/user-attachments/assets/bd6d9b0d-2765-45b5-b220-8fd93757e678)

  - 이번 시점의 정답인 to에 해당하는 확률은 높이고 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신함

![image](https://github.com/user-attachments/assets/df0b25ce-3ef7-4741-bf89-229ae1287386)

### 1.5 트랜스포머 블록
- 트랜스포머는 블록 형태로 구성된 인코더, 디코더 수십개가 반복적으로 쌓여 있는 형태를 블록 또는 레이어라고 부름

- 인코더 블록: 3가지 요소로 구성
  - 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 & 레이어 정규화

- 디코더 블록: 인코더 블록이 변형된 형태
  - 마스크 멀티 헤드 어텐션 추가됨

![image](https://github.com/user-attachments/assets/f939a424-3f3d-4972-adcc-56304c0e41ac)

### 1.6 셀프 어텐션
- 트랜스포머 구조에서 “멀티 헤드 어텐션”을 “셀프 어텐션“ 이라고 부름

- 어텐션
  - 시퀀스 입력에 수행하는 기계학습 방법의 일종
  - 시퀀스 요소 가운데 중요한 요소에 집중하고, 그렇지 않은 요소는 무시함으로써 태스크 수행 성능을 끌어올림
  - 디코딩 시 소스언어의 단어 시퀀스 중에서 디코딩에 도움이 되는 단어 위주로 취사 선택(가장 중요한 것만 추려서), 번역 품질을 끌어올림

- 셀프 어텐션은 자신에게 수행하는 어텐션 기법

- 트랜스포머 경쟁력의 원천으로 평가

### 1.7 셀프 어텐션과의 비교
- ① 합성곱 신경망(CNN)을 비교

![image](https://github.com/user-attachments/assets/1151967b-030c-4d80-8833-6f8af8f84951)

  - CNN: 합성곱 필터를 이용해 시퀀스의 지역적 특징을 잡아낼 수 있음
    - 자연어는 기본적으로 시퀀스
    - 특정 단어를 기준으로 한 주변 문맥이 의미 형성에 중요한 역할 수행 → 자연어 처리에 CNN도 사용되기 시작
    - 합성곱 필터 크기를 넘어서는 문맥은 읽어 내기 어려움 → 필터 크기가 3이면 4칸 이상 떨어져 있는 단어 사이의 의미는 잡아내기 어려움

- ② 순환 신경망(RNN)을 비교

  - RNN: 시퀀스 정보 압축에 강점이 있는 구조
    - 소스 시퀀스를 차례대로 처리
    - 그러나 RNN은 시퀀스 길이가 길어질 수록 정보 압축에 문제 발생
      - 오래 전에 입력된 단어를 잊어버리거나
      - 특정 단어 정보를 과도하게 반영해 전체 정보를 왜곡시키는 문제
    - “어제, 카페, 갔었어, 거기, 사람, 많더라”를 RNN으로 기계번역 한다면 → 인코더가 디코더로 넘기는 정보는 소스 시퀀스의 마지막 단어인 “많더라”의 의미가 많이 반영됨(입 력 정보를 차례로 처리하고, 오래된 단어는 잊어버리는 경향이 있기때문)

- ③ 어텐션을 비교

![image](https://github.com/user-attachments/assets/f08de97d-c130-4a55-bb88-9d5c1e7f26ed)

- cafe에 대응하는 소스 언어의 단어: 카페 → 소스 시퀀스의 초반부에 등장

- cafe라는 단어를 디코딩 할 때, 카페를 반드시 참조

- 어텐션이 없는 단순 RNN이라면... 워낙 초반에 입력된 단어라 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아 질 수 있다.

#### - 셀프 어텐션과 어텐션의 주요 차이
 
 ![image](https://github.com/user-attachments/assets/73aec88b-0c5a-4cb5-a4df-193455244c1e)

  - 어텐션은 소스 시퀀스 전체 단어들과 타깃 시퀀스 단어 하나 사이를 연결하는데 사용. 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결

  - 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작함
 
  - 타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만, 셀프 어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행

#### - 셀프 어텐션 계산 예시

![image](https://github.com/user-attachments/assets/03266557-018a-4e27-aef6-704792910e8c)

- 쿼리(Q), 키(K), 값(V)이 서로 영향을 주고 받으면서 문장의 의미 계산
- 각 단어 벡터는 블록 내에서 계산과정을 통해 Q, K, V로 변환
- 쿼리 단어 각각을 대상으로 모든 키 단어와 얼마나 유기적인 관계를 맺는지를 총합이 1인 확률 값으로 표현
- “카페”와 가장 관련이 높은 단어는 “거기(0.4)”
<br><br/>

## 2. 하이퍼 파라미터
### 2.1 하이퍼 파라미터
- 머신러닝에서 모델링 시에 사용자가 직접 설정해 주는 값

- 학습률(Learning Rate) 등 다양한 종류가 있음

- 하이퍼 파라미터의 정의 및 특성(By Machine Learning Mastery)
  - 모델 하이퍼파라미터는 모델 외부에 있고 데이터에서 값을 추정할 수 없는 구성
    - 모델 매개 변수를 추정하는 데 도움이 되는 프로세스에서 자주 사용
    - 휴리스틱을 사용하여 설정할 수 있음

![image](https://github.com/user-attachments/assets/97b14b14-1c51-4149-b131-e628a99e9b18)

### 2.2 트랜스포머의 주요 하이퍼 파라미터

- ![image](https://github.com/user-attachments/assets/fadcb33a-5ab7-48c0-b4e1-6b6323f52a10) : 트랜스포머의 인코더와 디코더에서의 정해진 입력과 출력의 크기를 의미함

- ![image](https://github.com/user-attachments/assets/baa4f33f-7d1a-4bbb-95e7-419e153a9dff) : 트랜스포머 모델에서 인코더와 디코더가 총 몇 층으로 구성되었는지를 의미함

- ![image](https://github.com/user-attachments/assets/622504a3-de44-47d4-854b-42ac33fd57e2) : 트랜스포머에서는 어텐션을 사용할 때, 한 번 하는 것 보다 여러 개로 분할해서 병렬로 어텐션을 수행하고 결과값 을 다시 하나로 합치는 방식을 택했으며, 이때 이 병렬의 개수를 의미함

- ![image](https://github.com/user-attachments/assets/7a400175-7b43-40a3-aaca-72b8914b12b1) : 트랜스포머 내부에 존재하는 피드 포워드 신경망의 은닉층의 크기를 의미함

### 2.3 트랜스포머의 전체 구조
- 인코더-디코더 구조

![image](https://github.com/user-attachments/assets/57d243e1-51ef-478e-8f93-d3fd410eab06)

![image](https://github.com/user-attachments/assets/2bd189d9-64d4-41a8-8062-277c1be19ac0)

![image](https://github.com/user-attachments/assets/521971c4-e33e-44d7-9856-db2ade1527d4)
<br><br/>

## 3. 포지셔널 인코딩
### 3.1 트랜스포머의 입력
- RNN이 자연어 처리에서 유용했던 이유
    - 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보 (position information)를 가질 수 있었기 때문

- 트랜스포머의 경우
    - 단어 입력을 순차적으로 받지 않음 → 단어의 위치 정보를 알려줄 방법 필요
    - 포지셔널 인코딩(Positional Encoding) 적용
      - 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용

### 3.2 포지셔널 인코딩
- 입력되는 임베딩 벡터들은 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 추가됨

![image](https://github.com/user-attachments/assets/f57fde4c-e09f-4793-8418-fbdec6ed6e71)

- 임베딩 벡터에 포지셔널 인코딩값이 더해지는 과정

![image](https://github.com/user-attachments/assets/086cc0df-1ac1-4f6c-8422-d9817e94640d)

- 포지셔널 인코딩 값의 계산 : 트랜스포머는 위치 정보를 가진 값을 만들기 위해서 아래의 두 개의 함수를 사용

![image](https://github.com/user-attachments/assets/b3f9c627-3a4c-496a-86d3-e6604aecb237)

![image](https://github.com/user-attachments/assets/6fe2961f-aa7d-4046-a243-6dcd0cc7922f)

- 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존됨
  - 각 임베딩 벡터에 포지셔널 인코딩의 값을 더하면 같은 단어라고 하더라도 문장 내의 위치에 따라서 트랜스포머의 입력으 로 들어가는 임베딩 벡터의 값이 달라짐
  - 이에 따라 트랜스포머의 입력은 순서 정보가 고려된 임베딩 벡터가 됨
<br><br/>

## 4. 어텐션
- 트랜스포머에서 사용되는 어텐션
  - Encoder Self-Attention
  - Masked Decoder Self-Attention
  - Encoder-Decoder Attention

![image](https://github.com/user-attachments/assets/0c81cf1b-1f0b-4856-bdc4-986a6d6bb75f)

![image](https://github.com/user-attachments/assets/1209bbdc-1b48-49d9-8ccb-8f4d635661ff)
<br><br/>

## 5. 인코더
### 5.1 인코더의 구조

![image](https://github.com/user-attachments/assets/548ec7a2-02f5-455b-80a8-05981a5c032b)

- 트랜스포머는 하이퍼 파라미터인 num_layers 개의 인코더 층이 쌓여서 구성됨

- 인코더를 하나의 층으로 생각한다면 하나의 인코더 층은 크게 총 2개의 서브층으로 구분
    - 셀프 어텐션 (= 멀티 헤드 셀프 어텐션)
    - 피드 포워드 신경망 (= 포지션 와이즈 피드 포워드 신경망)











