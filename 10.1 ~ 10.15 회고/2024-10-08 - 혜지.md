# 🗃️ 10월 8일 회고

# ✨ 임베딩

## 1. 임베딩
### 1-1. 개요
- **임베딩** : 단어나 문장을 수치화 하여 벡터공간으로 표현하는 과정

- **임베딩이 필요한 이유**
  - 컴퓨터는 자연어를 직접적으로 처리할 수 없다
  - 컴퓨터는 수치 연산만 가능하다
  - 따라서 자연어를 숫자나 벡터 형태로 변환할 필요가 있다
  - 이때 사용되는 일련의 과정을 “임베딩”이라고 한다

- **임베딩 기법의 종류**
  - 문장 임베딩 : 문장 전체를 벡터로 표현하는 방법
  - 단어 임베딩 : 개별 단어를 벡터로 표현하는 방법

### 1-2. 단어 임베딩
- 말뭉치에서 각각의 단어를 벡터로 변환
- 의미와 문법적 정보를 지님
- 단어를 표현하는 방법에 따라 다양한 모델 존재
- 토크나이징을 통해 문장에서 토큰 단위를 추출하는 경우, 추출된 토큰은 형태소 기반이므로 단어 임베딩이 효과적
<br><br/>

## 2. 단어 임베딩
### 2-1. 개요 
- 말뭉치에서 각각의 단어를 벡터로 변환
- 의미와 문법적 정보를 지님
- 단어를 표현하는 방법에 따라 다양한 모델 존재
- 토크나이징을 통해 문장에서 토큰 단위를 추출하는 경우, 추출된 토큰은 형태소 기반이므로 단어 임베딩이 효과적

### 2-2. 원핫 인코딩 (One-Hot Encoding)
#### 2-2-1. 개요
- 단어를 숫자 벡터로 변환하는 가장 기본적인 방법
- 요소들 중 단 하나의 값만이 1이고 나머지 요소들의 값은 0인 인코딩 방식
  
  ![image](https://github.com/user-attachments/assets/988e9dbe-7682-4cf3-b0a8-ca811868620c)

#### 2-2-2. 특징과 단점
- 원핫 인코딩 벡터의 차원은 전체 어휘의 개수 → 매우 큰 차원이 됨

  ex) 총 20,567 종의 동물 이름의 경우
![image](https://github.com/user-attachments/assets/0987d2a6-8726-4068-b5ec-d10504143c0d)

- 단어는 불연속적인 심볼이며 이산 확률 변수로 나타남 → 원핫 벡터는 이산 확률 분포에서 추출한 샘플 → 불연속적인 값을 가짐

![image](https://github.com/user-attachments/assets/19e2613c-7adc-4749-9620-ac26e5a88f7d)

### 2-3. 희소 표현
- 원-핫 인코딩 : 표현하고자 하는 단어의 인덱스 요소만 1이고 나머지 요소는 모두 0으로 표현되는 희소 벡터(또는 희소 행렬)

  → 단어가 희소 벡터로 표현되는 방식: 희소 표현

- 각각의 차원이 독립적인 정보를 지님

- 자연어 처리를 잘 하려면 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야 함 → 희소 표현은 이런 조건을 만족하지 못함

### 2-4. 분산 표현
- 희소 표현의 단점 해결을 위해 고안됨

- 각 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법

- 한 단어의 정보가 특정 차원에 표현되지 않고 여러 차원에 분산되어 표현됨

- 희소 표현에 비해 많은 장점을 가지므로 단어 임베딩 기법에서 많이 사용됨

![image](https://github.com/user-attachments/assets/fbfdc1c4-b221-47e0-96b3-f19ccf41e87d)

### 2-5. 희소 표현 대신 분산 표현을 사용한다면
- 임베딩 벡터의 차원을 데이터 손실을 최소화하면서 압축할 수 있다

- 임베딩 벡터에 단어의 의미, 주변 단어와의 관계 등 많은 정보가 내포되어 있다 → 일반화 능력이 뛰어남

![image](https://github.com/user-attachments/assets/efebc39f-1136-40ca-b8ae-3845b61af84d)
<br><br/>

## 3. 단어의 의미 파악
- 단어의 의미를 파악하기 위한 기법 : 시소러스 활용 기법, 통계 기반 기법, 추론 기반 기법

### 3-1. 시소러스 활용 기법
- 단어들을 의미의 상/하위 관계에 따라 그래프로 표현, 처리
  - 단어들을 의미의 상위, 하위 관계에 기초하여 그래프로 표현
  - 모든 단어에 대한 유의어 집합을 만들고, 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의한 후 단어의 네트워크를 이용하여 컴퓨터에게 단어 사이의 관계를 주입 → 컴퓨터에게 단어의 의미를 이해시킨 것으로 간주함

 ![image](https://github.com/user-attachments/assets/80eae493-cd84-4c1f-9e0b-2ae64af79e05)

 #### 3-1-1. 문제점
 - 시대 변화에 대응하기 어려움 : 언어는 변한다 → 사람이 꾸준히 갱신, 관리해야 함

 - 높은 비용 : 시소러스를 만들고 관리하기 위한 고가의 인적비용 발생

 - 단어의 미묘한 차이를 표현할 수 없다 (뜻은 비슷하지만 느낌, 활용 형태, 활용 상황이 다른 경우가 매우 많음 → 대응이 어려움)

 - **문제의 해결을 위하여: 통계 기반 기법, 신경망을 이용한 추론 기반 기법 등이 제안됨**
 
### 3-2. WordNet (시소러스의 대표적인 모델)
- 기계번역을 돕기 위한 목적으로 개발됨

- 동의어 집합, 상위어, 하위어에 대한 정보가 잘 구축되어 있음

- 유향 비순환 그래프(Directed Acyclic Graph, DAG)로 구성됨 (트리구조가 아닌 이유: 하나의 노드가 여러 개의 상위 노드를 가질 수 있음)

![image](https://github.com/user-attachments/assets/5dd53f9d-2180-4349-b27c-63aafd3a5d8a)

- **특징 추출하기**
  - TF-IDF(Term Frequency-Inverse Document Frequency)
    - 어떤 단어가 출현한 문서의 숫자의 역수
    - 어떤 단어 w가 문서 d 내에서 얼마나 중요한지 나타내는 수치
    - 이 수치가 높을 수록 w는 d를 대표하는 성질을 띠게 된다고 볼 수 있음
    - ![image](https://github.com/user-attachments/assets/7379ff9e-9263-4803-850d-a80d4e090ef5)
   
### 3-2. 통계 기반 기법
#### 3-2-1. 개요
- 통계를 기반으로 어떤 데이터가 많이 사용되는지, 데이터가 어떻게 분포되어 있는지, 데이터의 분포에 따라 어떤 연관성과 의미를 갖는지 등의 정보를 추출하여 자연어를 처리하는 기법

- 통계를 사용하므로 통계 결과를 계산하기 위한 충분한 크기를 가진 대규모의 텍스트 데이터가 요구됨

- **말뭉치(Corpus)를 이용**
  - 말뭉치: 자연어 처리에 대한 연구, 애플리케이션의 개발 등을 염두에 두고 수집된 대량의 텍스트 데이터
  - 말뭉치 자체는 단순한 텍스트 데이터이지만 텍스트에 담긴 문장은 사람이 쓴 글이다
  - 말뭉치에서 자동으로, 그리고 효율적으로 핵심을 추출하는 것 → 통계 기반의 자연어 처리 기법

#### 3-2-2. 분포 가설 (Distributional Hypothesis)
- 단어의 의미는 주변 단어에 의해 형성된다.
- 단어 자체에는 별 의미가 없고 그 단어가 사용된 맥락(Context)이 의미를 형성한다.
- 자연어 처리에 관한 중요한 기법은 대부분 분포가설을 기반으로 한다

![image](https://github.com/user-attachments/assets/b4bd3913-cdd2-4ad0-b155-f29c7eff05e6)

![image](https://github.com/user-attachments/assets/03005a3b-d9ce-4671-bc3d-75db81f5c580)

- **분포 가설을 기초로 통계적 분석을 하려면?**
  - 일단 어떤 단어가 몇 번이나 나오는지 세어보자! → 핵심
  - 동시발생 행렬: 주어진 단어의 맥락으로써 동시에 발생하는 단어의 출현빈도
    - 맥락의 크기를 윈도우라고 하고, 윈도우가 1인 경우(주어진 단어에서 한 단어까지 처리)

![image](https://github.com/user-attachments/assets/e6fff6f2-9691-491f-bfaa-14b9495da0eb)

  - 맥락이란 특정 단어를 중심에 둔 주변 단어를 말함
    - 맥락의 크기: “윈도우”라고 부름
      
![image](https://github.com/user-attachments/assets/cad410cf-97f4-4f29-a477-a76b4bb7d22d)

- **동시발생 행렬**
  - 분포 가설에 기초하여 단어를 벡터로 바꾸는 방법
  - 단어가 몇 개나 나오는지 세어본다. (통계 기반 기법)
![image](https://github.com/user-attachments/assets/74e3b8ea-fa36-495b-9139-d14ddce4e190)

![image](https://github.com/user-attachments/assets/a867d09b-59c2-463e-a80c-5e78bc7fe2a5)

![image](https://github.com/user-attachments/assets/876e5bec-0bb4-44a1-a531-8044be1c4d23)

- 유사 단어의 랭킹 표시
  - 유사도가 높은 순서로 순위를 매겨서 활용
  - 입력 데이터인 말뭉치(텍스트 데이터)가 커질수록 랭킹은 정확해짐
  - 말뭉치가 작을수록 납득하기 어려운 순위를 보여줌
 
- 개선 사항
  - 통계 기반 기법에서는 단어의 동시발생 행렬을 이용함
  - 발생횟수, 단어의 빈도수는 많이 활용되는 특성이지만 좋은 특징은 아님
  - 전문적인 내용의 말뭉치일 경우, 해당 주제에 관련된 말이 비정상적으로 많이 나올 수 있음
  - 영어권 언어에서는 관사 등 실제 의미와 무관한 단어가 최고 빈도수를 가짐

- 차원 감소
  - 벡터의 차원을 줄이는 기법
  - 단순한 차원 감소가 아니라 “중요한 정보”는 최대한 유지하면서 줄여야 함
  - 단어 벡터는 주로 원핫 인코딩과 같은 희소벡터(=희소행렬)를 사용하므로 희소벡터에서 중요한 축을 찾아내어 더 적은 차원으로 다시 표현하는 것이 핵심
  - 차원 감소 결과는 원소 대부분이 0이 아닌 값으로 구성된 밀집벡터가 됨 → 단어의 분산 표현
  - 다양한 차원감소 기법 중 단어 처리에서 주로 사용되는 것은 → 특잇값 분해(Singular Value Decomposition, SVD)
 
- 특잇값 분해 (SVD)
![image](https://github.com/user-attachments/assets/2c85a468-345f-4d54-868f-e5f2dd7be937)

### 3-3. 추론 기반 기법
- 신경망을 이용한 추론을 기반으로 단어의 분산 표현을 얻는 기법
- 통계 기반 기법과 마찬가지로 분포 가설을 기초로 함

![image](https://github.com/user-attachments/assets/2a38a898-71e9-45b3-a29c-dd66a0496fd9)

![image](https://github.com/user-attachments/assets/c8a598bd-2d8d-4a83-847b-96fdab383cb1)

![image](https://github.com/user-attachments/assets/d8a4fe8c-84d0-47df-a492-c26b9836f9fc)

![image](https://github.com/user-attachments/assets/cf371b00-edfe-4fa0-8d71-e5da784f84dc)

#### 3-3-1. Word2Vec
- 2013년 Google에서 발표, 현재 가장 많이 사용되고 있는 단어 임베딩 모델

- 기존 신경망 기반의 단어 임베딩과 구조의 차이는 크지 않으나 계산량을 획기적으로 줄여 빠른 학습을 가능하게 함

- CBOW(Continuous Bag-of-Words) 모델과 skip-gram 모델이 있음
  - CBOW : 신경망의 입력을 주변 단어들로 구성하고 출력을 타깃 단어로 설정하여 학습된 가중치 데이터를 임베딩 벡터로 활용
  - skip-gram : 하나의 타깃 단어를 이용하여 주변 단어들을 예측하는 모델(CBOW와 반대)

![image](https://github.com/user-attachments/assets/b78f1f82-9992-4af9-ab23-69ddcd0eba29)

![image](https://github.com/user-attachments/assets/d13c2fe6-ff84-4faa-b2db-5e134f3b9e56)

![image](https://github.com/user-attachments/assets/68f5f275-53ca-48af-b8a7-91b023fbb1cf)

![image](https://github.com/user-attachments/assets/709f2896-0d9b-47c7-a503-6155ce6ee63c)

![image](https://github.com/user-attachments/assets/5900a04f-3ff6-4dfa-a629-476bae578f1c)

![image](https://github.com/user-attachments/assets/af4888a5-2256-44da-86da-dac15e2c4c8b)

#### 3-3-2. CBOW 모델 vs. skip-gram 모델
- Skip-gram 모델의 경우가 더 좋은 결과를 출력함
- 정밀도 면에서 skip-gram 모델이 우세
- 말뭉치가 커질수록 저빈도 단어, 유추문제 성능에서 skip-gram 모델이 뛰어남
- 단, 학습 속도는 CBOW 모델이 빠름
- Skip-gram 모델은 맥락의 수 만큼 손실을 구해야 하므로 계산비용이 높아짐

![image](https://github.com/user-attachments/assets/a68d0b7a-f8d8-45e5-b611-176d989f006e)












 
