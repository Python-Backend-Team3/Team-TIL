# 😎 10월 11일 회고
# 😄 언어 모델
## 1. 개요
### 1-1. 정의
- 언어 모델이란 문장의 확률을 나타내는 모델
- 문장은 단어들로 이루어진 시퀀셜 데이터이므로 단어 시퀀스를 입력 받아 해당 시퀀스가 얼마나 그럴듯한지 확률을 부여하는 모델이라고 볼 수 있다.
- 한국어 말뭉치로 학습된 언어모델은 자연스러운 한국어 문장에 높은 확률값 부여 → 한국어 언어 모델이 필요한 이유
- **언어를 이루는 구성 요소(글자, 형태소, 단어, 단어열(문장), 문단 등)에 확률 값을 부여하여 이를 바탕으로 다음 구성요소를 예측하거나 생성하는 모델**
  
### 1-2. 언어 모델의 기술적 분류
- 확률에 기초한 통계적 언어 모델 (Statistical Language Model, SLM)

  - 단어열이 가지는 확률 분포를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델

- 신경망에 기초한 딥러닝 언어모델 (Deep Neural Network Language Model, DNN LM)

### 1-3. 확률을 기반으로 단어의 조합을 예측한다는 것
- 주어진 단어를 통해 다음 단어로 돌 확률이 가장 높은 단어를 예측하는 일련의 과정을 의미
  - 예시) 스마트 폰의 “자동 완성“ 기능

- 조건부 확률(Conditional Probabilities)을 언어 현상에 적용해 보는 것에서 출발함

### 1-4. 문장의 확률 표현
![image](https://github.com/user-attachments/assets/2c609b42-4770-42c2-9440-1d931c7a2969)

### 1-5. 언어 모델의 결합 확률과 조건부 확률
- 3개의 단어가 동시에 등장할 결합 확률
  
![image](https://github.com/user-attachments/assets/bbfac158-7e2d-44d9-bcba-e316e046fcf0)

  - 다음의 3가지 사건이 동시에 발생해야 함

    - 첫 번째 단어(𝑤1) 등장
    - 첫 번째 단어(𝑤1) 등장 후 두 번째 단어 (𝑤2) 등장
    - 첫 번째 단어(𝑤1) 와 두 번째 단어 (𝑤2) 등장 후 세 번째 단어 (𝑤3) 등장
  
  - 조건부 확률로 다시 쓰면
    
    ![image](https://github.com/user-attachments/assets/d3b451ad-953d-4ee0-a4e4-88bbfaa36400)

- 우리는 임의의 단어 시퀀스가 해당 언어에서 얼마나 자연스러운지를 이해하고 있는 언어 모델을 구축하고자 한다.

- 조건부 확률의 정의에 따라 수식의 좌변, 우변이 같으므로 이전 단어(컨텍스트)들이 주어졌을 때 다음 단어를 맞히는 문제로도 목표를 달성할 수 있다
  
![image](https://github.com/user-attachments/assets/07224eb1-80ff-4d8d-87e0-007748268a56)

### 1-6. 언어 모델의 형태적 분류
- **순방향 언어 모델(Forward Language Model)**

  - 문장의 앞에서 뒤로, 사람이 이해하는 순서대로 계산하는 모델

  - ex) GPT(Generative Pretrained Transformer), ELMo 등

- **역방향 언어 모델(Backward Language Model)**

  - 문장의 뒤부터 앞으로 계산하는 모델
 
  - ex) ELMo(Embeddings from Language Models) 등 → ELMo는 순방향, 역방향 모두 사용한다.

### 1-7. 넓은 의미의 언어 모델
- 전통적인 의미의 언어 모델은 조건부확률의 정의를 따르는 수식으로 표현
- 최근에는 컨텍스트(주변 맥락 정보)가 전제된 상태에서 특정 단어(𝒘)가 나타날 조건부 확률로 표현하기도 한다.

   ![image](https://github.com/user-attachments/assets/afdf7885-78ba-4ff1-8963-4f6481625c06)

### 1-8. 잘 학습된 언어 모델
- 어떤 문장이 자연스러운지 가려낼 수 있으므로 그 자체로 가치가 있다.

- 학습 대상 언어의 풍부한 맥락을 표현하고 있다. → 기계 번역, 문법 교정, 문장 생성 등 다양한 태스크 수행 가능

![image](https://github.com/user-attachments/assets/c2236c63-9f4b-4c38-96ca-9252ff41ecf6)

### 1-9. 언어 모델의 변화 추세
- 전통적인 언어처리 연구에서 딥러닝 패러다임으로 전환

- 시퀀스를 위한 합성곱 연산(학습속도 향상)

- 어텐션 모델(주의모델)의 주도

- 전이학습 활용

- 다양한 모델의 조합
  - ex) 단어의 문자를 인식하는 CNN 모델 + 시퀀스 처리를 위한 LSTM(RNN) + MLP를 이용한 LSTM의 출력 분류 등
<br><br/>






















# 🤩 언어 모델 : Seq2Seq
