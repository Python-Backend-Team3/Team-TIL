# 😎 10월 11일 회고
# 😄 언어 모델
## 1. 개요
### 1-1. 정의
- 언어 모델이란 문장의 확률을 나타내는 모델
- 문장은 단어들로 이루어진 시퀀셜 데이터이므로 단어 시퀀스를 입력 받아 해당 시퀀스가 얼마나 그럴듯한지 확률을 부여하는 모델이라고 볼 수 있음
- 한국어 말뭉치로 학습된 언어모델은 자연스러운 한국어 문장에 높은 확률값 부여 → 한국어 언어 모델이 필요한 이유
- **언어를 이루는 구성 요소(글자, 형태소, 단어, 단어열(문장), 문단 등)에 확률 값을 부여하여 이를 바탕으로 다음 구성요소를 예측하거나 생성하는 모델**
  
### 1-2. 언어 모델의 기술적 분류
- 확률에 기초한 통계적 언어 모델 (Statistical Language Model, SLM)

  - 단어열이 가지는 확률 분포를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델

- 신경망에 기초한 딥러닝 언어모델 (Deep Neural Network Language Model, DNN LM)

### 1-3. 확률을 기반으로 단어의 조합을 예측한다는 것
- 주어진 단어를 통해 다음 단어로 돌 확률이 가장 높은 단어를 예측하는 일련의 과정을 의미
  - 예시) 스마트 폰의 “자동 완성“ 기능

- 조건부 확률(Conditional Probabilities)을 언어 현상에 적용해 보는 것에서 출발함

### 1-4. 문장의 확률 표현
![image](https://github.com/user-attachments/assets/2c609b42-4770-42c2-9440-1d931c7a2969)

### 1-5. 언어 모델의 결합 확률과 조건부 확률
- 3개의 단어가 동시에 등장할 결합 확률
  
![image](https://github.com/user-attachments/assets/bbfac158-7e2d-44d9-bcba-e316e046fcf0)

  - 다음의 3가지 사건이 동시에 발생해야 함

    - 첫 번째 단어(𝑤1) 등장
    - 첫 번째 단어(𝑤1) 등장 후 두 번째 단어 (𝑤2) 등장
    - 첫 번째 단어(𝑤1) 와 두 번째 단어 (𝑤2) 등장 후 세 번째 단어 (𝑤3) 등장
  
  - 조건부 확률로 다시 쓰면
    
    ![image](https://github.com/user-attachments/assets/d3b451ad-953d-4ee0-a4e4-88bbfaa36400)

- 우리는 임의의 단어 시퀀스가 해당 언어에서 얼마나 자연스러운지를 이해하고 있는 언어 모델을 구축하고자 함

- 조건부 확률의 정의에 따라 수식의 좌변, 우변이 같으므로 이전 단어(컨텍스트)들이 주어졌을 때 다음 단어를 맞히는 문제로도 목표를 달성할 수 있음
  
![image](https://github.com/user-attachments/assets/07224eb1-80ff-4d8d-87e0-007748268a56)

### 1-6. 언어 모델의 형태적 분류
- **순방향 언어 모델(Forward Language Model)**

  - 문장의 앞에서 뒤로, 사람이 이해하는 순서대로 계산하는 모델

  - ex) GPT(Generative Pretrained Transformer), ELMo 등

- **역방향 언어 모델(Backward Language Model)**

  - 문장의 뒤부터 앞으로 계산하는 모델
 
  - ex) ELMo(Embeddings from Language Models) 등 → ELMo는 순방향, 역방향 모두 사용함

### 1-7. 넓은 의미의 언어 모델
- 전통적인 의미의 언어 모델은 조건부확률의 정의를 따르는 수식으로 표현
- 최근에는 컨텍스트(주변 맥락 정보)가 전제된 상태에서 특정 단어(𝒘)가 나타날 조건부 확률로 표현하기도 함

   ![image](https://github.com/user-attachments/assets/afdf7885-78ba-4ff1-8963-4f6481625c06)

### 1-8. 잘 학습된 언어 모델
- 어떤 문장이 자연스러운지 가려낼 수 있으므로 그 자체로 가치가 있음

- 학습 대상 언어의 풍부한 맥락을 표현하고 있다. → 기계 번역, 문법 교정, 문장 생성 등 다양한 태스크 수행 가능

![image](https://github.com/user-attachments/assets/c2236c63-9f4b-4c38-96ca-9252ff41ecf6)

### 1-9. 언어 모델의 변화 추세
- 전통적인 언어처리 연구에서 딥러닝 패러다임으로 전환

- 시퀀스를 위한 합성곱 연산(학습속도 향상)

- 어텐션 모델(주의모델)의 주도

- 전이학습 활용

- 다양한 모델의 조합
  - ex) 단어의 문자를 인식하는 CNN 모델 + 시퀀스 처리를 위한 LSTM(RNN) + MLP를 이용한 LSTM의 출력 분류 등
<br><br/>


# 🤩 언어 모델 : Seq2Seq
## 1. 기계 번역
### 1-1. 번역
- 어떤 언어로 된 글을 다른 언어의 글로 옮기는 것

- 인류가 언어를 사용하기 시작한 이래로 계속되어온 큰 관심사의 하나

- 자연어 처리 분야에서의 종합예술이라고 칭할 정도로 다양한 기술이 통합됨

- 번역의 궁극적인 목표 : 어떤 언어 𝑓의 문장이 주어졌을 때, 가능한 𝑒 언어의 번역 문장 중에서 최대 확률을 가지는 𝑒Ƹ를 찾아내는 것

![image](https://github.com/user-attachments/assets/ef75d28e-8840-4c60-95ff-7ad5c4833c59)

### 1-2. 번역이 어려운 이유
- 인간의 언어(자연어)는 컴퓨터 프로그래밍 언어처럼 명확하지 않음(모호성)

- 자연어는 그 활용에 있어서의 효율을 극대화하는 쪽으로 흘러감

  - 우리는 정보나 단어를 생략하고, 문장을 짧게 만들며, 동일한 단어와 어절을 상황에 따라서 다른 의미로 사용(경제성, 효율성)
  - 특히 한국어는 어순이 불규칙하고, 주어가 생략되는 등(그래도 다 이해한다) 그 효율성의 추구가 극대화된 언어의 하나
 
- 언어는 문화를 내포하고 있으므로 수천 년간 쌓여온 사람의 의식, 철학 등이 녹아 들어가 있어서 그러한 문화의 차이가 번역을 더욱 어렵게 만듦

### 1-3. 규칙 기반 기계 번역 (Rule-Based Machine Translation, RBMT)
- 가장 전통적인 번역 방식
- 주어진 문장의 구조를 분석하고, 그 분석에 따라 규칙을 세운 후, 분류를 나누어 정해진 규칙에 따라 번역
- 사람의 경우는 일반화 능력이 뛰어나므로 몇 가지 규칙으로 번역을 수행할 수 있지만 컴퓨터에서는 매우 어려움
- 규칙이 잘 만들어진다면 통계 기반 기계 번역보다 자연스러운 표현이 가능하지만 규칙을 사람이 일일이 만들어야 함

### 1-4. 통계 기반 기계 번역 (Statistical Machine Translation, SMT)
- 신경망 기계 번역 이전에 세상을 지배하던 번역 방식
- 대량의 양방향 코퍼스에서 통계를 얻어내어 번역 시스템 구성
- 구글의 초기 번역 시스템에 채용되면서 유명해짐
- 많은 모델로 구성되므로 매우 복잡함
- 통계 기반 방식이므로 언어쌍의 확장 시, 대부분의 알고리즘, 시스템이 유지되므로 기존의 규칙 기반 기계번역에 비해 비용적으로 유리했음

### 1-5. 딥러닝 이전의 신경망 기계 번역 (Neural Machine Translation, NMT)
- 신경망 모델이 외면 받던 도중에도 신경망을 이용하여 기계 번역을 해결하려는 시도가 있었음
- 현재의 언어모델에 사용된 인코더-디코더(Encoder-Decoder) 형태를 가지고 있었으나 컴퓨터의 성능 부족, 데이터의 부족 등의 이유로 제대로 된 성능을 발휘하지 못함
  
![image](https://github.com/user-attachments/assets/c1cd00e6-e2d2-4c3b-b155-ffeecfbdf112)

### 1-6. 딥러닝 이후의 신경망 기계 번역
- 기존 통계 기반 기계 번역 방식을 순식간에 앞질러버림

- 구글 번역기를 포함하여 대부분의 상용 번역기가 딥러닝 기술로 대체됨

- 신경망 기반 기계번역의 장점

![image](https://github.com/user-attachments/assets/2fbb2fc5-544f-427a-a562-637c6882961a)

![image](https://github.com/user-attachments/assets/fcda6f5a-26d8-4656-9623-362fff5c936e)

## 2. Seq2Seq (Sequence to sequence) 모델
### 2-1. 개요
- 한 도메인(예, 영어 문장)에서 다른 도메인(예, 한국어 문장)으로 시퀀스(Sequence)를 변환하는 모델

- 시퀀스를 다루는 모델이므로 당연히 순차데이터를 대상으로 함. 주로 시계열 데이터에 대하여 많이 활용됨

- 대표적인 시퀀스 모델인 RNN 모델을 기반으로 하고 있음

- 2개의 RNN 모델을 이용하여 인코더와 디코더를 구현하였고, 이 때문에 Encoder-Decoder 모델이라고도 부름

- 시퀀스에 대한 예측을 목표로 하고 있으며 자연어 모델링, 품사 태깅, 개체명 인식 등에 많이 활용되고 있음
  - 자연어 모델링: 각 타임 스텝에서 주어진 시퀀스를 기반으로 다음 단어 예측
  - 품사 태깅: 단어의 문법 품사 예측
  - 개체명 인식: 단어가 사람, 위치, 제품, 회사 같은 개체명에 속하는지 예측

### 2-2. Seq2Seq 모델의 목적
- 모델 구조를 이용하여 MLE를 수행, 주어진 데이터를 가장 잘 설명하는 파라미터 𝜽를 찾는 것
  - MLE(Maximum Likelihood Estimation, 최대 우도법)
    - 주어진 데이터를 토대로 확률 변수의 모수를 구하는 방법
    - 모수가 주어졌을 때, 원하는 값들이 나올 가능도 함수를 최대로 만드는 모수를 선택하는 방법

- seq2seq 모델의 목적을 수식으로 표현

![image](https://github.com/user-attachments/assets/4948f0a7-7942-40df-8554-a5355f8d6b0f)

![image](https://github.com/user-attachments/assets/9065570a-a926-4346-8078-8a3892a4bfb9)
      
### 2-3. Seq2Seq 모델: 인코더
- 주어진 소스 문장인 여러 개의 벡터를 입력으로 받아 문장을 함축하는 문장 임베딩 벡터 생성 (𝑷(𝒛|𝑿)의 모델링)

- RNN 분류 모델과 거의 같음

- 𝑷(𝒛|𝑿)를 모델링하고, 주어진 문장을 매니폴드를 따라 차원 축소하여 해당 도메인의 잠재 공간(Latent Space)에 있는 어떤 하나의 점에 투영하는 작업

- 인코더는 문장을 하나의 벡터로 압축하여 표현함

![image](https://github.com/user-attachments/assets/3001e429-5624-46c9-bac0-f61e776d758d)

- 기존의 텍스트 분류 문제에서는 모든 정보(특징, Feature)가 필요하지는 않지만, 기계 번역을 위한 문장 임베딩 벡터에서는 최대한 많은 정보를 요구함

![image](https://github.com/user-attachments/assets/cd2cbec4-9679-45ba-a1b7-b9339cadf8d6)

- 인코더를 구성하는 계층

![image](https://github.com/user-attachments/assets/4e74efce-1d27-4042-9e3d-3e60b3869939)

- 인코더는 문장을 고정 길이 벡터로 인코딩함

![image](https://github.com/user-attachments/assets/378f1c71-329a-4339-9403-38c3f33f7d1b)
    
### 2-4. Seq2Seq 모델: 디코더
- 인코더와 반대의 역할을 수행

- 앞에서 살펴 본 seq2seq의 수식을 time-step에 관해 출어서 나타내면 조건부 확률 변수에 𝑿 가 추가됨

![image](https://github.com/user-attachments/assets/291f7db3-931d-4a26-a4c1-8c3a339e5343)

- seq2seq 식에서 조건부 확률에 𝑿가 추가된 것은 인코더의 결과인 문장 임베딩 벡터와 이전 time-step까지 번역하여 생성한 단어들에 기반하여 현재 time-step의 단어를 생성함을 의미

![image](https://github.com/user-attachments/assets/032f941a-0284-471d-bb9a-c2b0361b04c9)

- 디코더를 구성하는 계층

![image](https://github.com/user-attachments/assets/130e84cc-3810-4d94-b8ac-ccbfff430d83)

  
### 2-5. Seq2Seq 모델: 생성자
![image](https://github.com/user-attachments/assets/b89056bd-44eb-459d-b363-4d685f96de98)

### 2-6. 번역 수행 예시
![image](https://github.com/user-attachments/assets/3f60d632-8131-4909-b91b-47a9cdc62014)

### 2-7. 활용 분야
- 특정 도메인의 시계열 데이터 또는 시퀀스 데이터 입력

- 다른 도메인의 시계열 또는 시퀀스 데이터로 출력하는데 탁월한 능력을 보임

 ![image](https://github.com/user-attachments/assets/cda641b9-d787-434c-bd44-c492da437a58)

### 2-8. 한계점
- seq2seq는 오토인코더(AutoEncoder)의 일종으로 특히 시계열 또는 시퀀스 데이터에 강점이 있는 모델이라고 볼 수 있음

- 장기 기억력 문제

  - 신경망 모델은 차원축소를 통한 데이터 압축에 탁월한 성능을 보이지만 정보를 무한하게 압축할 수는 없음 → 압축 가능한 정보량의 한계 → 문장(또는 time-step)이 길어질수록 압축 성능 하락
 
  - LSTM, GRU 등을 사용하여 RNN보다는 높은 성능을 낼 수 있지만 역시 한계가 존재함

- 구조 정보의 부재

  - 현재의 딥러닝 자연어 처리 추세에서는 문장을 이해할 때 구조 정보를 사용하기보다는 단순히 시퀀스 데이터로 다루는 경향
  - 아직까지는 성공적으로 사용되지만 다음 단계로 나아가려면 구조 정보가 필요할 것으로 추정되고 있음
 
- 챗봇 또는 QA봇

  - 시퀀스 데이터 입력을 다른 도메인의 시계열/시퀀스 데이터로 출력하는데 뛰어남
  - 많은 사람이 seq2seq를 학습 시키면 더욱 뛰어난 성능을 보일 것으로 기대
  - 그러나 지속적인 데이터, 정보 추가가 필요하지 않은 번역, 요약과 달리 대화의 경우 지속적인 관리가 요구되지만 해당하는 구조적 기능이 부족함 → 발전된 구조 요구











