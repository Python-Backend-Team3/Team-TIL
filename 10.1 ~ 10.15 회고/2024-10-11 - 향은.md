## 2024-10-11 회고

### 언어모델이란?
- 문장의 확률을 나타내는 모델
- 문장은 단어들로 이루어진 시퀀셜 데이터
- 언어모델이란, 단어 시퀀스에 확률을 부여하는 모델
  -> 단어 시퀀스를 입력받아 해당 시퀀스가 얼마나 그럴듯한지 확률을 출력
- 한국어 말뭉치로 학습된 언어모델은 자연스러운 문장에 높은 확률값 부여

인간과 같은 능력을 가진 언어 모델을 만들기 위해서는 일상 생활에서 사용하는 실제 언어의 분포를 정확하게 근사하는 것이 중요하다.

### 언어 모델의 기술적 분류
- 확률에 기초한 통계적 언어 모델(Statistical Language Model, SLM)
- 신경망에 기초한 딥러닝 언어모델(Deep Neural Network Language Model, DNN LM)
- 주어진 단어를 바탕으로 다음 단어 혹은 단어들의 조합을 예측
- 이를 바탕으로 문장 생성, 기계 번역, 음성 인식, 문서 요약 등 자연어 처리 문제를 해결함


- 통계적 언어 모델, 단어열이 가지는 확률 분포를 기반으로 각 단어의 조합을 예측하는 전통적인 언어 모델


확률을 기반으로 단어의 조합을 예측하는 것 = 주어진 단어를 통해 다음 단어로 돌 확률이 가장 높은 단어를 예측하는 일련의 과정을 의미
- 스마트폰의 자동완성 기능
- 조건부 확률을 언어 현상에 적용

### 문장의 확률 표현
- 문장에서 i번째로 등장하는 단어를 w_i로 표시한다면
- n개의 단어로 구성된 문장이 해당 언어에서 등장할 확률 = 언어모델의 출력

![image](https://github.com/user-attachments/assets/87dfde99-bcf6-430d-bfd4-4c4c3eb88f14)

- n개의 단어가 동시에 나타날 결합 확률

### 예시
- 잘 학습된 한국어 모델이 있다면
- P(무모, 운전) < P(난폭, 운전)
- 난폭이 나타난 다음 운전이 나타날 확률 -> P(운전|난폭)=P(난폭,운전)/P(난폭) : 조건부확률

### 결합확률과 조건부 확률
- 3개의 단어가 동시에 등장할 결합 확률

![image](https://github.com/user-attachments/assets/b9a6dd19-e638-4f0e-8f61-6925b7f5cf8f)

- 다음의 3가지 사건이 동시에 발생해야 함
  - 첫 번째 단어 (w_1) 등장
  - 첫 번째 단어 등장 후 두 번째 단어 (w_2) 등장
  - 첫 번째 단어와 두 번째 단어 등장 후 세 번째 단어 (w_3) 등장
- 조건부 확률로 표현하면

![image](https://github.com/user-attachments/assets/f9beedd1-545f-4efc-b0cd-153be27363b3)

임의의 단어 시퀀스가 해당 언어에서 얼마나 자연스러운지를 이해하고 있는 언어 모델을 구축하고자 함

조건부 확률의 정의에 따라 수식의 좌변, 우변이 같으므로

이전 단어(context)들이 주어졌을 때, 다음 단어를 맞히는 문제로도 목표 달성 가능함

#### 순방향 언어 모델(Forward Language Model)
- 문장의 앞에서 뒤로, 사람이 이해하는 순서대로 계산하는 모델
- GPT(Generaltive Pretrained Transformer), ELMo 등

#### 역방향 언어 모델(Backward Language Model)
- 문장의 뒤부터 앞으로 계산하는 모델
- ELMo(Embeddings from Language Models) 등

#### 넓은 의미의 언어 모델
- 조건부확률의 정의를 따르는 수식으로 표현

![image](https://github.com/user-attachments/assets/bb8c5237-29e4-4a27-a723-09b8b956bc56)

- 컨텍스트(주변 맥락 정보)가 전제된 상태에서 특정단어 W가 나타날 조건부 확률

#### Seq2Seq 모델
- 번역
  - 어떤 언어로 된 글을 다른 언어의 글로 옮기는 것
  - 어떤 언어 f의 문장이 주어졌을 때, 가능한 e 언어의 번역 문장 중에서 최대 확률을 가지는 e^를 찾아내는 것

 ![image](https://github.com/user-attachments/assets/be31c44e-6158-4b75-bc74-796bf105b9ad)
- 번역이 어려운 이유
  - 인간의 언어(자연어)는 컴퓨터 프로그래밍 언어처럼 명확하지 않음(모호성)
  - 자연어를 효율을 극대화하는 쪽으로 흘러감
  - 언어는 문화를 내포하고 있음
  
#### 규칙 기반 기계 번역 (Rule-Based Machine Translation, RBMT)
- 가장 전통적
- 주어진 문장의 구조 분석 -> 규칙을 세움 -> 분류를 나누어 정해진 규칙에 따라 번역
- 규칙을 사람이 일일이 만들어야 하므로 자원과 시간 등 소요 비용 높음

#### 통계 기반 기계 번역 (Statistical Machine Translation, SMT)
- 신경망 기계 번역 이전에 주로 사용함
- 대량의 양방향 코퍼스에서 통계를 얻어내 번역 시스템 구성
- 구글의 초기 번역에 채용됨
- 많은 모델로 구성 -> 매우 복잡함
- 언어쌍의 확장 시, 대부분의 알고리즘과 시스템이 유지되어 기존의 규칙 기반에 비해 비용적으로 유리함

#### 딥러닝 이전의 신경망 기계 번역(Neural Machine Translation, NMT)
- 현재의 언어모델에 사용된 Encoder-Decoder 형태를 가지고 있었으나, 컴퓨터의 성능 부족, 데이터의 부족 등으로 성능을 발휘X

![image](https://github.com/user-attachments/assets/dda8a3c7-66c2-4e25-abcf-c940be09b9cf)

#### 딥러닝 이후의 신경망 기계 번역
- 기존 통계 기반의 방식을 앞질러버림
- 대부분의 상용 번역기가 딥러닝 기술로 대체됨

|항목|내용|
|--------------|----------------------------------------------------------------------|
|end-to-end모델|기존 통계기반 시스템은 수많은 모듈로 구성 -> 매우 복잡, 훈련이 어려움 딥러닝 기반 방식은 단 하나의 모델로 번역을 수행 -> 성능 극대화|
|더 나은 언어 모델|신경망 기반의 모델, 기존의 n-gram방식보다 강력, 희소성 문제 해결 및 자연스러운 번역 결과와 문장 생성 가능|
|훌륭한 문장 임베딩|신경망의 특징에 따라 문장의 차원축소, 임베딩 능력 뛰어남. 문장단위에서 발생하는 노이즈,희소성 문제에 대한 대처능력 뛰어남|

<br/>
<br/>
<br/>
<br/>
<br/>
<br/>

----------------------------------------------------------------------------------
### Seq2Seq(Sequence to sequence) 모델
- 한 도메인에서 다른 도메인으로 시퀀스를 변환하는 모델
- 순차 데이터를 대상으로 함. 주로 시계열 데이터를 활용
- RNN 모델이 대표적
- 자연어 모델링, 품사 태깅, 개체명 인식 등에 많이 활용됨

### 목적
- 모델 구조를 이용하여 MLE를 수행, 주어진 데이터르 ㄹ가장 잘 설명하는 파라미터 θ를 찾는 것
- MLE(Maximum Likelihood Estimation, 최대 우도법)
  - 주어진 데이터를 토대로 확률 변수의 모수를 구하는 방법
  - 모수가 주어졌을 때, 원하는 값들이 나올 가능도 함수를 최대로 만드는 모수를 선택하는 방법
    - 가능도함수(LIkelihood Function) : 모수 θ가 주어졌을 때 주어진 표본 x가 얻어질 확률
- 수식으로 나타내면

   ![image](https://github.com/user-attachments/assets/eaf4db24-9d67-49a0-aa16-6a5575d0d6f3)

P(Y|X;θ)를 최대로 하는 모델 파라미터를 찾는 작업

파라미터에 대한 학습이 완료되면 사후 확률을 최대로 하는 Y 도출

![image](https://github.com/user-attachments/assets/f5d6071d-5e06-4d5c-96be-e4f0fcde5e5f)

<br/>
<br/>

### Seq2Seq 모델의 구성

![image](https://github.com/user-attachments/assets/619aeafc-d9d9-4b74-8e3f-1a0967793465)

<br/>
<br/>
<br/>

### 인코더
- 주어진 소스 문장인 여러 개의 벡터를 입력으로 받아 문장을 함축하는 문장 임베딩 벡터 생성의 모델링
- RNN 분류 모델과 거의 같음
- P(z|X)를 모델링 -> 주어진 문장을 매니폴드를 따라 차원 축소 -> 해당 도메인의 잠재 공간에 있는 어떤 하나의 점에 투영하는 작업

![image](https://github.com/user-attachments/assets/0db1312d-7fbe-4325-a3ad-c7906ec16eb5)

- 기존의 텍스트 분류 문제에서는 모든 정보가 필요하지 X
- 그러나 기계 번역을 위한 문장 임베딩 벡터에서는 최대한 많은 정보를 요구
- 인코더를 수식으로

  ![image](https://github.com/user-attachments/assets/3e69d732-7246-48b4-9b38-1ca77de8f8ad)

- 인코더를 실제 구현할 떄는 전체 time-step을 병렬로 한번에 처리함

  ![image](https://github.com/user-attachments/assets/2da3942f-8331-4b01-9f96-e4061966d44d)

### 디코더
- 인코더와 반대의 역할을 수행

  ![image](https://github.com/user-attachments/assets/5dbfc158-708e-4af3-a270-0ec000627be0)

- 조건부 확률에 X가 추가됨
  - 인코더의 결과인 문장 임베딩 벡터와 이전 time-step까지 번역하여 생성한 단어들에 기반하여 현재 time-step의 단어를 생성함을 의미

![image](https://github.com/user-attachments/assets/31c946d1-2621-478b-8bc0-8b3929991151)

### 생성자
- 디코더에서 각 time-step별로 결과 벡터를 받아 softmax를 계산하여 각 타겟 언어의 단어별 확률 값을 반환하는 작업 수행
- 생정자의 결과 값은 각 단어가 나타난 확률인 이산 확률 분포가 됨
- 문장의 길이가 |Y|=m일때, 맨 마지막에 반환되는 단어 Y_m은 EOS토큰이 됨
- EOS 토큰 : 디코더 계산의 종료를 나타냄

![image](https://github.com/user-attachments/assets/f897531a-e523-40cf-8009-bc9c7329d1d4)
<br/>
<br/>
### Encoder와 Decoder가 번역을 수행하는 예

![image](https://github.com/user-attachments/assets/84fd6076-23a5-4aec-b796-06f80293f2a8)

- 인코딩(부호화) : 정보를 어떤 규칙에 따라 변환하는 것
- 디코딩(복호화) : 인코딩된 정보를 원래의 정보로 되돌리는 것

<br/>
<br/>

### Encoder를 구성하는 계층

![image](https://github.com/user-attachments/assets/9de5a901-d9f7-40e5-b8c2-933001edea43)

- RNN을 이용해서 시계열 데이터를 h라는 은닉상태 벡터로 변환
- 출력벡터 h는 LSTM계층의 마지막 은닉상태
  -> 입력 문장을 번역하는데 필요한 정보가 인코딩 됨
- h는 고정길이 벡터 -> 인코딩 작업 = 임의 길이의 문장을 고정길이벡터로 변환하는 작업

### Decoder를 구성하는 계층

![image](https://github.com/user-attachments/assets/f62ebbfc-c158-4c14-b340-ffceb2b59731)

<br/>
<br/>
<br/>

### Seq2Seq 모델의 한계점
- 오토인코더의 일종으로 특히 시계열 또는 시퀀스 데이터에 강점이 있는 모델임
- 그러나 장기 기억력 문제가 있음
  - 신경망 모델은 차원축소를 통한 데이터 압축에 탁월한 성능을 보임
  - 정보를 무한하게 압축할 수는 X -> 압축 가능한 정보량의 한계
  - 문장(or time-step)이 길어질수록 압축 성능 하락
- 구조 정보의 부재
  - 딥러닝 자연어 처리 추세에서는 문장을 이해할 때 구조 정보를 사용하기 보다 단순히 시퀀스 데이터로 다루는 경향이 있음
  - 다음 단계로 나아가려면 구조 정보가 필요할 것으로 추정됨
- 챗봇 or QA봇
  - 지속적인 데이터, 정보 추가가 필요하지 않은 번역, 요약과 달리 대화의 경우 지속적인 관리가 요구되지만 해당하는 구조적 기능이 부족함


<br/>

----------------------------------------------------------------------
### Attention 모델
- 딥러닝 초창기 기계번역 기술은 주로 Sequence 방식
- 그러나 정보 손실 발생 문제가 존재 (고정된 길이의 벡터에 모든 단어에 대한 정보가 축약되어 있음)
- 따라서 매 step마다 인코더의 입력 시퀀스를 다시 참고하는 것으로..
- 즉, 단어를 하나 번역하면 그 단어를 넘기는 방식

![image](https://github.com/user-attachments/assets/13c3d392-b07e-47a0-aece-b81105549f81)

<br/>

### Attention Function

![image](https://github.com/user-attachments/assets/33847cad-ae0e-4220-b7ee-020082dd8b7e)

Attention(Q,K,V)=Attentionvalue

- Q:Query
- K:Key
- V:Value

어텐션 함수는 주어진 query에 대해 모든 key의 유사도를 각각 구함

이 유사도를 키와 매핑되어 있는 각각의 value에 반영해줌

유사도가 반영된 값(value)을 모두 더해서 리턴하고 어텐션 값을 반환

<br/>

### Dot-Product Attention
- Seq2Seq에 Attention기법을 적용한 예시의 기본 형태
- Q: Query - t 시점의 decoder cell에서의 은닉 상태
- K: Key - 모든 시점의 encoder cell에서의 은닉 상태
- V: Value - 모든 시점의 encoder cell에서의 은닉 상태

![image](https://github.com/user-attachments/assets/13073024-053f-40c9-b5c8-0f0b897a507d)

위 그림은 Decoder의 세번째 LSM Cell에서 출력 단어를 예측할 때 어텐션 메커니즘을 사용하는 예시

세 번째 단어를 예측할 떄, 예측 단어와 encoder의 모든 시퀀스('i','am','a','student')의 관계를 파악

파악하는 방식은 softmax를 이용

왼쪽 부분처럼 세번째 cell에서 출력단어를 예측할 때 softmax를 통과시킨 input sequence를 추가적으로 전달

decoder가 새로운 단어를 예측하는 데 있어서 Recurrent하게 전달된 정보 외에도 input sequence의 정보를 참고할 수 있는 path를 마련하게 됨

![image](https://github.com/user-attachments/assets/3354e8df-2a48-4358-8494-0ec98b87a950)

- h_t : t 시점에서 encoder의 hidden state (예시에서 4차원)
- s_t : t 시점에서 decoder의 hidden state (예시에서 4차원)

만약 Attention 기법을 사용하지 않는다면 decoder는 time step t에서 새로운 단어를 예측하기 위해 이전 스텝의 hidden state인 

S_t-1와 이전 스텝의 아웃풋 총 2개를 필요로 함

<br/>
<br/>

### Attention value를 구하기 위한 과정
1. Attention Score를 구한다

![image](https://github.com/user-attachments/assets/87c9c441-0e7c-4589-ad7b-3e5055464416)

2. 소프트맥스 함수를 통해 어텐션 분포 계산

![image](https://github.com/user-attachments/assets/03d6134e-afec-4948-b230-cdb11f60e1cb)

3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값 계산

![image](https://github.com/user-attachments/assets/204c314a-fbb0-4ff6-80fd-ac95089ec83e)

4. 어텐션 값과 디코더의 t시점의 은닉 상태를 연결

![image](https://github.com/user-attachments/assets/f16e1b3a-92ba-43c7-91c0-9453b78c0465)

5. 출력층 연산의 입력이 되는 S..를 계산

![image](https://github.com/user-attachments/assets/36407a8a-c0b0-4d52-92c7-b73509ce5b3e)

![image](https://github.com/user-attachments/assets/f34429e9-edd6-4da8-9747-efee3b428067)

6. S..를 출력층의 입력으로 사용하여 예측 벡터 획득

![image](https://github.com/user-attachments/assets/4ed7083e-abc1-4adf-800f-bc20b010e3f4)

<br/>
<br/>
<br/>
<br/>
<br/>

----------------------------------------------------------
### 바다나우 어텐션 함수(Bahdanau Attention FUnction)
Attention(Q,K,V) = Attention Value
- t = 어텐션 메커니즘이 수행되는 디코더 셀의 현재 시점을 의미

- Q = Query : t-1 시점의 디코더 셀에서의 은닉 상태
- K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
- V = Values : 모든 시점의 인코더 셀의 은닉 상태들

### 처리 과정
1. 어텐션 스코어 계산

![image](https://github.com/user-attachments/assets/2eb02789-e768-403c-9156-98f9c1413dbd)

인코더의 시점을 각각 1,2,...,N이라고 할 때, 인코더의 은닉 상태를 각각 h1,h2,..,hn라고 하고

디코더의 현재 시점(time step) t에서 디코더의 은닉 상태를 st라고 하고

인코더의 은닉 상태와 디코더의 은닉 상태의 차원이 같다고 가정

![image](https://github.com/user-attachments/assets/b3f43ea1-6a55-4889-9d3a-06ff4f6c3895)

![image](https://github.com/user-attachments/assets/3c5c1e88-c665-467f-bdb6-e648f0797248)

2. 소프트맥스 함수를 통해 어텐션 분포 계산

![image](https://github.com/user-attachments/assets/96274cac-eecf-4288-92e2-ad2f81d89e5d)

3. 각 인코더의 어텐션 가중치와 은닉 상태를 가중합하여 어텐션 값 계산

![image](https://github.com/user-attachments/assets/1d888326-6eec-46c6-9186-0d10383ac197)

4.컨텍스트 벡터로부터 St 계산

![image](https://github.com/user-attachments/assets/a7489570-3278-407b-afd0-7e800b433366)

