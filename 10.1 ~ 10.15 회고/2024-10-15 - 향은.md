## 2024-10-15

### 사전 학습
- 자비어(Xavier)방법 등을 통해 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법
- 예시
  - 텍스트 유사도 예측 모델을 만들기 전, 감정 분석 문제를 학습한 모델의 가중치를 활용-> 텍스트 유사도 모델의 가중치로 활용함

### 하위문제
- 사전 학습한 가중치를 활용해 학습하고자 하는 본 문제
- 앞 예시에서는
  - 상위 문제(=사전 학습 문제) : 사전 학습한 모델인 감정 분석 문제
  - 하위 문제 : 사전 학습된 가중치를 활용해 본격적으로 학습하고자 하는 문제인 텍스트 유사도 문제
 
### 사전 학습이 필요한 이유
- 잘못된 데이터로 학습 -> 모델의 성능을 떨어뜨림
- 잘못된 데이터를 모두 제거한 후 남은 데이터로 학습 -> 하지만 잘못된 데이터의 비율이 95% 라면?
- 학습을 진행하기 매우 부족한 데이터

### 사전 학습 모델 이용 X 학습하는 과정
1. 데이터 셋 분석 및 전처리
2. 모델 설계, 구현 및 생성
3. 모델 학습

### 사전 학습 모델 이용 O 학습 과정
1. 적절한 데이터 셋에 대해 잘 학습된 사전 학습 모델 확보
2. 모델 학습
   - 가중치 초기화 등의 과정 수행 X
   - 기존에 학습이 완료된 가중치를 초기값으로 사용
   - 해결하고자 하는 문제에 대한 유사도 데이터를 활용하여 학습 수행
   - 사전 학습 모델의 최종 출력 값을 뽑는 가중치 층은 제외 -> 우리가 해결하고자 하는 문제와 관계 X 데이터가 포함되어있으므로

![image](https://github.com/user-attachments/assets/5c2aab75-4e1d-45b4-a39e-9a8a5fd30c7d)

![image](https://github.com/user-attachments/assets/9c910c7f-ce94-465c-aaed-b13ed7198a0e)

#### 왜 언어 모델을 많이 활용하는가?
- 사전 학습은 어떤 문제에도 적용 가능
- 왜 대부분의 자연어 처리 관련 연구는 언어 모델을 활용하는가?
  - 언어 처리 문제는 Label이 주어지지 X 비지도 학습 문제에 포함됨
    -> 데이터에 제약이 X, 언어에 대한 전반적 이해를 사전 학습하는 언어 모델이 효율적임
  - 하위 문제 모델의 성능 향상시킴
  - 언어 모델은 대규모의 데이터를 활용한 사전 학습을 통해 언어에 대한 전반적 이해를 하게됨
    -> 이렇게 학습된 지식을 기반으로 하위 문제에 대한 성능 향상시킴

### 사전 학습한 가중치를 활용하는 방법
- 특징 기반 방법
  - 사전 학습된 특징을 하위 문제의 모델에 부가적인 특징으로 활용하는 방법
  - word2vec : 학습한 임베딩 특징을 우리가 학습하고자 하는 모델의 임베딩 특징으로 활용
- 미세 조정
  - 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가 -> 모델을 추가로 학습하는 방법
  - 대표 사례 : 감정 분석 문제에 사전 학습 시킨 가중치와 더불어 텍스트 유사도를 위한 부가적인 가중치를 추가하여 텍스트 유사도 문제를 학습하는 것
 
- 트랜스포머 모델 이후
  - 대부분의 자연어 처리 연구에서는 트랜스포머 모델을 기반으로 하는 비지도 사전 학습을 통해 많은 가중치들을 활용하여 다양한 자연어 처리 모델을 미세 조정하는 방법이 각광받고 있음
  - GPT(Generative Pre-trained Transformer)
  - BERT(Bidirectional Transformers for Language Understanding)

### GPT (Generative Pre-trained Transformer)
#### GPR-1
- 매우 큰 자연어 처리 데이터를 활용 -> 비지도 학습으로 사전 학습 수행 -> 학습된 가중치를 활용 -> 해결하고자 하는 문제에 미세조정을 적용하는 방식의 언어 모델
- 모델의 기반 구조는 트랜스포머 모델임
- 트랜스포머의 디코더 구조만 사용
- 순방향 마스크 어텐션 적용

![image](https://github.com/user-attachments/assets/6d896871-27e4-4b29-a567-46e3ba70998b)

- 하나의 사전 학습 방식 사용
- 앞의 단어들을 활용하여 해당 단어를 예측하는 방식
- 예시 : "나는 학교에 간다" 라는 문장을 활용하여 총 3번 학습 진행

![image](https://github.com/user-attachments/assets/f17d186b-35a9-4e76-9c52-0a18fa728295)

- 실제 문제를 대상으로 학습을 진행할 때도 언어 모델을 함께 학습
  - GPT-1 : 본 학습 시에도 실제로 학습해야 하는 문제의 손실값과 더불어 언어 모델의 손실 값 또한 학습함
  - BERT : 사전 학습에서만 언어 모델의 손실 값을 사용해서 학습함
 

#### 대부분의 딥러닝 task는
- 수동으로 Labeling된 많은 양의 데이터가 필요 -> 수동 작업이 뒤따라야 하므로 풍부하지 X
- 그러나 이런 데이터가 부족한 분야에는 적용 가능성이 제한됨
- 따라서 Unlabeled 데이터를 사용하기 위한 방안 필요

- Unlabeled Data
  - 시간 소모나 가격측면에서 좋은 대안 제공
  - 성능 상승에 효과적인 표현방식을 배울 수 있음
  - 풍부함
- Unlabeled Text 활용의 어려움
  - 단어 수준 이상의 정보를 얻기 쉽지 않음
  - 어떤 데이터가 용이한지, 불분명함
  - 학습된 효과를 target task 에 효과적으로 전달하는 일관성 있는 방안, 구조가 없음
  - 이러한 불확실성이 Semi-Supervised Learning(반지도학습) 접근법을 어렵게 만듦

#### GPT-1의 제안
- 접근법 : 비지도 사전 학습과 지도학습 미세조정을 결합한 반지도학습적 접근
- 목표 : 넓은 범위에서의 작업에 약간의 조정만으로 전이할 수 있는 범용 표현의 학습
- 2단계 구조를 적용
  - Unlabeled Data 상에서 동작하는 언어 모델 개체를 사용하여 사전학습을 수행 ->
  - 지도학습에 해당하는 Target Task를 대상으로 미세조정을 적용함
- 언어 모델 개체에는 트랜스포머 구조를 적용
  - 텍스트의 장기 의존성에 강한 결과를 보여줌
  - 기존 RNN 등에 비해 구조화된 메모리를 사용할 수 있게 함
-> **지도학습 모델의 일반화 성능 개선, 학습의 수렴을 빠르게 해주는 장점**

#### 기존 트랜스포머 모델의 단점
- 테스크가 가진 특정 아키텍처의 목표에 따라 학습 수행
- 이는 상당한 양의 태스크 특정 커스터마이징이 필요, 전이 학습을 어렵게 함
-> 사전 학습 모델이 잘 동작할 수 있도록 입력 구조를 변환시키는 방법을 적용

![image](https://github.com/user-attachments/assets/c47646bd-f47d-4af0-aaee-f2709b538d25)

GPT-1도입 결과 -> 여러 부문에서 최고의 성능을 보임

![image](https://github.com/user-attachments/assets/b04dcdea-25c8-46f7-86e4-b14cdc3dd3e4)

![image](https://github.com/user-attachments/assets/ad5cde83-72b7-460c-8953-1fc47cf3db05)

![image](https://github.com/user-attachments/assets/325e2480-e6ec-4fed-a8d5-97541891ba9a)

### GPT-2
- GPT-1의 성능을 향상시킨 모델
#### 모델 구조
- 거의 동일하지만
- 기존의 디코더에서 각 레이어 직후의 잔차 연결(Residual Connection)과 함께 적용되던 층 정규화가 각 부분 블록의 입력 쪽으로 이동함
- 마지막 셀프 어텐션 레이어 이후에 층 정규화가 적용됨

#### 모델의 크기
- GPT-1 : 총 12개의 레이어, 총 117만개의 가중치
- GPT-2 : 총 48개의 레이어, 총 1,542만개의 가중치

#### 모델의 입력
- GPT-1 : 텍스트를 특정 단위로 나눠서 모델의 입력으로 사용
- GPT-2 : BEF(Byte Pair Encoding) 방식을 사용해 텍스트를 나눠서 모델의 입력으로 사용
- 글자와 문자 사이의 적절한 단위를 나누어 줌으로써 높은 성능 발휘

#### GPT-2의 제안
- 기존에는 pre-training과 supervised fine-tuning의 결합으로 만들어졌던 두 개의 작업을 연결
- 기존에 ㅅ용해 오던 전이 방법은 계속 유지
- 지도 학습이 없는 상태로 만들어지면 일반 상식 추론 등 범용적으로 사용할 수 있을 것
- 언어 모델
  - 매개변수, 모델 아키텍처 수정없이 사용할 수 있는 zero-shot방식 적용
  - 모델이 바로 하위 작업에 적용됨
  - zero-shot 설정으로 인하여 범용성 있는 언어 모델 능력 향상이 가능해짐

![image](https://github.com/user-attachments/assets/975ef94d-a113-48a0-861e-9879e9f57c22)

#### 강점
- Byte 수준에서 동작
- 손실이 있는 전처리 또는 토큰화 불필요
- 모든 언어 모델에 대한 벤치마크 평가 가능

### GPT-3
- 총 1,750억 개의 가중치 보유
- 기존의 사전 학습-미세 조정 방법론의 구조적 한계 지적, 새로운 방법론 제시
  - 기존의 학습 방법은 학습된 모델이 특정 문제에 국한된다는 한계점이 존재
  - 각 하위 문제를 해결하기 위해 미세 조정 과정이 필요
  - 미세 조정을 위한 하위 문제의 데이터셋이 추가적으로 필요함
  - 일반적으로 이러한 데이터셋의 경우 각 문제에 대해 수십만 개의 데이터를 요구

#### 메타 학습 방법론
- 사전 학습 과정에서 학습된 다양한 언어적인 능력 및 패턴을 인식하는 능력만을 활용해서 새로운 문제에 적용하는 방법
- 방대한 데이터로 가중치를 사전 학습하고, 학습된 모델의 능력을 활용해서 특정 문제에 적용 및 예측하는 방법
- 기존 방법론 : 사전학습 + 미세 조정 후 새로운 문제에 특정된 가중치로 업데이트
- 메타 학습 방법론 : 사전 학습 과정에서 학습된 정보만을 활용하여 문제 해결

#### GPT-3의 한계점
- 낮은 효율성
  - 1,750억개의 매개변수 -> 인간이 평생 보는 정보보다 많은 데이터를 학습해야 함
- 현실세계의 물리적 상식 부족
- 모든 분야에서 뛰어난 것은 아님
- 학습에 사용된 예제를 외운 것인지 실제 추론한 것인지 구분하기 어려움
- 새로운 정보를 수용하기 어려움(기억력이 없음)
- 방대한 양의 텍스트를 통해 다음 단어를 예측하는 방식으로 학습됨

<br/>

### BERT(Bidirectional Encoder Reoresentation Transformers)
- 2018 구글이 발표한 자연어 처리를 위한 딥러닝 모델
- 비지도 사전학습을 한 모델에 추가로 하나의 완전 연결 계층만 추가한 후 미세 조정을 통해 총 11개의 자연어 처리 문제에서 최고의 성능을 보여줌
- 영상인식 계열에 비해 발전이 늦은 언어처리 딥러닝의 한계를 돌파하는 계기가 될 것으로 기대받음
- 양방향성의 사전 학습 모델
  - 기존 모델인 GPT,ELMo 등고 달리 양방향성을 가진 사전 학습 모델,
  - 이로 인해 기존 모델보다 뛰어난 성능 보유
  
![image](https://github.com/user-attachments/assets/1ab311e2-c6fb-4c26-9e59-a3dcd7aa3f82)

- 문장의 각 단어를 문장의 다른 모든 단어와 연결 -> 그 관련성 인지 -> 임베딩 수행

![image](https://github.com/user-attachments/assets/d597e65e-e66a-4468-b39a-4f29a252fd26)

- 트랜스포머 모델을 기반, 인코더만 사용
- BERT에 입력된 문장의 각 단어 표현 출력

![image](https://github.com/user-attachments/assets/cd9d26c1-b5f3-4652-b3e7-e44398bd5138)

### BERT의 구조
- BERT-base, BERT-large의 두 가지 구성의 모델 제시

![image](https://github.com/user-attachments/assets/38832cc2-97f5-4e08-b334-4715bd179143)

### BERT의 사전 학습
- 입력 표현과 MLM,NSP 의 두 가지 태스크를 이용하여 사전 학습 수행

### 입력 표현
- 다음 세 가지의 임베딩의 합으로 구성
  - 토큰 임베딩
  - 세그먼트 임베딩
  - 위치 임베딩

- 토크나이저 : WordPiece Tokenizer
  - 하위 단어 토큰화 알고리즘을 기반으로 함

    ![image](https://github.com/user-attachments/assets/36e46b3d-f7f8-43e9-ad36-f1ea9b2ba362)

  - 토근화할 때, 단어가 어휘사전에 있는지 확인 -> 있으면 그대로 토큰으로 사용, 없으면 Subword로 분할하고 다시 확인
 
#### 토큰 임베딩

![image](https://github.com/user-attachments/assets/cbf29c88-68ef-4988-89d1-d407527aea4d)

<br/>

#### 세그먼트 임베딩

![image](https://github.com/user-attachments/assets/bb5ec499-9739-48ed-a74d-16bbd48fc17d)

#### 위치 임베딩
- 트랜스포머 모델은 어떤 반복 메커니즘 사용X 모든 단어를 병렬 처리
  -> 단어 순서와 관련된 정보 제공이 필수 -> 위치 인코딩 사용
- BERT는 트랜스포머의 인코더이므로
- 데이터를 직접 입력하기 전 문장에서 토큰의 위치에 대한 정보 제공 요구
  -> 위치 임베딩 레이어를 사용해 각 토큰에 대한 위치 임베딩 출력을 확보

  ![image](https://github.com/user-attachments/assets/bc30bc0e-02d5-4c18-aab6-7f113f87b2d9)

<br/>

#### 최종 입력 데이터 표현
![image](https://github.com/user-attachments/assets/e1efa4c3-d867-40ee-aa55-fde31964a2e8)

### BERT는 두 가지 태스크에 대해 사전 학습 수행
- 마스크 언어 모델링(Masked Language Modeling, MLM)
- 다음 문장 예측(Next Sentence Prediction, NSP)
