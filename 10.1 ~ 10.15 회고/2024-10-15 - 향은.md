## 2024-10-15

### 사전 학습
- 자비어(Xavier)방법 등을 통해 임의의 값으로 초기화하던 모델의 가중치들을 다른 문제에 학습시킨 가중치들로 초기화하는 방법
- 예시
  - 텍스트 유사도 예측 모델을 만들기 전, 감정 분석 문제를 학습한 모델의 가중치를 활용-> 텍스트 유사도 모델의 가중치로 활용함

### 하위문제
- 사전 학습한 가중치를 활용해 학습하고자 하는 본 문제
- 앞 예시에서는
  - 상위 문제(=사전 학습 문제) : 사전 학습한 모델인 감정 분석 문제
  - 하위 문제 : 사전 학습된 가중치를 활용해 본격적으로 학습하고자 하는 문제인 텍스트 유사도 문제
 
### 사전 학습이 필요한 이유
- 잘못된 데이터로 학습 -> 모델의 성능을 떨어뜨림
- 잘못된 데이터를 모두 제거한 후 남은 데이터로 학습 -> 하지만 잘못된 데이터의 비율이 95% 라면?
- 학습을 진행하기 매우 부족한 데이터

### 사전 학습 모델 이용 X 학습하는 과정
1. 데이터 셋 분석 및 전처리
2. 모델 설계, 구현 및 생성
3. 모델 학습

### 사전 학습 모델 이용 O 학습 과정
1. 적절한 데이터 셋에 대해 잘 학습된 사전 학습 모델 확보
2. 모델 학습
   - 가중치 초기화 등의 과정 수행 X
   - 기존에 학습이 완료된 가중치를 초기값으로 사용
   - 해결하고자 하는 문제에 대한 유사도 데이터를 활용하여 학습 수행
   - 사전 학습 모델의 최종 출력 값을 뽑는 가중치 층은 제외 -> 우리가 해결하고자 하는 문제와 관계 X 데이터가 포함되어있으므로

![image](https://github.com/user-attachments/assets/5c2aab75-4e1d-45b4-a39e-9a8a5fd30c7d)

![image](https://github.com/user-attachments/assets/9c910c7f-ce94-465c-aaed-b13ed7198a0e)

#### 왜 언어 모델을 많이 활용하는가?
- 사전 학습은 어떤 문제에도 적용 가능
- 왜 대부분의 자연어 처리 관련 연구는 언어 모델을 활용하는가?
  - 언어 처리 문제는 Label이 주어지지 X 비지도 학습 문제에 포함됨
    -> 데이터에 제약이 X, 언어에 대한 전반적 이해를 사전 학습하는 언어 모델이 효율적임
  - 하위 문제 모델의 성능 향상시킴
  - 언어 모델은 대규모의 데이터를 활용한 사전 학습을 통해 언어에 대한 전반적 이해를 하게됨
    -> 이렇게 학습된 지식을 기반으로 하위 문제에 대한 성능 향상시킴

### 사전 학습한 가중치를 활용하는 방법
- 특징 기반 방법
  - 사전 학습된 특징을 하위 문제의 모델에 부가적인 특징으로 활용하는 방법
  - word2vec : 학습한 임베딩 특징을 우리가 학습하고자 하는 모델의 임베딩 특징으로 활용
- 미세 조정
  - 사전 학습한 모든 가중치와 더불어 하위 문제를 위한 최소한의 가중치를 추가 -> 모델을 추가로 학습하는 방법
  - 대표 사례 : 감정 분석 문제에 사전 학습 시킨 가중치와 더불어 텍스트 유사도를 위한 부가적인 가중치를 추가하여 텍스트 유사도 문제를 학습하는 것
 
- 트랜스포머 모델 이후
  - 대부분의 자연어 처리 연구에서는 트랜스포머 모델을 기반으로 하는 비지도 사전 학습을 통해 많은 가중치들을 활용하여 다양한 자연어 처리 모델을 미세 조정하는 방법이 각광받고 있음
  - GPT(Generative Pre-trained Transformer)
  - BERT(Bidirectional Transformers for Language Understanding)

### GPT (Generative Pre-trained Transformer)
#### GPR-1
- 매우 큰 자연어 처리 데이터를 활용 -> 비지도 학습으로 사전 학습 수행 -> 학습된 가중치를 활용 -> 해결하고자 하는 문제에 미세조정을 적용하는 방식의 언어 모델
- 모델의 기반 구조는 트랜스포머 모델임
- 트랜스포머의 디코더 구조만 사용
- 순방향 마스크 어텐션 적용

![image](https://github.com/user-attachments/assets/6d896871-27e4-4b29-a567-46e3ba70998b)

- 하나의 사전 학습 방식 사용
- 앞의 단어들을 활용하여 해당 단어를 예측하는 방식
- 예시 : "나는 학교에 간다" 라는 문장을 활용하여 총 3번 학습 진행

![image](https://github.com/user-attachments/assets/f17d186b-35a9-4e76-9c52-0a18fa728295)

- 실제 문제를 대상으로 학습을 진행할 때도 언어 모델을 함께 학습
  - GPT-1 : 본 학습 시에도 실제로 학습해야 하는 문제의 손실값과 더불어 언어 모델의 손실 값 또한 학습함
  - BERT : 사전 학습에서만 언어 모델의 손실 값을 사용해서 학습함
 

#### 대부분의 딥러닝 task는
- 수동으로 Labeling된 많은 양의 데이터가 필요 -> 수동 작업이 뒤따라야 하므로 풍부하지 X
- 그러나 이런 데이터가 부족한 분야에는 적용 가능성이 제한됨
- 따라서 Unlabeled 데이터를 사용하기 위한 방안 필요

- Unlabeled Data
  - 시간 소모나 가격측면에서 좋은 대안 제공
  - 성능 상승에 효과적인 표현방식을 배울 수 있음
  - 풍부함
- Unlabeled Text 활용의 어려움
  - 단어 수준 이상의 정보를 얻기 쉽지 않음
  - 어떤 데이터가 용이한지, 불분명함
  - 학습된 효과를 target task 에 효과적으로 전달하는 일관성 있는 방안, 구조가 없음
  - 이러한 불확실성이 Semi-Supervised Learning(반지도학습) 접근법을 어렵게 만듦

#### GPT-1의 제안
- 접근법 : 비지도 사전 학습과 지도학습 미세조정을 결합한 반지도학습적 접근
- 목표 : 넓은 범위에서의 작업에 약간의 조정만으로 전이할 수 있는 범용 표현의 학습
- 2단계 구조를 적용
  - Unlabeled Data 상에서 동작하는 언어 모델 개체를 사용하여 사전학습을 수행 ->
  - 지도학습에 해당하는 Target Task를 대상으로 미세조정을 적용함
- 언어 모델 개체에는 트랜스포머 구조를 적용
  - 텍스트의 장기 의존성에 강한 결과를 보여줌
  - 기존 RNN 등에 비해 구조화된 메모리를 사용할 수 있게 함
-> **지도학습 모델의 일반화 성능 개선, 학습의 수렴을 빠르게 해주는 장점**

#### 기존 트랜스포머 모델의 단점
- 테스크가 가진 특정 아키텍처의 목표에 따라 학습 수행
- 이는 상당한 양의 태스크 특정 커스터마이징이 필요, 전이 학습을 어렵게 함
-> 사전 학습 모델이 잘 동작할 수 있도록 입력 구조를 변환시키는 방법을 적용

![image](https://github.com/user-attachments/assets/c47646bd-f47d-4af0-aaee-f2709b538d25)

GPT-1도입 결과 -> 여러 부문에서 최고의 성능을 보임

![image](https://github.com/user-attachments/assets/b04dcdea-25c8-46f7-86e4-b14cdc3dd3e4)

![image](https://github.com/user-attachments/assets/ad5cde83-72b7-460c-8953-1fc47cf3db05)

![image](https://github.com/user-attachments/assets/325e2480-e6ec-4fed-a8d5-97541891ba9a)

### GPT-2
- GPT-1의 성능을 향상시킨 모델
#### 모델 구조
- 거의 동일하지만
- 기존의 디코더에서 각 레이어 직후의 잔차 연결(Residual Connection)과 함께 적용되던 층 정규화가 각 부분 블록의 입력 쪽으로 이동함
- 마지막 셀프 어텐션 레이어 이후에 층 정규화가 적용됨

#### 모델의 크기
- GPT-1 : 총 12개의 레이어, 총 117만개의 가중치
- GPT-2 : 총 48개의 레이어, 총 1,542만개의 가중치

#### 모델의 입력
- GPT-1 : 텍스트를 특정 단위로 나눠서 모델의 입력으로 사용
- GPT-2 : BEF(Byte Pair Encoding) 방식을 사용해 텍스트를 나눠서 모델의 입력으로 사용
- 글자와 문자 사이의 적절한 단위를 나누어 줌으로써 높은 성능 발휘

#### GPT-2의 제안
- 기존에는 pre-training과 supervised fine-tuning의 결합으로 만들어졌던 두 개의 작업을 연결
- 기존에 ㅅ용해 오던 전이 방법은 계속 유지
- 지도 학습이 없는 상태로 만들어지면 일반 상식 추론 등 범용적으로 사용할 수 있을 것
- 언어 모델
  - 매개변수, 모델 아키텍처 수정없이 사용할 수 있는 zero-shot방식 적용
  - 모델이 바로 하위 작업에 적용됨
  - zero-shot 설정으로 인하여 범용성 있는 언어 모델 능력 향상이 가능해짐

![image](https://github.com/user-attachments/assets/975ef94d-a113-48a0-861e-9879e9f57c22)

#### 강점
- Byte 수준에서 동작
- 손실이 있는 전처리 또는 토큰화 불필요
- 모든 언어 모델에 대한 벤치마크 평가 가능
