## 2024-10-04 회고
### 회고 목표: YOLO모델에 대해 정리

## YOLO(You Only Look Once)
### 특징
- 이미지 전체를 한번만 봄
  - R-CNN계열의 방식처럼 이미지를 여러 장 분할 -> 여러 번 분석 X ,원본 이미지 그대로를 CNN에 통과시킴
- region proposal, feature extraction, classification, bbox regression ->  one-stage detection로 통합하여 빠름
  - 이전 모델에 비해 상대적으로 빠르고 실시간 객체 탐지 가능
- 주변 정보까지 학습하여 이미지 전체를 처리 -> background error가 적음
- 훈련 단계에서 보지 못한 새로운 이미지에 대해서도 검출 정확도가 높음

Yolo가 유명해진 이유는 높은 성능은 아니더라도 실시간으로  object detection이 가능하고 

기존의  Faster R-CNN 보다 6배 빠른 성능을 보였기 때문.

하지만 R-CNN계열 모델들에 비해 낮은 정확도를 가지고 있으며 특히 작은 객체에 대해 정확도가 아쉬울 정도로 낮은 편



### YOLO의 동작과정

![image](https://github.com/user-attachments/assets/f9064838-aa34-4cf3-a205-b83f1ce2214c)

1. 사진이 입력되면 가로,세로를 동일한 그리드 영역으로 나눔
2. 각 그리드 영역에 대해 어디에 사물이 존재하는지 바운딩박스와 박스에 대한 신뢰도 점수를 예측함
3. 신뢰도가 높을수록 굵게 박스를 그려줌
4. 그와 동시에 (2)에서와 같이 어떤 사물인지 classification작업을 진행함
5. 굵은 박스들만 남기고 얇은 것, 즉 사물이 있을 확률이 낮은 것들은 지워줌
6. 최종 경계박스들은 NMS(Non- Maximum Suppression)알고리즘을 이용해 선별함

### 동작 과정 예시

![image](https://github.com/user-attachments/assets/f2b9ab8b-6dfa-4c95-beaa-5165ab81e5b6)

이미지를 4X4 그리드로 나눈다고 가정, 각 그리드 셀마다 예측하는 바운딩 박스는 2개, 전체 고려하는 class개수는 20개로 가정

첫 번째 예측된 바운딩 박스는 파란색

이때 총 5개의 output이 나옴
- 예측된 바운딩 박스의 정 중앙 좌표인 x,y좌표
- 바운딩 박스의 너비와 높이를 전체 이미지로 나눠 노멀라이즈한 w,h(0~1)
- 박스 내부에 물체가 존재할 확률 confidence score인 pc
  - pc는 pr*IOU해준 값, 해당 바운딩 박스 내부에 물체가 있으면 1, 없으면 0

### IOU(intersection Over Union)
![image](https://github.com/user-attachments/assets/1b982a2c-42b3-424e-a25f-12e2355df87b)

IOU는 바운딩 박스의 위치 정확도에 대한 평가 도구,

예측된 바운딩 박스와 실제 물체의 바운딩 박스 두 개의 교집합을 총 너비로 나눈 값

수치가 클수록 정확도가 높음

![image](https://github.com/user-attachments/assets/ae1a8598-a648-469e-8f38-3cb2d7f3e705)

위에서 설명한 과정을 한 번 더 하여 두 번째 바운딩 박스에 대한 5가지 output에 대해 1차원 텐서로 넣어줌

마지막으로 하늘색 박스를 보면 그리드 셀에 있는 오브젝트가 어떤 클래스인지 확률이 들어가게 됨

이로써 하나의 그리드 셀에 댛나 아웃풋이 나오게 되고, 이 과정을 모든 그리드 셀(16개)에 대해 적용시킴

![image](https://github.com/user-attachments/assets/0acda812-bf3a-47ff-a63c-9543ea92293a)

이와 같이 4x4x30의 아웃풋 텐서가 나옴

(4x4x30인 이유: 4x4그리드 셀 마다 5개의 값을 예측하는 2개의 바운딩 박스와(5x2) 

거기에 예시를 들었던 20개의 클래스일 각각의 확률이 20개가 들어감으로(5x2+20) 총 30개가 나옴)

### YOLO의 Network Design
![image](https://github.com/user-attachments/assets/625d4efb-a76a-46cc-8e85-616a70961444)

googleNet의 Inception블록 대신 단순한 컨볼루션으로 네트워크를 구성했다고 함

총 24개의 conv layer와 2개의 fc layer로 구성됨

앞의 20개의 conv layer에 대해서는 1000개의 class의 imageNet 데이터 셋으로 pretrained된 상태,

뒤의 4개의 conv layer와 2개의 fc layer를 더 붙여서 Pascal VOC 데이터로 Fine tuning을 시킨 과정을 거침

그리고 노란색으로 표시된 부분에 1x1 reduction layer로 연산량을 감소시킨 특징도 있음

### Training Stage
![image](https://github.com/user-attachments/assets/f6732b23-2a4a-4a62-9aa6-69888e78fa52)

특정 오브젝트에 대해 responsibel한 cell을 찾아야 하는데, 이는 Ground truth 박스의 중심 좌표가 위치하는 셀로 할당하게 됨

위 그림에서, 하늘색 그리드 셀이 강아지를 찾는데 responsible한 셀이 되는 것

(강아지의 Ground truth 박스의 빨간 점이 위치하므로)

이때 하늘색 셀의 영역에서 두 개의 노란색과 남색 박스를 예측했다고 가정,

(conv layer를 거쳐 나온 예측된 박스를 의미)

여기서 하나의 바운딩 박스를 고르는 기준은 IOU값

Ground truth와 겹치는 IOU값이 더 높은 노란색 바운딩 박스만이 학습에 참여하는 것

### Loss function
training 단계에서 사용하는 loss는 SSE(Sum of Squared Error)를 사용함

손실 함수는 예측값과 정답 레이블 간의 오차를 측정하기 위해 사용됨
이 오차를 최소화 하는것이 중요

![image](https://github.com/user-attachments/assets/ba954bb0-4683-47a1-88f4-1fea5dfd8feb)

- Regression loss : 모든 그리드 셀에서 예측한 B개의 바운딩 박스의 좌표(x, y, width, height)와 GT box 좌표 의 오차를 구하는 공식
- Confidence loss : 모든 그리드 셀에서 예측한 B개의 클래스에 속할 확률값과 GT값의 오차를 구하는 공식
- Classification loss : 셀 내에 클래스가 존재할 확률을 구하는 공식
- i는 cell의 index, j는 bounding box predictor index

### B-box regression Loss
![image](https://github.com/user-attachments/assets/c551524a-57d6-46b8-b1be-bbb172406b43)

- S^2: feature map의 grid 수
- B : Anchor Box의 수

![image](https://github.com/user-attachments/assets/ac2f9601-29ec-4df3-85b9-bbb0092402bc)

responsible이나 아니냐를 나타내는 것,

i 번째 cell 안에 2개의 detector 중에서 j 번째 detector가 
responsible이면 1이고, not responsible이면 0

![image](https://github.com/user-attachments/assets/5c17f59a-90e6-4fb5-9136-b8b403e39904)

스케일링 적용한 것, 밸런식 파라미터

### Object Confidence Loss
![image](https://github.com/user-attachments/assets/1126e4da-07bb-434d-a092-fc9abedcc0c1)

예측한 object의 loss를 나타냄

- Ci : 모델에서 구한 object일 확률 값과 IOU를 곱한 값
- Ci hat : Ground truth confidence score, 값은 1 

![image](https://github.com/user-attachments/assets/96664a8a-4846-4d2b-992e-8c6cff59e717)

object가 없어야 하는 확률에 대해서도 b-box의 confidence loss를 구해서 포함 함 시킴

![image](https://github.com/user-attachments/assets/f21f061b-126b-4867-8b9c-b8aab8bc37ce)

noobj loss 에는 람다 가중치를 곱해줌, 보통 값은 0.5

object 객체가 탐지되지 않은 영역에 대한 loss를 0.5를 곱해주서 낮춰 주는 이유는 클래스 불균형 문제를 해소하기 위함 

대부분의 b box는 오브젝트를 포함하고 않을 것이기 때문에 배경에 대해서도 가중을 똑같이 1로 주게 된다면 모델은 배경 학습에 더 집중할 수 있기 때문임

### Classification Loss
![image](https://github.com/user-attachments/assets/5d955afa-39a7-40bf-b9f6-0c91a9fb4d82)

classification loss는 말 그대로 클래스별 확률 값에 대한 오차 제곱으로 구해 줌

pi(c): 해당 cell i 내부에 존재하는 object가 c class 일 확률임

### Inference stage
![image](https://github.com/user-attachments/assets/2ab1d397-d261-47f4-8b6e-649758ba620a)

어떤 사물인지 추론하는 단계,

아웃풋 텐서의 하나의 그리드를 확대해보면

총 크기 30의 아웃풋이 들어가 있음

여기서 class specific confidence score이라는 것을 계산,

첫 번째 바운딩 박스의 confidence score와 각 클래스일 확률을 곱해줌

![image](https://github.com/user-attachments/assets/f15107c9-0bb9-4d6c-836c-7eda7b8d8c60)

이 과정을 예측한 바운딩 박스의 개수만큼 32번 (16 셀 개수 x 2개의 바운딩 박스) 해줌

그런데 하나의 오브젝트마다 바운딩 박스 개수가 많아지게 되므로 NMS 알고리즘을 사용

### NMS(Non-Maximum Suppression)
![image](https://github.com/user-attachments/assets/d89f7ade-7e4a-4114-8dae-0dab92dd63ed)

NMS란 object detection이 예측한 바운딩 박스들 중에서 가장 정확한 하나의 박스만 선택하도록 하는 알고리즘

각 cell마다 confidence score이 가장 큰 박스를 고르고

선택한 박스와의 IOU가 임계값보다 큰 박스는 모두 제거

하나의 박스만 남을 때까지 이 과정을 반복 실행
