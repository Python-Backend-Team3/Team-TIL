# 📝 2024.10.14 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------
# Transformer 아키텍처

### 트랜스포머란?
- 2017 구글이 제안한 Sequence-to-Sequence 모델의 하나
- Attention is all you need라는 논문에서 제안된 모델
- 기존의 seq2seq 구조인 Encoder-Decoder 방식을 따르면서도 논문 제목처럼 Attention 만으로 구현한 모델임
- RNN 모델을 사용하지 않고 Encoder-Decoder를 설계 했음에도 번역 등 성능에서 더 우수한 성능 보임

#### 기존 seq2seq 모델 한계
- 기존의 seq2seq 모델은 인코더-디코더 구조로 구성
- 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 통해서 출력 시퀀스를 만들어 냄 → 이러한 구조는
  - 인코더가 입력 시퀀스를 하나의 벡터(고정길이)로 압축하는 과정에서
  - 입력 시퀀스의 정보가 일부 손실된다는 단점이 발생하였고
  - 이를 보정하기 위해 어텐션(Attention) 모델이 사용됨

### 트랜스포머의 등장
- RNN에서 사용한 순환방식을 사용하지 않고 순수하게 어텐션만 사용한 모델(중요하다 싶은 것만 사용)
- 기존에 사용되었던 RNN, LSTM, GRU 등은 점차 트랜스포머로 대체되기 시작
- GPT, BERT, T5 등과 같은 다양한 자연어 처리 모델에 트랜스포머 아키텍처가 적용됨


#### 트랜스포머의 인코더-디코더 구조
- 인코더
  - 소스 시퀀스의 정보를 압축해 디코더로 보냄
- 디코더
  - 인코더가 보내 준 소스 시퀀스의 정보를 받아서 타겟 시퀀스 생성

![image](https://github.com/user-attachments/assets/fc3e1233-0529-4bec-823a-777b49a19b91)

- 트랜스포머는 Sequence to Sequence 형태의 과제 수행에 특화된 모델.
- 임의의 시퀀스를 해당 시퀀스와 속성이 다른 시퀀스로 변환하는 작업이라면 꼭 기계 번역 이 아니라도 수행할 수 있음
- 예: 필리핀 앞바다의 한 달 치 기온 데이터 → 앞으로 1주일간 하루 단위로 태풍이 발생할지를 맞히는 과제(기온의 시퀀스 → 태풍발생 여부의 시퀀스)도 트랜스포머가 할 수 있는일임

#### 트랜스포머의 학습 방법
- 1. I를 예측하는 학습
- 인코더 입력: 어제, 카페, 갔었어, 거기, 사람, 많더라 → (소스 시퀀스 전체)
  - 소스 시퀀스를 압축하여 디코더로 보내고
- 디코더 입력:
  - 인코더에서 보내온 정보와 현재 디코더 입력을 모두 고려하여 토큰(I)를 예측
- 디코더의 최종 출력:
  - 타깃 언어의 어휘 수만큼의 차원으로 구성된 벡터 → 이 벡터는 요소의 값이 모두 확률 값 (예: 타깃 언어의 어휘가 총 3만개라면 디코더 출력은 3만 차원의 벡터, 3만개 각각은 확률값)

![image](https://github.com/user-attachments/assets/c1623be0-7d72-4cbc-89ba-10513164a65b)

- 2. 트랜스포머의 학습 진행
- 인코더와 디코더의 입력이 주어졌을 때, 정답에 해당하는 단어의 확률값을 높이는 방식으로 학습함
- → 그림에서...
  - 모델은 이번 시점의 정답인 I에 해당하는 확률은 높이고, 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신

![image](https://github.com/user-attachments/assets/b1830c27-b66c-4205-b58a-b2bbcf1173a7)

- 3. went 를 예측할 차례 (y target으로 went가 들어감)
- 인코더 입력: 어제, 카페, 갔었어, 거기, 사람, 많더라 → (소스 시퀀스 전체)
- 디코더 입력:
   - 특이사항
     - 학습 중의 디코더 입력과 학습을 마친 후 모델을 실제 기계번역에 사용할 때(인퍼런스)의 디코더 입력이 다름
     - 학습 중: 디코더 입력에 예측해야 할 단어(went) 이전의 정답 타깃 시퀀스(I)를 넣어줌
     - 학습 후(인퍼런스 때): 현재 디코더 입력에 직전 디코딩 결과를 사용 (예: 인퍼런스 때 직전 디코더 출력이 I 대신 you가 나오면 다음 디코더 입력은 you)

![image](https://github.com/user-attachments/assets/41ad70b7-288f-4bda-9906-66bec070c025)

- 4. ‘went’확률 높이기
- 학습 과정 중 인코더, 디코더의 입력이 아래 그림과 같으면...
- 모델은 이번 시점의 정답인 went에 해당하는 확률은 높이고
- 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신함

![image](https://github.com/user-attachments/assets/ecaab281-b8f8-45d3-8d75-79f17cff53c8)

- 5. ‘to’ 예측하기
- 인코더 입력은 소스 시퀀스 전체
- 디코더 입력은 정답인 I went
- 인퍼런스 할 때, 디코더 입력은 직전 디코딩 결과
- 이번 시점의 정답인 to에 해당하는 확률은 높이고
- 나머지 단어의 확률은 낮아지도록 모델 전체를 갱신함

![image](https://github.com/user-attachments/assets/a68347fe-10b8-472d-b734-5accb410c1ed)
- 이상의 방식으로 말뭉치 전체를 반복해서 학습

![image](https://github.com/user-attachments/assets/049b50e7-5b00-44d1-bc26-354bcad51796)


### 트랜스포머 블록

- 트랜스포머는 블록 형태로 구성된 인코더, 디코더 수십개가 반복적으로 쌓여 있는 형태
- 이런 구조를 블록 또는 레이어라고 부름
- 인코더 블록: 3가지 요소로 구성
  - 멀티 헤드 어텐션, 피드포워드 뉴럴 네트워크, 잔차 연결 & 레이어 정규화
- 디코더 블록: 인코더 블록이 변형된 형태
  - 마스크 멀티 헤드 어텐션 추가됨(정답을 입력할때 넣는 것이므로 사실상 형태가 동일하다 볼 수 있다)

![image](https://github.com/user-attachments/assets/4a05560a-f40b-4a94-8b5a-246dedf65ec7)

### 셀프 어텐션
- 트랜스포머 구조에서 “멀티 헤드 어텐션”을 “셀프 어텐션“ 이라고 부름

- 어텐션
  - 시퀀스 입력에 수행하는 기계학습 방법의 일종
  - 시퀀스 요소 가운데 중요한 요소에 집중하고, 그렇지 않은 요소는 무시함으로써 태스크 수행 성능을 끌어올림
  - 디코딩 시 소스언어의 단어 시퀀스 중에서 디코딩에 도움이 되는 단어 위주로 취사 선택(가장 중요한 것만 추려서), 번역 품질을 끌어올림
- 셀프 어텐션은 자신에게 수행하는 어텐션 기법 이다.
- 트랜스포머 경쟁력의 원천으로 평가된다.

#### 셀프 어텐션과 합성곱 신경망(CNN)을 비교하면...
- CNN: 합성곱 필터를 이용해 시퀀스의 지역적 특징을 잡아낼 수 있다
  - 자연어는 기본적으로 시퀀스
  - 특정 단어 기준으로 한 주변 문맥이 의미 형성에 중요한 역할을 수행
    - 자연어 처리에 CNN이 사용되기 시작
  - 합성곱 필터 크기를 넘어서는 문맥은 읽어 내기 어렵다
    - 필터 크기가 3이면 4칸 이상 떨어져 있는 단어 사이의 의미는 잡아내기 어려움(이와 같은 문제로 셀프 어텐션이 사용됨)

![image](https://github.com/user-attachments/assets/56a8f4d4-7b37-4df2-98d8-c07cb956a2fd)

#### 셀프 어텐션과 어텐션을 비교

![image](https://github.com/user-attachments/assets/40f3aee4-8494-49b3-9498-f3b06f6cb02f)

- cafe에 대응하는 소스 언어의 단어: 카페 → 소스 시퀀스의 초반부에 등장
- cafe라는 단어를 디코딩 할 때, 카페를 반드시 참조
- 어텐션이 없는 단순 RNN이라면 워낙 초반에 입력된 단어라 잊었을 가능성이 크고, 이 때문에 번역 품질이 낮아 질 수 있다.

#### 셀프 어텐션과 어텐션의 주요 차이
- 어텐션은 소스 시퀀스 전체 단어들과 타깃 시퀀스 단어 하나 사이를 연결하는데 사용. 셀프 어텐션은 입력 시퀀스 전체 단어들 사이를 연결

![image](https://github.com/user-attachments/assets/dfc71ffd-ebe9-48d4-a8ba-e8dfbc0b41dd)

- 어텐션은 RNN 구조 위에서 동작하지만 셀프 어텐션은 RNN 없이 동작함
- 타깃 언어의 단어를 1개 생성할 때 어텐션은 1회 수행하지만, 셀프 어텐션은 인코더, 디코더 블록의 개수만큼 반복 수행

#### 셀프 어텐션 계산 예시
- 쿼리(Q), 키(K), 값(V)이 서로 영향을 주고 받으면서 문장의 의미 계산
- 각 단어 벡터는 블록 내에서 계산과정을 통해 Q, K, V로 변환
- 쿼리 단어 각각을 대상으로 모든 키 단어와 얼마나 유기적인 관계를 맺는지를 총합이 1인 확률 값으로 표현
- “카페”와 가장 관련이 높은 단어는 “거기(0.4)”

![image](https://github.com/user-attachments/assets/88df3fa4-8b1b-45c6-9d67-fe8d4530d283)

### 하이퍼 파라미터
- 머신러닝에서 모델링 시에 사용자가 직접 설정해 주는 값
- 학습률 등 다양한 종류가 있음
- 하이퍼 파라미터의 정의 및 특성
  - 모델 하이퍼파라미터는 모델 외부에 있고 데이터에서 값을 추정할 수 없는 구성
    - 모델 매개 변수를 추정하는 데 도움이 되는 프로세스에 자주 사용
    - 관련 전문가에 의해 지정
    - 휴리스틱(전문가 경험)을 사용해 설정할 수 있다
    - 주어진 예측 모델링 문제에 맞게 조정됨

- 하이퍼 파라미터와 파라미터를 혼용해 사용하고 있는데 둘은 엄연히 다른 개념으로, 파라미터는 모델 내부에서 결정되는 매개변수 이고, 데이터로 부터 결정된다. 반면 하이퍼 파라미터는 모델 외부에서 사용자가 직접 지정하는 값이다.

#### 트랜스포머의 주요 하이퍼 파라미터

![image](https://github.com/user-attachments/assets/6d788272-4250-4ec1-a540-95118577cca8)

### 트랜스포머의 전체 구조

- 인코더-디코더 구조
  - seq2seq: 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조
  - 트랜스포머: 인코더와 디코더라는 단위가 N개로 구성되는 구조

![image](https://github.com/user-attachments/assets/482fde75-debb-4cdc-87d8-809f01e7a094)

- 트랜스포머를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개 사용

![image](https://github.com/user-attachments/assets/83bfebc8-461e-4680-a58d-56801e50444b)

- 트랜스포머의 동작 형태
  - 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 구조
  - 디코더는 seq2seq 구조처럼 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행 → RNN은 사용되지 않지만 인코더-디코더의 구조는 유지되고 있다.

![image](https://github.com/user-attachments/assets/e4e09790-d3fe-41fc-917f-b7dbe124246a)

### 포지셔널 인코딩

-트랜스포머의 입력
- RNN이 자연어 처리에서 유용했던 이유
  - 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보 (position information)를 가질 수 있었기 때문

- 트랜스포머의 경우
  - 단어 입력을 순차적으로 받지 않음 -> 단어의 위치 정보를 알려줄 방법이 필요함
  - 포지셔널 인코딩 적용
    - 단어의 위치 정보를 얻기 위해
    - 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용

#### 포지셔널 인코딩
- 입력되는 임베딩 벡터들은 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 추가됨

![image](https://github.com/user-attachments/assets/5bbba8a1-671a-47d1-9e97-37445da0caae)

- 임베딩 벡터에 포지셔널 인코딩값이 더해지는 과정

![image](https://github.com/user-attachments/assets/a875fbee-863c-4e0a-b3ab-c26abe8128cf)

- 포지셔널 인코딩 값의 계산
  - 트랜스포머는 위치 정보를 가진 값을 만들기 위해서 아래의 두 개의 함수를 사용

![image](https://github.com/user-attachments/assets/058461e8-04fb-4222-8ff8-f5448fcbfbd5)

- (𝑑𝑚𝑜𝑑𝑒𝑙은 트랜스포머에서는 512)
- 사인, 코사인 함수의 그래프에서 값의 형태를 생각해볼 수 있다. 트랜스포머는 사인 함수와 코사인 함수의 값을 임베딩 벡터에 더해줌으로써 단어의 순서 정보를 추가한다.

![image](https://github.com/user-attachments/assets/3090be0d-254c-45db-92cc-733a3c609320)

- 𝑝𝑜𝑠 : 입력 문장에서의 임베딩 벡터의 위치
- 𝑖 : 임베딩 벡터 내의 차원의 인덱스
- 𝑑𝑚𝑜𝑑𝑒𝑙 : 트랜스포머의 모든 층의 출력 차원을 의미하는 트랜 스포머의 하이퍼 파라미터

- 짝수(pos, 2i) 인 경우에는 사인 함수의 값을 사용
- 홀수(pos, 2i+1) 인 경우에는 코사인 함수의 값을 사용

- 임베딩 벡터도 𝑑𝑚𝑜𝑑𝑒𝑙 의 차원을 가짐
- 위의 그림은 4차원의 예시를 들고 있으나 논문에서는 512차원을 사용함

- 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존됨
  - 각 임베딩 벡터에 포지셔널 인코딩의 값을 더하면 같은 단어라고 하더라도 문장 내의 위치에 따라서 트랜스포머의 입력으 로 들어가는 임베딩 벡터의 값이 달라짐
  - 이에 따라 트랜스포머의 입력은 순서 정보가 고려된 임베딩 벡터가 됨


### 어텐션
- 트랜스포머에서 사용되는 어텐션
- Encoder Self-Attention
- Masked Decoder Self-Attention
- Encoder-Decoder Attention

![image](https://github.com/user-attachments/assets/79bf4a3f-e991-4cb0-b5dc-2908bfd458f4)

![image](https://github.com/user-attachments/assets/00d7366a-d9f3-4db9-9460-10da4447b636)

### 인코더
- 인코더의 구조
- 트랜스 포머는 하이퍼 파라미터인 num_layers(6)개의 인코더 층이 쌓여서 구성됨
- 하나의 인코더 층은 크게 총 2개의 서브층으로 구분
  - 셀프 어텐션 (= 멀티 헤드 셀프 어텐션)
  - 피드 포워드 신경망 (= 포지션 와이즈 피드 포워드 신경망)

![image](https://github.com/user-attachments/assets/c7596d04-cb08-447a-a102-37e5d3dcc012)

### 셀프 어텐션

- 기존 어텐션 개념
  - 어텐션 함수는 주어진 '쿼리(Query)'에 대해서 모든 '키(Key)'와의 유사도를 각각 계산
  - 계산된 유사도를 가중치로 하여 키와 맵핑되어있는 각각의 '값(Value)'에 반영
  - 유사도가 반영된 '값(Value)'을 모두 가중합하여 리턴

![image](https://github.com/user-attachments/assets/1890b526-fe21-477f-aad8-8bb419d418e5)

- 셀프 어텐션(self-attention)
  - 어텐션을 자기 자신에게 수행한다는 의미
  - 어텐션을 사용할 경우의 Q, K, V의 정의

![image](https://github.com/user-attachments/assets/49a8177d-3075-41a6-aa4c-784c436e50cf)

- t 시점: 계속 변화하면서 반복적으로 쿼리를 수행함을 표현
- → 전체 시점에 대해서 일반화한다면

![image](https://github.com/user-attachments/assets/948ae6d9-c222-486f-8446-9b25c11801a5)

- 기존 어텐션: 디코더 셀의 은닉 상태가 Q, 인코더 셀의 은닉 상태가 K → Q와 K가 서로 다른 값
- 셀프 어텐션: Q, K, V가 전부 동일

![image](https://github.com/user-attachments/assets/9cb12c44-b18e-4dda-9deb-8a9965e16e1b)

- 셀프 어텐션을 통해 얻을 수 있는 대표적인 효과

![image](https://github.com/user-attachments/assets/82d3e37f-521d-4a9a-820c-e0283b2362f9)

- Q, K, V 벡터 얻기
- 셀프 어텐션은 입력 문장의 단어 벡터들을 가지고 수행한다
→ 인코더의 초기 입력인 𝑑𝑚𝑜𝑑𝑒𝑙 의 차원을 가지는 단어 벡터들을 사용하여 셀프 어텐션을 수행하는 것이 아니라 각 단어 벡터들로부터 Q벡터, K벡터, V벡터를 얻는 작업 수행

- Q, K, V들은 초기 입력인 𝑑𝑚𝑜𝑑𝑒𝑙 의 차원을 가지는 단어 벡터들보다 더 작은 차원을 가짐
- 작은 차원의 크기는 하이퍼파라미터인 num_heads로 인해 결정

![image](https://github.com/user-attachments/assets/ce38c74a-739e-4c0e-a862-02b767ce3b71)

- 예문 중 student라는 단어 벡터를 Q, K, V의 벡터로 변환하는 과정

![image](https://github.com/user-attachments/assets/28684b39-626f-453a-b083-93ff927573ab)

![image](https://github.com/user-attachments/assets/59b255c3-c91a-4625-a034-4d5935ecf528)

![image](https://github.com/user-attachments/assets/c521100b-9c5f-48ed-b3e9-86c27a389775)

![image](https://github.com/user-attachments/assets/781f9da2-a9a3-41e2-a22f-12c2150e85e0)

- 굳이 각 Q벡터마다 따로 연산할 필요가 있을까? → 행렬 연산으로 일괄 처리하기

![image](https://github.com/user-attachments/assets/d39378f0-5496-40cb-aca9-e26298c23cc9)

![image](https://github.com/user-attachments/assets/e15892a8-7463-4eaa-9669-751cdd5aa5bf)

- 행렬연산에 사용된 행렬의 크기를 정리하면

![image](https://github.com/user-attachments/assets/71bbadff-ca9e-458c-8ba2-41c1877ff998)

- 멀티 헤드 어텐션
  - 앞서 배운 어텐션에서는 𝑑𝑚𝑜𝑑𝑒𝑙 의 차원을 가진 단어 벡터를 → num_heads로 나눈 차원을 가지는 Q, K, V벡터 로 바꾸고 어텐션을 수행
  - 왜 𝑑𝑚𝑜𝑑𝑒𝑙 의 차원을 가진 단어 벡터를 가지고 어텐션을 하지 않고 차원을 축소시킨 벡터로 어텐션을 수행하였는가?

![image](https://github.com/user-attachments/assets/d308b270-18fe-42e9-80ae-1fb96acc6682)

















































```
