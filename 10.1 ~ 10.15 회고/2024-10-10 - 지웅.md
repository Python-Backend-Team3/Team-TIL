# 📝 2024.10.10 회고 📝
#### 1. 수업 내용 복습정리
#### 2. 백준

---------------------------------

# CBOW 임베딩
- CBOW 모델은 활성화 함수를 사용하지 않는 간단한 구성의 신경망 모델이다.
- 다중 분류 작업으로, 단어 텍스트를 스캔하여 단어의 문맥 윈도우를 만든 후, 문맥 윈도우에서 중앙의 단어를 제거하고, 문맥 윈도우를 사용해 누락된 단어를 예측한다. 이를 통해서 단어의 의미를 벡터 공간에 효율적으로 표현할 수 있게 된다.

### CBOW 모델의 특징
- 단순한 신경망 모델: CBOW 모델은 입력층과 출력층 사이에 하나의 은닉층만을 가지고 있으며, 활성화 함수 없이 선형 계산을 통해 단어 임베딩을 학습. 이는 매우 간단한 구조의 신경망으로 볼 수 있다.

- 다중 분류 문제로 구성: CBOW 모델은 주변 단어들을 입력으로 받아, 중심 단어를 예측하는 다중 분류 작업을 수행. 예를 들어, 문장 "The quick brown fox"가 주어졌을 때, "quick"과 "brown"이 주변 단어라면, "fox"를 예측하는 방식

- 문맥 윈도우 사용: CBOW 모델은 주어진 텍스트를 스캔하여 문맥 윈도우(예: 2개의 왼쪽 단어, 2개의 오른쪽 단어)로 단어의 문맥을 구성. 이 윈도우에서 중심 단어를 제거하고, 남은 문맥 단어들을 기반으로 중심 단어를 예측

- 누락된 단어 예측: CBOW 모델의 주된 역할은 문맥에서 누락된 단어를 예측하는 것이다. 예를 들어, "The quick ___ fox"에서, "___"에 들어갈 단어가 "brown"임을 예측한다.

### CBOW 모델 구현

1. 입력 데이터의 벡터화:

- 텍스트 데이터를 정수형 ID로 변환하고, 단어 ID를 임베딩 벡터로 변환하여 신경망의 입력으로 사용
- 이 과정에서 nn.Embedding 레이어가 자주 사용되며, nn.Embedding은 주어진 토큰의 정수 ID를 해당 임베딩 벡터로 매핑해주는 역할을 한다.

2. CBOW 모델의 학습:

- 모델은 주변 단어를 입력으로 받아 중심 단어를 예측한다.
- 중심 단어의 정답 레이블을 기반으로 손실(Loss)을 계산하고, 옵티마이저를 사용하여 손실을 최소화하는 방향으로 모델을 학습 시킨다.
- 옵티마이저는 모델 가중치뿐만 아니라 nn.Embedding 레이어의 임베딩 벡터도 업데이트하여, 학습이 진행될수록 단어 벡터들이 의미적으로 정렬되도록 한다.

3. 모델의 평가 및 추론:

- 학습이 완료된 모델은 새로운 문맥을 입력으로 받아 중심 단어를 예측한다.
- 모델의 성능은 정확도, 손실 등을 기준으로 평가할 수 있으며, 학습된 임베딩 벡터를 시각화하여 단어 간의 유사성을 분석할 수도 있다.


#### 실습
- 실습에서는 PyTorch를 사용하여 CBOW 모델을 구현하고, 모델을 학습 및 평가하는 과정을 진행했다.

##### 1. 임베딩 행렬 구현 (nn.Embedding 사용)

- nn.Embedding은 정수형 토큰 ID를 해당 임베딩 벡터로 매핑해주는 역할을 한다.
- 예를 들어, 단어 "fox"의 정수 ID가 7이라면, nn.Embedding은 이 ID를 길이 50의 벡터로 변환하여 신경망의 입력으로 사용할 수 있는 것이다.
- 이 과정에서 임베딩 벡터는 초기에는 무작위 값으로 설정되지만, 학습이 진행됨에 따라 손실을 최소화하는 방향으로 업데이트 되어진다.

##### 2. 옵티마이저를 통한 모델 학습

- 옵티마이저는 손실을 최소화하기 위해 모델의 가중치와 임베딩 벡터를 업데이트한다.
- 일반적으로 SGD, Adam 등의 옵티마이저가 사용되며, 학습률, 모멘텀 등의 하이퍼파라미터를 조정하여 모델의 학습 속도와 성능을 최적화할 수 있다.

##### 3. 프랑켄슈타인 (Frankenstein) 데이터셋 사용

- 실습에서는 문학작품 **프랑켄슈타인(Frankenstein)**을 데이터셋으로 사용하여, 단어 임베딩을 학습한다.
- 작품 내 단어들을 분석하고, 단어 간의 의미적 관계를 학습하는 데 사용되어진다.

##### 4. 토큰에서 벡터의 미니배치를 만드는 벡터화 파이프라인

- 대규모 텍스트 데이터를 처리할 때는, 데이터를 배치(batch) 단위로 나누어 학습하는 것이 일반적이다.
- 이를 위해 미니배치 형태로 벡터화하여 입력 데이터셋을 구성하고, 학습 과정에서 효율적으로 데이터를 공급한다.

##### 5. CBOW 분류모델 및 Embedding 층 사용

- CBOW 모델은 nn.Embedding 층을 통해 입력 데이터를 벡터화하고, 이 벡터를 입력으로 받아 중심 단어를 예측하는 분류 모델을 구성하며
- 출력층은 소프트맥스(Softmax) 함수를 사용하여 중심 단어의 확률 분포를 계산한다.

##### 6. 모델 학습 후 모델 평가 및 추론

- 학습이 완료된 모델은 새로운 문맥을 입력으로 받아 중심 단어를 예측하고, 이를 통해 모델의 성능을 평가한다.
- 또한, 학습된 임베딩 벡터를 시각화하여 단어 간의 유사성을 확인하고, 임베딩 벡터의 의미적 관계를 분석할 수 있다.


## 오늘 학습한 내용의 파일이 담긴 깃허브 주소
- https://github.com/Astero0803/Machine-Learning-Frameworks-/blob/main/pytorch/2024_10_10_word_embedding.ipynb
```
임베딩 행렬을 캡슐화하는 파이토치 모듈인 nn.Embedding 층을 사용하여 토큰의 정수 ID를 신경망 계산에 사용되는 벡터로 매핑
옵티마이저는 모델 가중치를 업데이트할 때, 이 벡터값도 업데이트하여 손실을 최소화
메리 셸리(Mary Shelley)의 프랑켄슈타인(Frankenstein) 데이터셋 사용
토큰에서 벡터의 미니배치를 만드는 벡터화 파이프라인 적용
CBOW 분류모델과 Embedding 층을 사용함
모델 학습 후 모델 평가, 추론 및 모델 검사 수행
```

## 백준



